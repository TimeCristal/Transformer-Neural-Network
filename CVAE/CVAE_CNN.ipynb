{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:41.822382Z",
     "start_time": "2024-09-27T20:46:40.587855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a custom dataset with transformations applied\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Apply transform if available\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ],
   "id": "4193909e8a45dcf4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:42.610461Z",
     "start_time": "2024-09-27T20:46:41.823377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Encoder\n",
    "class ConvCVAEEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, label_dim):\n",
    "        super(ConvCVAEEncoder, self).__init__()\n",
    "        self.label_embedding = nn.Linear(label_dim, 28 * 28)\n",
    "        self.conv1 = nn.Conv2d(2, 28, kernel_size=2, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(28, 56, kernel_size=2, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(56, 112, kernel_size=2, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(11200, 224)\n",
    "        self.fc_z_mean = nn.Linear(224, latent_dim)\n",
    "        self.fc_z_log_var = nn.Linear(224, latent_dim)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        label_embedding = self.label_embedding(labels).view(-1, 1, 28, 28)  # Reshape to match image channel dimension\n",
    "        x = torch.cat([x, label_embedding], dim=1)  # Concatenate along the channel dimension\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        \n",
    "        # print(x.shape)  # Add this to check the shape before the fully connected layer\n",
    "    \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        z_mean = self.fc_z_mean(x)\n",
    "        z_log_var = self.fc_z_log_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class ConvCVAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, label_dim):\n",
    "        super(ConvCVAEDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + label_dim, 7 * 7 * 64)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 112, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv2 = nn.ConvTranspose2d(112, 56, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv3 = nn.ConvTranspose2d(56, 28, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.ConvTranspose2d(28, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        x = torch.cat([z, labels], dim=1)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = x.view(-1, 64, 7, 7)  # Reshape for convolution\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))  # Apply sigmoid to ensure output is between 0 and 1\n",
    "        return x\n",
    "    \n",
    "# Full CVAE Model\n",
    "class ConvCVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, label_dim):\n",
    "        super(ConvCVAE, self).__init__()\n",
    "        self.encoder = ConvCVAEEncoder(latent_dim, label_dim)\n",
    "        self.decoder = ConvCVAEDecoder(latent_dim, label_dim)\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        z_mean, z_log_var = self.encoder(x, labels)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        return self.decoder(z, labels), z_mean, z_log_var\n",
    "\n",
    "# Loss Function\n",
    "def loss_function(reconstructed_x, x, z_mean, z_log_var):\n",
    "    reconstruction_loss = nn.functional.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Training and Validation Loop\n",
    "def train_cvae(model, train_loader, val_loader, optimizer, epochs, device, num_classes):\n",
    "    early_stopping = EarlyStopping(patience=10, delta=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            target = torch.nn.functional.one_hot(target, num_classes=num_classes).float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_x, z_mean, z_log_var = model(data, target)\n",
    "            loss = loss_function(reconstructed_x, data, z_mean, z_log_var)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = validate_cvae(model, val_loader, device, num_classes=2)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader.dataset):.4f}, Val Loss: {val_loss / len(val_loader.dataset):.4f}\")\n",
    "        \n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "def validate_cvae(model, val_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = torch.nn.functional.one_hot(target, num_classes=num_classes).float().to(device)\n",
    "            reconstructed_x, z_mean, z_log_var = model(data, target)\n",
    "            val_loss += loss_function(reconstructed_x, data, z_mean, z_log_var).item()\n",
    "    return val_loss\n",
    "\n",
    "# Inference and Image Generation\n",
    "def generate_images(model, label, latent_dim, num_classes, num_images=5, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        label_tensor = torch.eye(num_classes)[[label] * num_images].to(device)\n",
    "        z = torch.randn(num_images, latent_dim).to(device)\n",
    "        generated_images = model.decoder(z, label_tensor).cpu()\n",
    "        generated_images = generated_images.view(num_images, 28, 28)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(1, num_images, i + 1)\n",
    "            plt.imshow(generated_images[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# # Data Preparation\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# \n",
    "# dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# \n",
    "batch_size = 256\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# \n",
    "# # Model, Optimizer, and Device\n",
    "# latent_dim = 16\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ConvCVAE(latent_dim=latent_dim).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)"
   ],
   "id": "d791c9910e35274f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:42.614521Z",
     "start_time": "2024-09-27T20:46:42.612541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Model\n",
    "# epochs = 5\n",
    "# train_cvae(model, train_loader, val_loader, optimizer, epochs, device)"
   ],
   "id": "e33f0be975bd531e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:42.618064Z",
     "start_time": "2024-09-27T20:46:42.615810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate new images based on a label\n",
    "# generate_images(model, label=4, latent_dim=latent_dim, num_images=5, device=device)"
   ],
   "id": "77a982f46f21ddda",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:44.399792Z",
     "start_time": "2024-09-27T20:46:42.619042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from KL.kl.utils import load_fx\n",
    "window_size = 30\n",
    "pair = 'EURUSD'\n",
    "X, y, returns = load_fx(data_start=0, data_end=5000, window_size=window_size, shift=1, pair=pair)\n"
   ],
   "id": "c8e970bd2888c7d6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:46.407437Z",
     "start_time": "2024-09-27T20:46:44.400835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyts.image import GramianAngularField\n",
    "transformer = GramianAngularField()\n",
    "\n",
    "X_new = transformer.transform(X)\n",
    "\n",
    "X_new = X_new.astype('float32')\n",
    "y = y.astype('int64')  # Ensure labels are in correct format\n",
    "print(X_new.shape, y.shape, X_new.dtype, y.dtype)\n",
    "print(X_new.min(), X_new.max(), y.min(), y.max())"
   ],
   "id": "fd283520e1a83213",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4970, 28, 28) (4970,) float32 int64\n",
      "-1.0 1.0 0 1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:46:46.422415Z",
     "start_time": "2024-09-27T20:46:46.409691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modify the transform to convert data to float32\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "    #transforms.Normalize((0.5,), (0.5,)),  # Normalize\n",
    "    transforms.Lambda(lambda x: (x + 1) / 2),  # Rescale from [-1, 1] to [0, 1]\n",
    "    transforms.Lambda(lambda x: x.float())  # Ensure data is in float32\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(X_new, y, transform=transform)\n",
    "\n",
    "# Define the sizes for train and validation split\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size   # 20% for validation\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for both train and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "b17fddcd1ff092eb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:25:04.258625Z",
     "start_time": "2024-09-27T20:46:46.423993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latent_dim = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_fx = ConvCVAE(latent_dim=latent_dim, label_dim=2).to(device)\n",
    "optimizer = Adam(model_fx.parameters(), lr=0.001)\n",
    "epochs = 200\n",
    "train_cvae(model_fx, train_loader, val_loader, optimizer, epochs, device, num_classes=2)"
   ],
   "id": "186f4a27c00ecec8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 429.8616, Val Loss: 390.1148\n",
      "Epoch 2, Train Loss: 382.6198, Val Loss: 371.2801\n",
      "Epoch 3, Train Loss: 371.2884, Val Loss: 367.1723\n",
      "Epoch 4, Train Loss: 369.6861, Val Loss: 366.3271\n",
      "Epoch 5, Train Loss: 369.0987, Val Loss: 365.4795\n",
      "Epoch 6, Train Loss: 368.0409, Val Loss: 364.3188\n",
      "Epoch 7, Train Loss: 365.7725, Val Loss: 362.3715\n",
      "Epoch 8, Train Loss: 364.7710, Val Loss: 362.1905\n",
      "Epoch 9, Train Loss: 364.6663, Val Loss: 361.9516\n",
      "Epoch 10, Train Loss: 364.4333, Val Loss: 362.2750\n",
      "Epoch 11, Train Loss: 364.3132, Val Loss: 361.5821\n",
      "Epoch 12, Train Loss: 364.1678, Val Loss: 361.7905\n",
      "Epoch 13, Train Loss: 364.3111, Val Loss: 362.3815\n",
      "Epoch 14, Train Loss: 364.2024, Val Loss: 361.6979\n",
      "Epoch 15, Train Loss: 364.1260, Val Loss: 361.5276\n",
      "Epoch 16, Train Loss: 363.6675, Val Loss: 360.5324\n",
      "Epoch 17, Train Loss: 363.1403, Val Loss: 360.6198\n",
      "Epoch 18, Train Loss: 362.8850, Val Loss: 360.2281\n",
      "Epoch 19, Train Loss: 362.4349, Val Loss: 359.6426\n",
      "Epoch 20, Train Loss: 361.8712, Val Loss: 359.2774\n",
      "Epoch 21, Train Loss: 361.3704, Val Loss: 358.7405\n",
      "Epoch 22, Train Loss: 360.9631, Val Loss: 359.1314\n",
      "Epoch 23, Train Loss: 360.9144, Val Loss: 358.5103\n",
      "Epoch 24, Train Loss: 360.6620, Val Loss: 359.2773\n",
      "Epoch 25, Train Loss: 360.6700, Val Loss: 358.2742\n",
      "Epoch 26, Train Loss: 360.2485, Val Loss: 357.6423\n",
      "Epoch 27, Train Loss: 359.8369, Val Loss: 357.8564\n",
      "Epoch 28, Train Loss: 359.4768, Val Loss: 357.4246\n",
      "Epoch 29, Train Loss: 359.1772, Val Loss: 356.8842\n",
      "Epoch 30, Train Loss: 359.0872, Val Loss: 357.0173\n",
      "Epoch 31, Train Loss: 358.8399, Val Loss: 356.4676\n",
      "Epoch 32, Train Loss: 358.5146, Val Loss: 356.6890\n",
      "Epoch 33, Train Loss: 358.1889, Val Loss: 355.9541\n",
      "Epoch 34, Train Loss: 357.6622, Val Loss: 355.4862\n",
      "Epoch 35, Train Loss: 357.4570, Val Loss: 355.1362\n",
      "Epoch 36, Train Loss: 357.3869, Val Loss: 355.0844\n",
      "Epoch 37, Train Loss: 357.0977, Val Loss: 354.7740\n",
      "Epoch 38, Train Loss: 356.8556, Val Loss: 354.4136\n",
      "Epoch 39, Train Loss: 356.4982, Val Loss: 354.7126\n",
      "Epoch 40, Train Loss: 356.6199, Val Loss: 354.4246\n",
      "Epoch 41, Train Loss: 356.2684, Val Loss: 354.1713\n",
      "Epoch 42, Train Loss: 355.6032, Val Loss: 353.4469\n",
      "Epoch 43, Train Loss: 355.3820, Val Loss: 353.4586\n",
      "Epoch 44, Train Loss: 355.3743, Val Loss: 352.9803\n",
      "Epoch 45, Train Loss: 355.0596, Val Loss: 353.1605\n",
      "Epoch 46, Train Loss: 354.8845, Val Loss: 352.7236\n",
      "Epoch 47, Train Loss: 354.6978, Val Loss: 352.3219\n",
      "Epoch 48, Train Loss: 354.3976, Val Loss: 352.3498\n",
      "Epoch 49, Train Loss: 354.3189, Val Loss: 351.7982\n",
      "Epoch 50, Train Loss: 353.8975, Val Loss: 351.6739\n",
      "Epoch 51, Train Loss: 353.6520, Val Loss: 351.6747\n",
      "Epoch 52, Train Loss: 353.4475, Val Loss: 351.2475\n",
      "Epoch 53, Train Loss: 353.0409, Val Loss: 350.7634\n",
      "Epoch 54, Train Loss: 352.7289, Val Loss: 351.0884\n",
      "Epoch 55, Train Loss: 352.2872, Val Loss: 350.0260\n",
      "Epoch 56, Train Loss: 351.6209, Val Loss: 349.3524\n",
      "Epoch 57, Train Loss: 351.0411, Val Loss: 349.5187\n",
      "Epoch 58, Train Loss: 350.5142, Val Loss: 348.6249\n",
      "Epoch 59, Train Loss: 350.1437, Val Loss: 348.0082\n",
      "Epoch 60, Train Loss: 349.5024, Val Loss: 348.3451\n",
      "Epoch 61, Train Loss: 349.0680, Val Loss: 346.9293\n",
      "Epoch 62, Train Loss: 348.3214, Val Loss: 346.5534\n",
      "Epoch 63, Train Loss: 348.0266, Val Loss: 346.4086\n",
      "Epoch 64, Train Loss: 347.7720, Val Loss: 346.0187\n",
      "Epoch 65, Train Loss: 347.3110, Val Loss: 345.8277\n",
      "Epoch 66, Train Loss: 346.9530, Val Loss: 344.9772\n",
      "Epoch 67, Train Loss: 346.5042, Val Loss: 345.2249\n",
      "Epoch 68, Train Loss: 346.2759, Val Loss: 344.9020\n",
      "Epoch 69, Train Loss: 345.9162, Val Loss: 344.6477\n",
      "Epoch 70, Train Loss: 346.0777, Val Loss: 344.5859\n",
      "Epoch 71, Train Loss: 345.5312, Val Loss: 343.8817\n",
      "Epoch 72, Train Loss: 345.0420, Val Loss: 343.2936\n",
      "Epoch 73, Train Loss: 344.3351, Val Loss: 342.9278\n",
      "Epoch 74, Train Loss: 344.0073, Val Loss: 342.8192\n",
      "Epoch 75, Train Loss: 343.4424, Val Loss: 341.4597\n",
      "Epoch 76, Train Loss: 342.6393, Val Loss: 341.6914\n",
      "Epoch 77, Train Loss: 342.3444, Val Loss: 340.8790\n",
      "Epoch 78, Train Loss: 342.0488, Val Loss: 341.6917\n",
      "Epoch 79, Train Loss: 341.5623, Val Loss: 340.8134\n",
      "Epoch 80, Train Loss: 341.1324, Val Loss: 339.6986\n",
      "Epoch 81, Train Loss: 340.3475, Val Loss: 339.6518\n",
      "Epoch 82, Train Loss: 340.0706, Val Loss: 338.5160\n",
      "Epoch 83, Train Loss: 339.8257, Val Loss: 338.8805\n",
      "Epoch 84, Train Loss: 339.5414, Val Loss: 338.6254\n",
      "Epoch 85, Train Loss: 338.9282, Val Loss: 337.6106\n",
      "Epoch 86, Train Loss: 338.5249, Val Loss: 337.1624\n",
      "Epoch 87, Train Loss: 337.9236, Val Loss: 336.9520\n",
      "Epoch 88, Train Loss: 337.7291, Val Loss: 337.0463\n",
      "Epoch 89, Train Loss: 337.5422, Val Loss: 336.6391\n",
      "Epoch 90, Train Loss: 337.1584, Val Loss: 337.0675\n",
      "Epoch 91, Train Loss: 337.0123, Val Loss: 336.4634\n",
      "Epoch 92, Train Loss: 336.9822, Val Loss: 336.0346\n",
      "Epoch 93, Train Loss: 336.5354, Val Loss: 336.4405\n",
      "Epoch 94, Train Loss: 336.2364, Val Loss: 335.8586\n",
      "Epoch 95, Train Loss: 335.8438, Val Loss: 335.2094\n",
      "Epoch 96, Train Loss: 335.5371, Val Loss: 335.1766\n",
      "Epoch 97, Train Loss: 335.4964, Val Loss: 335.1263\n",
      "Epoch 98, Train Loss: 335.7495, Val Loss: 335.2753\n",
      "Epoch 99, Train Loss: 335.3079, Val Loss: 335.1627\n",
      "Epoch 100, Train Loss: 334.8847, Val Loss: 335.3606\n",
      "Epoch 101, Train Loss: 334.8714, Val Loss: 334.4586\n",
      "Epoch 102, Train Loss: 334.5881, Val Loss: 334.7943\n",
      "Epoch 103, Train Loss: 334.4419, Val Loss: 334.7261\n",
      "Epoch 104, Train Loss: 334.2974, Val Loss: 334.5722\n",
      "Epoch 105, Train Loss: 334.0555, Val Loss: 334.9783\n",
      "Epoch 106, Train Loss: 334.0234, Val Loss: 334.5196\n",
      "Epoch 107, Train Loss: 333.8848, Val Loss: 334.4841\n",
      "Epoch 108, Train Loss: 333.8240, Val Loss: 334.4664\n",
      "Epoch 109, Train Loss: 333.7514, Val Loss: 333.1282\n",
      "Epoch 110, Train Loss: 333.5890, Val Loss: 334.5991\n",
      "Epoch 111, Train Loss: 333.3113, Val Loss: 333.1275\n",
      "Epoch 112, Train Loss: 333.0788, Val Loss: 333.5644\n",
      "Epoch 113, Train Loss: 332.9462, Val Loss: 334.1404\n",
      "Epoch 114, Train Loss: 333.1040, Val Loss: 334.0612\n",
      "Epoch 115, Train Loss: 332.8816, Val Loss: 333.5205\n",
      "Epoch 116, Train Loss: 332.6833, Val Loss: 333.5413\n",
      "Epoch 117, Train Loss: 332.6321, Val Loss: 334.0203\n",
      "Epoch 118, Train Loss: 332.4981, Val Loss: 333.5405\n",
      "Epoch 119, Train Loss: 332.8683, Val Loss: 333.9807\n",
      "Epoch 120, Train Loss: 332.6769, Val Loss: 333.1457\n",
      "Epoch 121, Train Loss: 332.0952, Val Loss: 333.2955\n",
      "Early stopping\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:25:05.015078Z",
     "start_time": "2024-09-27T21:25:04.265207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate new images based on a label\n",
    "generate_images(model_fx, label=0, num_classes=2, latent_dim=latent_dim, num_images=5, device=device)"
   ],
   "id": "38b10a28dfb16a6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:25:05.581223Z",
     "start_time": "2024-09-27T21:25:05.064444Z"
    }
   },
   "cell_type": "code",
   "source": "generate_images(model_fx, label=1, num_classes=2, latent_dim=latent_dim, num_images=5, device=device)",
   "id": "b3c48c8173e7846f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:25:07.126649Z",
     "start_time": "2024-09-27T21:25:05.583933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_fx.eval()\n",
    "num_classes = 2\n",
    "label = 0\n",
    "num_images = 1000\n",
    "with torch.no_grad():\n",
    "    label_tensor = torch.eye(num_classes)[[label] * num_images].to(device)\n",
    "    z = torch.randn(num_images, latent_dim).to(device)\n",
    "    generated_images = model_fx.decoder(z, label_tensor).cpu()\n",
    "    generated_images = generated_images.view(num_images, 28, 28)"
   ],
   "id": "86aa74810f9a5fa5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:27:23.232493Z",
     "start_time": "2024-09-27T21:25:07.150022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 7x7 is the output size after pooling twice\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the feature map\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training and validation function\n",
    "def train_cnn(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "              f'Train Acc: {correct / total:.4f}, Val Loss: {val_loss / len(val_loader):.4f}, '\n",
    "              f'Val Acc: {val_correct / val_total:.4f}')\n",
    "\n",
    "# Load data and initialize the model, optimizer, and criterion\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize CNN, criterion, and optimizer\n",
    "cnn_model = SimpleCNN(num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_cnn(cnn_model, train_loader, val_loader, criterion, optimizer, device, epochs=10)"
   ],
   "id": "6ff135d893585df7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6949, Train Acc: 0.4950, Val Loss: 0.6929, Val Acc: 0.5131\n",
      "Epoch 2/10, Train Loss: 0.6943, Train Acc: 0.4977, Val Loss: 0.6930, Val Acc: 0.5151\n",
      "Epoch 3/10, Train Loss: 0.6937, Train Acc: 0.5136, Val Loss: 0.6935, Val Acc: 0.4869\n",
      "Epoch 4/10, Train Loss: 0.6933, Train Acc: 0.4990, Val Loss: 0.6940, Val Acc: 0.4759\n",
      "Epoch 5/10, Train Loss: 0.6917, Train Acc: 0.5111, Val Loss: 0.6938, Val Acc: 0.5131\n",
      "Epoch 6/10, Train Loss: 0.6911, Train Acc: 0.5229, Val Loss: 0.6941, Val Acc: 0.5020\n",
      "Epoch 7/10, Train Loss: 0.6907, Train Acc: 0.5171, Val Loss: 0.6941, Val Acc: 0.5050\n",
      "Epoch 8/10, Train Loss: 0.6868, Train Acc: 0.5420, Val Loss: 0.6985, Val Acc: 0.5111\n",
      "Epoch 9/10, Train Loss: 0.6874, Train Acc: 0.5289, Val Loss: 0.6966, Val Acc: 0.5050\n",
      "Epoch 10/10, Train Loss: 0.6814, Train Acc: 0.5566, Val Loss: 0.7045, Val Acc: 0.4869\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-27T21:32:12.187077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Generate and save images\n",
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def generate_and_save_images(model, latent_dim, num_images_per_class, save_dir, num_classes=2, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label in range(num_classes):\n",
    "            label_dir = os.path.join(save_dir, f'class_{label}')\n",
    "            os.makedirs(label_dir, exist_ok=True)\n",
    "            \n",
    "            label_tensor = torch.eye(num_classes)[[label] * num_images_per_class].to(device)\n",
    "            z = torch.randn(num_images_per_class, latent_dim).to(device)\n",
    "            generated_images = model.decoder(z, label_tensor).cpu()\n",
    "\n",
    "            for i in range(num_images_per_class):\n",
    "                if i%1000==0:\n",
    "                    print(f'done{i}')\n",
    "                img = generated_images[i].view(1, 28, 28)  # Reshape to 1x28x28 (grayscale)\n",
    "                save_image(img, os.path.join(label_dir, f'class_{label}_img_{i}.png'))\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16\n",
    "num_images_per_class = 500  # Number of images per class\n",
    "save_dir = './generated_images/'\n",
    "\n",
    "generate_and_save_images(model_fx, latent_dim, num_images_per_class, save_dir, num_classes=2, device=device)"
   ],
   "id": "1629f520bb1f18f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:27:26.437798Z",
     "start_time": "2024-09-27T21:27:26.423395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a transform for loading images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure it's single-channel grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the dataset from the saved images\n",
    "generated_dataset = datasets.ImageFolder(root=save_dir, transform=transform)\n",
    "\n",
    "# Define the sizes for train and validation split\n",
    "generated_train_size = int(0.8 * len(generated_dataset))  # 80% for training\n",
    "generated_val_size = len(generated_dataset) - generated_train_size   # 20% for validation\n",
    "\n",
    "generated_train_dataset, generated_val_dataset = random_split(generated_dataset, [generated_train_size, generated_val_size])\n",
    "\n",
    "# Create DataLoader for the generated dataset\n",
    "generated_train_loader = DataLoader(generated_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "generated_val_loader = DataLoader(generated_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "generated_loader = DataLoader(generated_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now you can train your CNN using this generated data"
   ],
   "id": "3ee4b82e9f7bacbb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:30:59.451161Z",
     "start_time": "2024-09-27T21:27:26.440352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn_model = SimpleCNN(num_classes=2)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "# train_cnn(cnn_model, train_loader, generated_loader, criterion, optimizer, device, epochs=10)\n",
    "train_cnn(cnn_model, generated_loader, train_loader, criterion, optimizer, device, epochs=100)\n",
    "# train_cnn(cnn_model, generated_train_loader, generated_val_loader, criterion, optimizer, device, epochs=100)"
   ],
   "id": "e071f42102d51f68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.7079, Train Acc: 0.4830, Val Loss: 0.6947, Val Acc: 0.4912\n",
      "Epoch 2/100, Train Loss: 0.6943, Train Acc: 0.4970, Val Loss: 0.6934, Val Acc: 0.4912\n",
      "Epoch 3/100, Train Loss: 0.6930, Train Acc: 0.5000, Val Loss: 0.6944, Val Acc: 0.4912\n",
      "Epoch 4/100, Train Loss: 0.6945, Train Acc: 0.5000, Val Loss: 0.6947, Val Acc: 0.4912\n",
      "Epoch 5/100, Train Loss: 0.6951, Train Acc: 0.4810, Val Loss: 0.6944, Val Acc: 0.4912\n",
      "Epoch 6/100, Train Loss: 0.6934, Train Acc: 0.4960, Val Loss: 0.6968, Val Acc: 0.4912\n",
      "Epoch 7/100, Train Loss: 0.6927, Train Acc: 0.5040, Val Loss: 0.6969, Val Acc: 0.4912\n",
      "Epoch 8/100, Train Loss: 0.6921, Train Acc: 0.5240, Val Loss: 0.6970, Val Acc: 0.4912\n",
      "Epoch 9/100, Train Loss: 0.6930, Train Acc: 0.5090, Val Loss: 0.6988, Val Acc: 0.4912\n",
      "Epoch 10/100, Train Loss: 0.6918, Train Acc: 0.5150, Val Loss: 0.7032, Val Acc: 0.4912\n",
      "Epoch 11/100, Train Loss: 0.6915, Train Acc: 0.5120, Val Loss: 0.7046, Val Acc: 0.4912\n",
      "Epoch 12/100, Train Loss: 0.6897, Train Acc: 0.5430, Val Loss: 0.7190, Val Acc: 0.4912\n",
      "Epoch 13/100, Train Loss: 0.6882, Train Acc: 0.5380, Val Loss: 0.7138, Val Acc: 0.4912\n",
      "Epoch 14/100, Train Loss: 0.6863, Train Acc: 0.5620, Val Loss: 0.7726, Val Acc: 0.4912\n",
      "Epoch 15/100, Train Loss: 0.6831, Train Acc: 0.5510, Val Loss: 0.7649, Val Acc: 0.4912\n",
      "Epoch 16/100, Train Loss: 0.6834, Train Acc: 0.5430, Val Loss: 0.7773, Val Acc: 0.4912\n",
      "Epoch 17/100, Train Loss: 0.6766, Train Acc: 0.5710, Val Loss: 0.9055, Val Acc: 0.4912\n",
      "Epoch 18/100, Train Loss: 0.6752, Train Acc: 0.5770, Val Loss: 0.8738, Val Acc: 0.4912\n",
      "Epoch 19/100, Train Loss: 0.6691, Train Acc: 0.5780, Val Loss: 1.0670, Val Acc: 0.4912\n",
      "Epoch 20/100, Train Loss: 0.6694, Train Acc: 0.5930, Val Loss: 1.2327, Val Acc: 0.4912\n",
      "Epoch 21/100, Train Loss: 0.6624, Train Acc: 0.6000, Val Loss: 1.1354, Val Acc: 0.4912\n",
      "Epoch 22/100, Train Loss: 0.6636, Train Acc: 0.5920, Val Loss: 1.0612, Val Acc: 0.4912\n",
      "Epoch 23/100, Train Loss: 0.6587, Train Acc: 0.5990, Val Loss: 1.0067, Val Acc: 0.4912\n",
      "Epoch 24/100, Train Loss: 0.6562, Train Acc: 0.6130, Val Loss: 1.0217, Val Acc: 0.4912\n",
      "Epoch 25/100, Train Loss: 0.6395, Train Acc: 0.6260, Val Loss: 1.2703, Val Acc: 0.4912\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(cnn_model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# train_cnn(cnn_model, train_loader, generated_loader, criterion, optimizer, device, epochs=10)\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtrain_cnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcnn_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerated_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# train_cnn(cnn_model, generated_train_loader, generated_val_loader, criterion, optimizer, device, epochs=100)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[12], line 39\u001B[0m, in \u001B[0;36mtrain_cnn\u001B[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001B[0m\n\u001B[1;32m     37\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     41\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[12], line 19\u001B[0m, in \u001B[0;36mSimpleCNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)))\n\u001B[0;32m---> 19\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     20\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m64\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m7\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m7\u001B[39m)  \u001B[38;5;66;03m# Flatten the feature map\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x))\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "565721d532b6a1ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
