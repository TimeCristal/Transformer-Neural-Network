{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:34.865772Z",
     "start_time": "2024-09-07T20:49:34.858613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class VAE_Loss:\n",
    "    def __init__(self, mode='standard', alpha=1.0, gamma=1.0):\n",
    "        \"\"\"\n",
    "        VAE Loss Function with multiple modes\n",
    "        Args:\n",
    "        - mode (str): The loss function mode ('standard', 'negative_kl', 'reduced_reconstruction', 'combined')\n",
    "        - alpha (float): Weight for the reconstruction loss (default=1.0)\n",
    "        - gamma (float): Weight for the KL divergence (default=1.0, can be negative in 'negative_kl' mode)\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, recon_x, x, z_mean, z_log_var):\n",
    "        \"\"\"\n",
    "        Compute the VAE loss based on the selected mode\n",
    "        Args:\n",
    "        - recon_x: The reconstructed output from the decoder\n",
    "        - x: The original input data\n",
    "        - z_mean: The mean of the latent variable distribution\n",
    "        - z_log_var: The log variance of the latent variable distribution\n",
    "        Returns:\n",
    "        - Loss (tensor): Computed loss value\n",
    "        \"\"\"\n",
    "        # Reconstruction Loss: Can be MSE or binary cross-entropy, depending on your input/output type.\n",
    "        reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "\n",
    "        if self.mode == 'standard':\n",
    "            # Standard VAE: Minimize both reconstruction and KL divergence\n",
    "            return reconstruction_loss + kl_loss\n",
    "\n",
    "        elif self.mode == 'negative_kl':\n",
    "            # Encourage higher KL divergence by applying a negative weight to the KL term\n",
    "            return reconstruction_loss - self.gamma * kl_loss\n",
    "\n",
    "        elif self.mode == 'reduced_reconstruction':\n",
    "            # Reduce the weight of reconstruction loss to allow more diversity in the latent space\n",
    "            return self.alpha * reconstruction_loss + kl_loss\n",
    "\n",
    "        elif self.mode == 'combined':\n",
    "            # Combination: Reduce reconstruction weight and increase KL divergence\n",
    "            return self.alpha * reconstruction_loss - self.gamma * kl_loss\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode selected: {self.mode}. Choose from ['standard', 'negative_kl', 'reduced_reconstruction', 'combined']\")\n",
    "\n",
    "# Example usage of VAE_Loss class\n",
    "\n",
    "# Assume recon_x, x, z_mean, z_log_var are the outputs from the model\n",
    "\n",
    "# Initialize the loss class\n",
    "# vae_loss_standard = VAE_Loss(mode='standard')\n",
    "# vae_loss_negative_kl = VAE_Loss(mode='negative_kl', gamma=1.5)\n",
    "# vae_loss_reduced_reconstruction = VAE_Loss(mode='reduced_reconstruction', alpha=0.5)\n",
    "# vae_loss_combined = VAE_Loss(mode='combined', alpha=0.5, gamma=1.5)\n",
    "\n",
    "# Example: Using the loss function for each mode\n",
    "# loss_standard = vae_loss_standard(recon_x, x, z_mean, z_log_var)\n",
    "# loss_negative_kl = vae_loss_negative_kl(recon_x, x, z_mean, z_log_var)\n",
    "# loss_reduced_reconstruction = vae_loss_reduced_reconstruction(recon_x, x, z_mean, z_log_var)\n",
    "# loss_combined = vae_loss_combined(recon_x, x, z_mean, z_log_var)\n",
    "# \n",
    "# print(f\"Standard Loss: {loss_standard.item()}\")\n",
    "# print(f\"Negative KL Loss: {loss_negative_kl.item()}\")\n",
    "# print(f\"Reduced Reconstruction Loss: {loss_reduced_reconstruction.item()}\")\n",
    "# print(f\"Combined Loss: {loss_combined.item()}\")"
   ],
   "id": "33f4c39bddea0779",
   "execution_count": 201,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:36.555607Z",
     "start_time": "2024-09-07T20:49:34.867568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from kl.utils import load_fx\n",
    "import numpy as np\n",
    "X, y = load_fx(data_len=5000, shift=2)\n",
    "print(X.shape)"
   ],
   "id": "60af7bdb8596ceb3",
   "execution_count": 202,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:36.574102Z",
     "start_time": "2024-09-07T20:49:36.556582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# VAE definition\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=8, latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2_mean = nn.Linear(128, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, 128)\n",
    "        self.fc4 = nn.Linear(128, input_dim)\n",
    "\n",
    "    # def encode(self, x):\n",
    "    #     h1 = F.relu(self.fc1(x))\n",
    "    #     z_mean = self.fc2_mean(h1)\n",
    "    #     z_log_var = self.fc2_logvar(h1)\n",
    "    #     return z_mean, z_log_var\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        z_mean = self.fc2_mean(h1)\n",
    "        z_log_var = self.fc2_logvar(h1)\n",
    "        \n",
    "        # Clamp z_log_var to avoid extreme values\n",
    "        z_log_var = torch.clamp(z_log_var, min=-10, max=100)\n",
    "    \n",
    "        return z_mean, z_log_var\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        return self.decode(z), z_mean, z_log_var\n",
    "\n",
    "# Loss function (Reconstruction + KL divergence)\n",
    "# def loss_function(recon_x, x, z_mean, z_log_var):\n",
    "#     reconstruction_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "#     kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "#     return reconstruction_loss + kl_loss\n",
    "# Loss function (Reconstruction using MSE + KL divergence)\n",
    "# def loss_function(recon_x, x, z_mean, z_log_var):\n",
    "#     reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')  # Using MSE instead of BCE\n",
    "#     kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "#     return reconstruction_loss + kl_loss\n",
    "\n",
    "# Initialize the VAE\n",
    "input_dim = 8  # For your dataset with 8 features\n",
    "latent_dim = 2\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "\n",
    "# Example input batch (replace with your actual data later)\n",
    "# x = torch.randn(64, input_dim)  # Batch of 64 examples, each with 8 features\n",
    "\n",
    "# Forward pass\n",
    "recon_x, z_mean, z_log_var = vae(X_tensor)"
   ],
   "id": "2111f4f6e56931aa",
   "execution_count": 203,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:36.580919Z",
     "start_time": "2024-09-07T20:49:36.576473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate initial KL divergence\n",
    "initial_kl_divergence = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp()).item()\n",
    "print(f\"Initial KL Divergence: {initial_kl_divergence}\")"
   ],
   "id": "5b57454168652849",
   "execution_count": 204,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:36.586590Z",
     "start_time": "2024-09-07T20:49:36.582349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate loss (for later usage during training)\n",
    "vae_loss = VAE_Loss(mode='standard')\n",
    "loss = vae_loss(recon_x, X_tensor, z_mean, z_log_var)\n",
    "# loss = loss_function(recon_x, x, z_mean, z_log_var)"
   ],
   "id": "1f3bd8634ddf9812",
   "execution_count": 205,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:36.595841Z",
     "start_time": "2024-09-07T20:49:36.587819Z"
    }
   },
   "cell_type": "code",
   "source": "print(X_tensor.mean(), X_tensor.std())",
   "id": "76a1b3c821f4cbed",
   "execution_count": 206,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:36.787929Z",
     "start_time": "2024-09-07T20:49:36.597564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the means and log variances from the latent space\n",
    "z_mean_values = z_mean.detach().numpy()\n",
    "z_log_var_values = z_log_var.detach().numpy()\n",
    "\n",
    "# Plot overlapping histograms for z_mean and z_log_var\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(z_mean_values.flatten(), bins=30, color='blue', alpha=0.5, label='z_mean')\n",
    "plt.hist(z_log_var_values.flatten(), bins=30, color='green', alpha=0.5, label='z_log_var')\n",
    "\n",
    "plt.title('Overlapping Histogram of z_mean and z_log_var')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ],
   "id": "c41354fd6c47d432",
   "execution_count": 207,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T06:38:00.484088Z",
     "start_time": "2024-09-08T06:36:07.948836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you already have your data loaded as X, y\n",
    "# X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(X_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the model (using the VAE defined earlier)\n",
    "input_dim = 8\n",
    "latent_dim = 2\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "\n",
    "# Choose the loss mode\n",
    "# loss_function = VAE_Loss(mode='combined', alpha=0.5, gamma=0.1)  # Lower gamma to stabilize\n",
    "# loss_function = VAE_Loss(mode='negative_kl', alpha=0.5, gamma=0.1)\n",
    "loss_function = VAE_Loss(mode='standard')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-6)\n",
    "\n",
    "# Training loop parameters\n",
    "epochs = 1500  # Number of training epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (x_batch,) in enumerate(dataloader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the VAE\n",
    "        recon_x, z_mean, z_log_var = vae(x_batch)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = 1/loss_function(recon_x, x_batch, z_mean, z_log_var)\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=5.0)\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss for the epoch\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Print latent variable statistics to monitor during training\n",
    "        #print(f\"Batch {batch_idx+1}, z_mean: {z_mean.mean().item()}, z_log_var: {z_log_var.mean().item()}\")\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_epoch_loss:.4f}\")"
   ],
   "id": "337a9fb1dcd65b8",
   "execution_count": 218,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T06:38:12.259038Z",
     "start_time": "2024-09-08T06:38:12.238694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recon_x_trained, z_mean_trained, z_log_var_trained = vae(X_tensor)\n",
    "# Calculate initial KL divergence\n",
    "final_kl_divergence = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp()).item()\n",
    "print(f\"Final KL Divergence: {final_kl_divergence}\")"
   ],
   "id": "34fef6146f43a012",
   "execution_count": 219,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T06:38:19.178465Z",
     "start_time": "2024-09-08T06:38:18.987209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the means and log variances from the latent space\n",
    "z_mean_values_trained = z_mean_trained.detach().numpy()\n",
    "z_log_var_values_trained = z_log_var_trained.detach().numpy()\n",
    "\n",
    "# Plot overlapping histograms for z_mean and z_log_var\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(z_mean_values_trained.flatten(), bins=30, color='blue', alpha=0.5, label='z_mean')\n",
    "plt.hist(z_log_var_values_trained.flatten(), bins=30, color='green', alpha=0.5, label='z_log_var')\n",
    "\n",
    "plt.title('Overlapping Histogram of z_mean and z_log_var after training')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ],
   "id": "66f04cc07dffac83",
   "execution_count": 220,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T06:43:32.643715Z",
     "start_time": "2024-09-08T06:43:32.574834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.savetxt('z_mean_trained.txt', z_mean_values_trained)\n",
    "np.savetxt('z_log_var_trained.txt', z_log_var_values_trained)\n",
    "np.savetxt('y.txt', y)\n",
    "np.savetxt('recon_x_trained.txt', recon_x_trained.detach().numpy())"
   ],
   "id": "3559397cec03b927",
   "execution_count": 221,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T20:49:48.147014Z",
     "start_time": "2024-09-07T20:49:48.144712Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "80d712b3111acb32",
   "execution_count": 211,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
