{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "import pandas as pd",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_start = 0\n",
    "data_end = 5000\n",
    "df = pd.read_csv(\n",
    "        \"/Users/krasimirtrifonov/Documents/GitHub/Transformer-Neural-Network/dataset/EURUSD_Daily_200005300000_202405300000.csv\",\n",
    "        delimiter=\"\\t\")\n",
    "\n",
    "# Extract the closing prices\n",
    "closing = df[\"<CLOSE>\"].iloc[data_start:data_end]"
   ],
   "id": "619ab3d23d348ca0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Augmented Dickey-Fuller (ADF) test using the statsmodels library:",
   "id": "f5624843730d9038"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Assume 'data' is your dataframe and '<CLOSE>' is the column of interest\n",
    "\n",
    "# Perform Augmented Dickey-Fuller test on the <CLOSE> column\n",
    "adf_result = adfuller(df['<CLOSE>'])\n",
    "\n",
    "# Extract the test statistics and p-value\n",
    "adf_statistic = adf_result[0]\n",
    "p_value = adf_result[1]\n",
    "critical_values = adf_result[4]\n",
    "\n",
    "# Print the results\n",
    "print(f'ADF Statistic: {adf_statistic:.4f}')\n",
    "print(f'p-value: {p_value:.4f}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'   {key}: {value:.4f}')"
   ],
   "id": "e66882de3b097268",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        •\tThe p-value of 0.253 is higher than the usual significance levels (1%, 5%, or even 10%), meaning we fail to reject the null hypothesis of the test. This suggests that the data is non-stationary.\n",
    "        •\tThe ADF statistic is greater than all critical values, further confirming that the series likely has a unit root (non-stationary).\n",
    "    Let try to make it stationary by Differencing and we perform the ADF test   \n",
    "        "
   ],
   "id": "c0b35c01b77def74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Differencing\n",
    "close_diff = df['<CLOSE>'].diff().dropna()"
   ],
   "id": "d2fd238e5f2e98d5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### *Apply ADF on Difference Close",
   "id": "dfe8b21910e10d00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform Augmented Dickey-Fuller test on the <CLOSE> column\n",
    "adf_result = adfuller(close_diff)\n",
    "\n",
    "# Extract the test statistics and p-value\n",
    "adf_statistic = adf_result[0]\n",
    "p_value = adf_result[1]\n",
    "critical_values = adf_result[4]\n",
    "\n",
    "# Print the results\n",
    "print(f'ADF Statistic on Difference Close: {adf_statistic:.4f}')\n",
    "print(f'p-value: {p_value:.4f}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'   {key}: {value:.4f}')"
   ],
   "id": "44af50e165ed060",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\t•\tThe ADF statistic is much lower than the critical values, and the p-value is 0.0, meaning we can now reject the null hypothesis.\n",
    "\t•\tThis indicates that the differenced data is stationary."
   ],
   "id": "429b10ca1aa35b04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###    -Now let check for Engle’s ARCH test to check for <strong>heteroscedasticity</strong>",
   "id": "f56093ea4dbb6c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "# Perform Engle's ARCH test\n",
    "arch_test_result = het_arch(close_diff)\n",
    "\n",
    "# Extract the test statistic and p-value\n",
    "arch_statistic = arch_test_result[0]\n",
    "arch_p_value = arch_test_result[1]\n",
    "\n",
    "# Print the results\n",
    "print(f'ARCH Test Statistic on Close Diff: {arch_statistic:.4f}')\n",
    "print(f'p-value: {arch_p_value:.4f}')"
   ],
   "id": "9a500939a5aa84f7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##\t•\tA low p-value (typically < 0.05) suggests heteroscedasticity (i.e., variance that changes over time).\n",
    "### Let try on repeating diff"
   ],
   "id": "7f726d6f4d8cddbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "close_diff_diff = close_diff.diff().dropna()\n",
    "# Perform Engle's ARCH test on Repeating Close Diff\n",
    "arch_test_result = het_arch(close_diff_diff)\n",
    "\n",
    "# Extract the test statistic and p-value\n",
    "arch_statistic = arch_test_result[0]\n",
    "arch_p_value = arch_test_result[1]\n",
    "\n",
    "# Print the results\n",
    "print(f'ARCH Test Statistic on Repeating Close Diff: {arch_statistic:.4f}')\n",
    "print(f'p-value: {arch_p_value:.9f}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first 50 points of the second differenced data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the second difference of <CLOSE>\n",
    "plt.plot(close_diff_diff.head(50), marker='o', linestyle='-', label='Second Difference of <CLOSE>')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.title('First 50 Points of Second Difference <CLOSE>')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Second Difference Close')\n",
    "plt.grid(True)  # Adds a grid for better readability\n",
    "plt.legend()  # Shows the legend\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "id": "60c3b5647a2c80d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###\t•\tThe extremely low p-value suggests that even after applying a second differencing, the data still exhibits heteroscedasticity (variance is not constant over time).\n",
    "## It is Time to apply GARCH "
   ],
   "id": "251cd2d16dcb3c39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "\n",
    "# Load your data (ensure the percent change data is properly calculated)\n",
    "# Assuming 'data' is the DataFrame with a 'CLOSE_PERCENT_CHANGE' column\n",
    "\n",
    "# Prepare the percent change data for GARCH modeling\n",
    "garch_data = close_diff\n",
    "\n",
    "# Fit a GARCH(1, 1) model on the percent change data\n",
    "garch_model = arch_model(garch_data, vol='Garch', p=1, q=1)\n",
    "garch_fitted = garch_model.fit(disp=\"off\")\n",
    "\n",
    "# Summarize the model\n",
    "print(garch_fitted.summary())\n",
    "\n",
    "# Forecast future volatility (for the next 5 days, for example)\n",
    "forecast = garch_fitted.forecast(horizon=5)\n",
    "print(forecast.variance[-1:])  # This gives the predicted variance (volatility) for the next 5 periods"
   ],
   "id": "2b3c12383c1151a3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The results of your GARCH(1,1) model are as follows, with some key interpretations and next steps:\n",
    "\n",
    "GARCH Model Summary:\n",
    "\n",
    "Mean Model:\n",
    "\n",
    "\t•\tmu: The mean (constant) of the series is approximately 0.000024, with a p-value of 0.729, which indicates that the mean is not statistically significant.\n",
    "\n",
    "Volatility (GARCH) Model:\n",
    "\n",
    "\t•\tomega: 1.0101 \\times 10^{-6} (p-value = 0.000) – This is the baseline level of variance, and it is statistically significant.\n",
    "\t•\talpha[1]: 0.0500 (p-value = 0.000) – This measures the influence of past shocks (squared returns or volatility) on future volatility. The significance indicates that past volatility influences future volatility.\n",
    "\t•\tbeta[1]: 0.9300 (p-value = 0.000) – This measures the persistence of volatility. A value close to 1 indicates high persistence, meaning that volatility tends to remain elevated for longer periods.\n",
    "\n",
    "Key Observations:\n",
    "\n",
    "\t1.\tHigh persistence of volatility: With a beta of 0.93, volatility is persistent, meaning that once volatility rises, it tends to remain high for several periods.\n",
    "\t2.\tStatistical significance: Both the alpha and beta coefficients are highly significant, meaning the GARCH model is capturing the volatility structure well.\n",
    "\t3.\tNormal Distribution: The residuals (errors) are assumed to follow a normal distribution, but other distributions (like Student’s t) might improve the fit.\n",
    "\n",
    "Forecast:\n",
    "\n",
    "The model produces forecasts for the next 5 periods of volatility:\n",
    "\n",
    "\t•\tThe forecasted volatility (variance) starts at 0.000021 and increases slightly to 0.000023 by the 5th period.\n",
    "\n",
    "Convergence Warning:\n",
    "\n",
    "\t•\tThe message \"Inequality constraints incompatible\" suggests that the model did not converge fully. This could mean:\n",
    "\t•\tThe starting values or constraints may need adjustment.\n",
    "\t•\tTrying different distributions (e.g., Student’s t instead of normal) might help improve model fit.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "\t1.\tRefining the Model:\n",
    "\t•\tTry changing the distribution assumption from normal to Student’s t to allow for heavier tails.\n",
    "\t•\tEnsure proper convergence by adjusting model parameters or re-running the optimization with different settings.\n",
    "\t2.\tVolatility Interpretation:\n",
    "\t•\tThe forecasted volatility is low (around 0.000021), indicating stable conditions in the short term. If you’re looking for predictive insights, this information could be useful in understanding future market behavior.\n",
    "\t3.\tResidual Analysis:\n",
    "\t•\tCheck the residuals from the model to ensure no significant patterns or autocorrelation are left."
   ],
   "id": "c87ff137a3f44595"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rescale the data (multiply by 100)\n",
    "scaled_data = close_diff * 100\n",
    "\n",
    "# Fit the GARCH model with rescaled data\n",
    "garch_model = arch_model(scaled_data, vol='Garch', p=1, q=1)\n",
    "garch_rescaled_fitted = garch_model.fit(disp=\"off\")\n",
    "\n",
    "# Summarize the model\n",
    "print(garch_rescaled_fitted.summary())\n",
    "\n",
    "\n",
    "# Forecasting future volatility with rescaled data\n",
    "forecast = garch_rescaled_fitted.forecast(horizon=5)\n",
    "forecast_variance_rescaled = forecast.variance[-1:] / 100  # Scale back\n",
    "print(forecast_variance_rescaled)"
   ],
   "id": "77c9879ac2930100",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The recommendation to scale rather than normalize might seem unusual because in many machine learning contexts, normalization or standardization is a common preprocessing step. However, in the case of GARCH models (and other econometric models), there are reasons why scaling is preferred over normalization for better optimization and interpretation:\n",
    "\n",
    "Why Scaling is Recommended in GARCH Models:\n",
    "\n",
    "\t1.\tEconometric Models Work with Raw Value Ranges:\n",
    "\t•\tIn time series econometrics, especially with models like GARCH, the parameters are sensitive to the actual magnitude of the data. The variance and returns are calculated directly from the raw data, so the absolute scale of the input values affects how the optimizer estimates the volatility structure.\n",
    "\t•\tScaling by a constant factor (like multiplying by 100) maintains the relationships in the data, while simply normalizing (which changes the range to [0, 1]) may not preserve the underlying structure of volatility or returns in the same way.\n",
    "\t2.\tNormalization Alters Statistical Properties:\n",
    "\t•\tNormalizing the data (scaling it between 0 and 1 or z-scoring) would modify the mean and variance of the data. For models like GARCH, which rely on these properties to model volatility, such changes can disrupt the model’s ability to accurately estimate parameters.\n",
    "\t•\tScaling by a constant factor (e.g., multiplying by 100) preserves the mean-to-variance ratio and leaves the overall structure of the time series unchanged.\n",
    "\t3.\tConvergence Stability:\n",
    "\t•\tOptimization algorithms in econometric models often perform better when the data is in a reasonable range (not too small or too large). If the data has very small values (as seen with your original data), the optimizer might struggle to find a solution due to the precision required for floating-point calculations. Rescaling moves the data into a more manageable range, improving convergence.\n",
    "\t4.\tMaintaining Interpretability:\n",
    "\t•\tWhen you scale the data by a constant (e.g., 100), the output from the GARCH model can still be interpreted in terms of the original scale. You can easily scale the results (e.g., volatility forecasts) back to the original units.\n",
    "\t•\tIn contrast, normalization would make interpreting the model’s output more difficult, as it would no longer be in the original units of the data.\n",
    "\n",
    "To Clarify:\n",
    "\n",
    "\t•\tScaling is a linear transformation that preserves the relationships between the data points (i.e., a proportional increase or decrease in values).\n",
    "\t•\tNormalization changes the distribution of the data, which can affect models that rely on the original statistical properties of the series (like variance, mean, and residuals).\n",
    "\n",
    "Example:\n",
    "\n",
    "If your data values are small (e.g., 5.05 \\times 10^{-5}), the optimizer might have trouble accurately estimating parameters because the changes between points are too subtle. Scaling by 100 simply stretches the data but keeps the relationship between data points intact.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\t•\tNormalization could alter the statistical properties (mean, variance) that are critical for GARCH modeling.\n",
    "\t•\tScaling preserves the data’s structure while making the optimization easier and improving convergence.\n",
    "\t•\tAfter fitting the model, you can easily scale the results back to the original units if needed."
   ],
   "id": "ad35d5dec5c662a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 1: Plot Conditional Volatility Over Time\n",
    "\n",
    "You can plot the conditional variance (volatility) over time, which gives insight into how well the model captures changing volatility."
   ],
   "id": "6a9ed2ebde8e399f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the conditional volatility over time (variance over time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot the conditional volatility (square root of variance)\n",
    "conditional_volatility = garch_rescaled_fitted.conditional_volatility\n",
    "plt.plot(garch_rescaled_fitted.conditional_volatility, label='Conditional Volatility')\n",
    "plt.title('Conditional Volatility Over Time (GARCH Model)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Conditional Volatility')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "44082ce5cc1e55bc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 2: Residual Analysis\n",
    "\n",
    "After fitting the GARCH model, we can check whether the residuals are behaving like white noise (uncorrelated and normally distributed). This is a key test to verify if the model has captured the volatility dynamics adequately."
   ],
   "id": "f08a382cc40a4104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the standardized residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "resid = garch_rescaled_fitted.resid\n",
    "plt.plot(garch_rescaled_fitted.resid, label='Standardized Residuals')\n",
    "plt.title('Standardized Residuals from GARCH Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c87a33dba43066a8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import numpy as np\n",
    "# Test for autocorrelation in the residuals using the Ljung-Box test\n",
    "ljung_box_test = acorr_ljungbox(garch_rescaled_fitted.resid, lags=[10], return_df=True)\n",
    "\n",
    "print(ljung_box_test)\n",
    "if  np.array(ljung_box_test.lb_pvalue)[0]  > 0.05:\n",
    "    print('there’s no significant autocorrelation in the residuals, suggesting that the GARCH model has adequately captured the volatility dynamics')\n",
    "else:\n",
    "     print(\"GARCH is not adequate!\")   "
   ],
   "id": "b808f61e26b56acc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If the p-values from the Ljung-Box test are greater than 0.05, it means there’s no significant autocorrelation in the residuals, suggesting that the GARCH model has adequately captured the volatility dynamics.",
   "id": "403ca6423ff4b2bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use a Q-Q plot to check if residuals are normally distributed\n",
    "import scipy.stats as stats\n",
    "stats.probplot(garch_rescaled_fitted.resid, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of GARCH Model Residuals')\n",
    "plt.show()"
   ],
   "id": "94ca3c2f9f38aed9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Q-Q plot compares the distribution of the residuals to a theoretical normal distribution. In an ideal case where the residuals are normally distributed, the points should fall along the diagonal line.\n",
    "\n",
    "In this case, deviations from the line, especially in the tails, suggest that the residuals might not be perfectly normally distributed. This could indicate the presence of fat tails (kurtosis) or skewness, which are common in financial time series data."
   ],
   "id": "641be5adaf18e93f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forecast future volatility for 5 periods\n",
    "forecast_rescaled = garch_rescaled_fitted.forecast(horizon=5)\n",
    "\n",
    "# Print forecasted variance\n",
    "print(\"Forecasted variance over next 5 periods:\")\n",
    "print(forecast_rescaled.variance[-1:])"
   ],
   "id": "9c71626572a984fc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# Assuming 'resid_data' contains the residuals from your GARCH model\n",
    "# Replace 'resid_data['Residuals']' with your own data if necessary\n",
    "\n",
    "# Calculate Skewness\n",
    "skewness = skew(resid)\n",
    "\n",
    "# Calculate Kurtosis\n",
    "kurt = kurtosis(resid)\n",
    "\n",
    "# Print the results\n",
    "print(f'Skewness: {skewness:.4f}')\n",
    "print(f'Kurtosis: {kurt:.4f}')"
   ],
   "id": "e892d656ae94ca2c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\tSkewness: 0.0247\n",
    "\t•\tA skewness value close to zero suggests that the residuals are nearly symmetric. This means there’s no significant skewness in the data.\n",
    "\tKurtosis: 2.47\n",
    "\t•\tKurtosis less than 3 indicates platykurtic behavior, meaning the residuals have lighter tails than a normal distribution. \n",
    "\t    This suggests that extreme events (outliers) are less frequent compared to what a normal distribution would predict.\n",
    "\t•\tThe residuals appear to have little skewness, meaning they are relatively symmetric around the mean.\n",
    "\t•\tThe kurtosis is slightly lower than 3, indicating fewer extreme values (fat tails) compared to a normal distribution. \n",
    "\t    This suggests that the normal distribution assumption might still hold, although it’s on the borderline."
   ],
   "id": "fce14d07c6cdca6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 1: Fit GARCH Models with Different Distributions\n",
    "\n",
    "You can fit the GARCH model with various distributions to determine the best fit:"
   ],
   "id": "398f44fa15f5513f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from arch import arch_model\n",
    "\n",
    "# Assuming 'rescaled_data' contains your rescaled returns or percent changes\n",
    "# Fit GARCH(1,1) with different distributions\n",
    "\n",
    "# Normal distribution (default)\n",
    "garch_normal = arch_model(scaled_data, vol='Garch', p=1, q=1, dist='normal').fit(disp=\"off\")\n",
    "\n",
    "# Student's t-distribution\n",
    "garch_t = arch_model(scaled_data, vol='Garch', p=1, q=1, dist='t').fit(disp=\"off\")\n",
    "\n",
    "# Skewed Student's t-distribution\n",
    "garch_skewt = arch_model(scaled_data, vol='Garch', p=1, q=1, dist='skewt').fit(disp=\"off\")\n",
    "\n",
    "# Print model summaries\n",
    "print(\"Normal Distribution GARCH:\")\n",
    "print(garch_normal.summary())\n",
    "\n",
    "print(\"\\nStudent's t-Distribution GARCH:\")\n",
    "print(garch_t.summary())\n",
    "\n",
    "print(\"\\nSkewed t-Distribution GARCH:\")\n",
    "print(garch_skewt.summary())"
   ],
   "id": "743e6a5bd3ed2ba3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 2: Compare the Distributions Using AIC\n",
    "\n",
    "The AIC (Akaike Information Criterion) can be used to compare the different models. The model with the lowest AIC is considered the best fit."
   ],
   "id": "6a5ecd7aee833fdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare AIC values for the different models\n",
    "print(f\"Normal AIC: {garch_normal.aic}\")\n",
    "print(f\"Student's t AIC: {garch_t.aic}\")\n",
    "print(f\"Skewed t AIC: {garch_skewt.aic}\")"
   ],
   "id": "294fe24131092de",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\t•\tThe Student’s t-distribution is likely capturing the fat tails better, which means that your data contains\n",
    "\t more extreme values than a normal distribution would predict.\n",
    "\t Since the Student’s t-distribution GARCH model has the best fit, let’s move forward with a deeper analysis. Here’s a breakdown of what we can do next:\n",
    "\n",
    "1. Residual Analysis for the Student’s t-Distribution GARCH Model:\n",
    "\n",
    "\t•\tWe need to ensure that the residuals from the Student’s t-distribution GARCH model behave like white noise (i.e., no autocorrelation and are approximately normally distributed).\n",
    "\n",
    "2. Forecast Volatility Using the Student’s t-Distribution GARCH Model:\n",
    "\n",
    "\t•\tWe can also forecast future volatility using this model to see how well it captures future uncertainty."
   ],
   "id": "c85142322c8c2989"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the standardized residuals from the Student's t-distribution GARCH model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(garch_t.resid, label='Standardized Residuals (Student\\'s t GARCH)')\n",
    "plt.title('Standardized Residuals from Student\\'s t GARCH Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "8ef2eee00ca48a13",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 2: Test for Autocorrelation in the Residuals\n",
    "\n",
    "We’ll use the Ljung-Box test to check if there is any remaining autocorrelation in the residuals. Ideally, there should be no significant autocorrelation left after the GARCH model is applied."
   ],
   "id": "179d75f0204ce742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Test for autocorrelation in the residuals from the Student's t GARCH model\n",
    "ljung_box_test_t = acorr_ljungbox(garch_t.resid, lags=[10], return_df=True)\n",
    "\n",
    "# Print the Ljung-Box test result\n",
    "print(ljung_box_test_t)"
   ],
   "id": "26baaef6218f3c81",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Q-Q plot to check if the residuals are normally distributed\n",
    "stats.probplot(garch_t.resid, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals from Student\\'s t GARCH Model')\n",
    "plt.show()"
   ],
   "id": "3648c8911bf9df77",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forecast future volatility for the next 5 periods using the Student's t GARCH model\n",
    "forecast_t = garch_t.forecast(horizon=5)\n",
    "\n",
    "# Print forecasted variance\n",
    "print(\"Forecasted variance over the next 5 periods:\")\n",
    "print(forecast_t.variance[-1:])"
   ],
   "id": "38d41ccbd4d3ba3d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Normal\n",
    "Forecasted variance over next 5 periods:\n",
    "           h.1       h.2       h.3       h.4       h.5\n",
    "6240  0.136139  0.137133  0.138124  0.139113  0.140099\n",
    "\n",
    "t Student\n",
    "\n",
    "Forecasted variance over the next 5 periods:\n",
    "           h.1       h.2       h.3       h.4       h.5\n",
    "6240  0.129124  0.129916  0.130706  0.131496  0.132284"
   ],
   "id": "d1f70c308ca40c66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. Test for Stationarity (Augmented Dickey-Fuller Test)\n",
    "\n",
    "We’ll apply the Augmented Dickey-Fuller (ADF) test to the residuals to check if they are stationary."
   ],
   "id": "5bf523fb9d4b4f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform ADF test on GARCH residuals\n",
    "adf_test = adfuller(garch_t.resid)\n",
    "\n",
    "# Extract and print ADF test results\n",
    "adf_statistic = adf_test[0]\n",
    "adf_p_value = adf_test[1]\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {adf_p_value}')\n",
    "if adf_p_value < 0.05:\n",
    "    print('p-value is less than 0.05, the residuals are stationary.')"
   ],
   "id": "150ae68815610eff",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "# Perform Engle's ARCH test for heteroscedasticity on the residuals\n",
    "arch_test_resid = het_arch(garch_t.resid)\n",
    "\n",
    "# Extract the test statistic and p-value\n",
    "arch_stat_resid = arch_test_resid[0]\n",
    "arch_p_value_resid = arch_test_resid[1]\n",
    "\n",
    "print(f'ARCH Test Statistic: {arch_stat_resid}')\n",
    "print(f'p-value: {arch_p_value_resid}')\n",
    "if arch_p_value_resid > 0.05:\n",
    "    print(\"A p-value > 0.05 means that heteroscedasticity is no longer present in the residuals, indicating that GARCH has successfully reduced it.\")\n",
    "else:\n",
    "    print(\"GARCH fail to reduce heteroscedasticity\")\n",
    "    "
   ],
   "id": "2aedb73edb3016d3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A p-value > 0.05 means that heteroscedasticity is no longer present in the residuals, indicating that GARCH has successfully reduced it.",
   "id": "5321642c4d2db73a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let’s move forward with more advanced GARCH models, such as GJR-GARCH and EGARCH, to better capture the complexity of your data and reduce heteroscedasticity.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "\t1.\tGJR-GARCH Model: This model accounts for asymmetric volatility, meaning it can capture the fact that negative shocks might have a larger impact on volatility than positive ones.\n",
    "\t2.\tEGARCH Model: The Exponential GARCH model can handle leverage effects and asymmetry more effectively. It models the logarithm of the variance, allowing it to capture both positive and negative shocks in a flexible way.\n",
    "\n",
    "Step-by-Step Implementation:\n",
    "\n",
    "1. GJR-GARCH Model"
   ],
   "id": "f6630e34ebd98c7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from arch import arch_model\n",
    "\n",
    "# Fit a GJR-GARCH(1,1) model with Student's t-distribution\n",
    "gjr_garch_model = arch_model(scaled_data, vol='Garch', p=1, q=1, o=1, dist='t').fit(disp=\"off\")\n",
    "\n",
    "# Print the GJR-GARCH model summary\n",
    "print(gjr_garch_model.summary())\n",
    "\n",
    "# Extract residuals from the GJR-GARCH model\n",
    "gjr_garch_residuals = gjr_garch_model.resid\n",
    "\n",
    "# Perform Engle's ARCH test on the GJR-GARCH residuals to check for heteroscedasticity\n",
    "arch_test_gjr_resid = het_arch(gjr_garch_residuals)\n",
    "print(f\"GJR-GARCH Residuals - ARCH Test Statistic: {arch_test_gjr_resid[0]}, p-value: {arch_test_gjr_resid[1]}\")"
   ],
   "id": "3aca35bb17340922",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. EGARCH Model",
   "id": "5f25bced1a96d902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit an EGARCH(1,1) model with Student's t-distribution\n",
    "egarch_model = arch_model(scaled_data, vol='EGARCH', p=1, q=1, dist='t').fit(disp=\"off\")\n",
    "\n",
    "# Print the EGARCH model summary\n",
    "print(egarch_model.summary())\n",
    "\n",
    "# Extract residuals from the EGARCH model\n",
    "egarch_residuals = egarch_model.resid\n",
    "\n",
    "# Perform Engle's ARCH test on the EGARCH residuals to check for heteroscedasticity\n",
    "arch_test_egarch_resid = het_arch(egarch_residuals)\n",
    "print(f\"EGARCH Residuals - ARCH Test Statistic: {arch_test_egarch_resid[0]}, p-value: {arch_test_egarch_resid[1]}\")"
   ],
   "id": "13758af1e8af457",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using a Variational Autoencoder (VAE) to minimize variance can be a powerful approach to model time series data, especially for reducing volatility or variance in a latent space representation of the data. VAEs are generative models that aim to learn a probabilistic latent space from which data can be generated, and they can be used to create smoother or more regularized versions of time series data.\n",
    "\n",
    "Key Idea Behind Using a VAE:\n",
    "\n",
    "A VAE models the distribution of data points in a lower-dimensional latent space while enforcing a regularization constraint (usually using a Kullback-Leibler (KL) divergence term) to ensure the latent representations follow a known distribution (often a standard normal distribution). This can help in:\n",
    "\t•\tReducing volatility or variance in the time series data by mapping it to a smoother latent space.\n",
    "\t•\tGenerating new samples from the learned latent space, potentially with reduced variance.\n",
    "\n",
    "Steps for Using a VAE on Time Series Data to Minimize Variance:\n",
    "\n",
    "\t1.\tData Preparation: The same as with neural networks, we’ll prepare the time series data into sequences (lags).\n",
    "\t2.\tDefine the VAE Model: The VAE will have an encoder that maps the input time series data into a latent space, and a decoder that reconstructs the time series from the latent space.\n",
    "\t3.\tTrain the VAE: The VAE will minimize both the reconstruction loss (mean squared error between the original and reconstructed data) and the KL divergence (to ensure the latent variables follow a normal distribution).\n",
    "\t4.\tGenerate or Transform Data: Once trained, the VAE can generate new time series with minimized variance by regularizing the latent space.\n",
    "\n",
    "Step-by-Step Implementation:\n",
    "\n",
    "Step 1: Data Preparation"
   ],
   "id": "3a9d3d9e0620344d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a function to create sequences (lags) for time series data\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequence = data[i:i+seq_length]\n",
    "        sequences.append(sequence)\n",
    "    return torch.tensor(np.array(sequences), dtype=torch.float32)\n",
    "\n",
    "# Example data (use your residuals or transformed data)\n",
    "data = garch_t.resid.values\n",
    "\n",
    "# Normalize the data (you can choose to standardize or normalize)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Create sequences of data (lags)\n",
    "seq_length = 5  # Number of lagged observations to use\n",
    "X = create_sequences(data_scaled, seq_length)"
   ],
   "id": "e00305c9b8375944",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 2: Define the VAE Model\n",
    "\n",
    "We will define a simple Variational Autoencoder with an encoder and decoder. The encoder compresses the input data into a latent space, and the decoder reconstructs the data from the latent representation."
   ],
   "id": "8a8d88ca66678027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:31:45.978528Z",
     "start_time": "2024-09-17T16:31:45.966851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HomoscedasticTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(HomoscedasticTransformer, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)  # Mean of latent space\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size)  # Log variance of latent space\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std  # Sample from latent space\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Initialize the VAE\n",
    "input_size = seq_length\n",
    "hidden_size = 50\n",
    "latent_size = 10  # Latent space dimension\n",
    "vae_model = HomoscedasticTransformer(input_size, hidden_size, latent_size)"
   ],
   "id": "f9f9ad1139852476",
   "execution_count": 111,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 3: Loss Function and Training the VAE\n",
    "\n",
    "The VAE loss function consists of two parts:\n",
    "\n",
    "\t1.\tReconstruction Loss: Ensures the output matches the input (minimizing reconstruction error).\n",
    "\t2.\tKL Divergence: Ensures the latent space follows a standard normal distribution, regularizing the latent representation."
   ],
   "id": "7c4e078502cef1e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:31:58.162355Z",
     "start_time": "2024-09-17T16:31:57.625856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.MSELoss()(recon_x, x)  # Reconstruction loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL divergence\n",
    "    # Volatility reduction term: penalize high variance in the reconstructed data\n",
    "    # variance_penalty = torch.var(recon_x)\n",
    "    return recon_loss + kl_loss #+ 0.1*variance_penalty\n",
    "\n",
    "# Training settings\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    vae_model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    recon_x, mu, logvar = vae_model(X)\n",
    "    loss = vae_loss(recon_x, X, mu, logvar)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Variance loss:{torch.var(recon_x):.6f}')"
   ],
   "id": "fbe8ee3b0ba082d4",
   "execution_count": 112,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 4: Use the Trained VAE to Generate Smoothed Data\n",
    "\n",
    "Once trained, the VAE can generate new time series data by sampling from the latent space."
   ],
   "id": "bcdd79661fc93092"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:32:14.336591Z",
     "start_time": "2024-09-17T16:32:14.108879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the trained VAE to generate new (smoothed) data\n",
    "vae_model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_data, _, _ = vae_model(X)\n",
    "    reconstructed_data = reconstructed_data.numpy()\n",
    "\n",
    "# Reverse the normalization\n",
    "reconstructed_data_rescaled = scaler.inverse_transform(reconstructed_data)\n",
    "# reconstructed_data_rescaled = scaler.fit_transform(reconstructed_data)\n",
    "\n",
    "# Plot the original and reconstructed data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "zzz = data[-len(reconstructed_data):]\n",
    "reconstructed_data_rescaled_zero = reconstructed_data_rescaled[:,0]\n",
    "# plt.plot(data[-len(reconstructed_data):], label='Original')\n",
    "# plt.plot(reconstructed_data_rescaled.flatten(), label='Reconstructed (VAE)')\n",
    "plt.plot(reconstructed_data_rescaled[:,0], label='Reconstructed (VAE)')\n",
    "plt.legend()\n",
    "plt.title('Original vs Reconstructed Time Series (VAE)')\n",
    "plt.show()"
   ],
   "id": "32c93499043ac878",
   "execution_count": 113,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T10:31:10.692269Z",
     "start_time": "2024-09-17T10:31:10.647824Z"
    }
   },
   "cell_type": "code",
   "source": "np.savetxt(\"reconstructed_data_rescaled.txt\", reconstructed_data_rescaled)",
   "id": "4df3813b80cb1d",
   "execution_count": 85,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:32:24.799489Z",
     "start_time": "2024-09-17T16:32:24.766818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from termcolor import colored\n",
    "# Perform Engle's ARCH test for heteroscedasticity on the residuals\n",
    "arch_test_resid = het_arch(reconstructed_data_rescaled[:,0])\n",
    "\n",
    "# Extract the test statistic and p-value\n",
    "arch_stat_resid = arch_test_resid[0]\n",
    "arch_p_value_resid = arch_test_resid[1]\n",
    "\n",
    "print(f'ARCH Test Statistic: {arch_stat_resid}')\n",
    "print(f'p-value: {arch_p_value_resid:.4f}')\n",
    "if arch_p_value_resid > 0.05:\n",
    "    print(colored(\"A p-value > 0.05 means that heteroscedasticity is no longer present in the residuals, indicating that VAE has successfully reduced it.\",'red'))\n",
    "else:\n",
    "    print(f\"VAE fail to reduce heteroscedasticity\")"
   ],
   "id": "68162a00d6f50be5",
   "execution_count": 114,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "162ee81d5189f77f",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
