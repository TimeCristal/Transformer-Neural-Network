{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 11\n",
    "from kl.utils import load_fx\n",
    "X, y, returns = load_fx(data_start=0, data_end=5000, window_size=window_size, shift=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2_mean = nn.Linear(128, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        z_mean = self.fc2_mean(h)\n",
    "        z_logvar = self.fc2_logvar(h)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        x_recon = torch.sigmoid(self.fc2(h))  # Use sigmoid for binary data\n",
    "        return x_recon\n",
    "\n",
    "# Classifier network for latent space\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, latent_dim, num_classes):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(latent_dim, num_classes)\n",
    "# \n",
    "#     def forward(self, z):\n",
    "#         return self.fc1(z)\n",
    "\n",
    "# Classifier works on latent dimension !    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return self.fc3(h)  # No softmax, CrossEntropyLoss applies it    \n",
    "\n",
    "# VAE model combining Encoder, Decoder, and Classifier\n",
    "class DichotomyVAE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, latent_dim, num_classes):\n",
    "        super(DichotomyVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, output_dim)\n",
    "        self.classifier = Classifier(latent_dim, num_classes)\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encoder(x)\n",
    "        latent = self.reparameterize(z_mean, z_logvar)\n",
    "        x_recon = self.decoder(latent)\n",
    "        y_pred = self.classifier(latent)  # Predicted class labels from latent space\n",
    "        return x_recon, z_mean, z_logvar, y_pred, latent\n",
    "\n",
    "# Loss function for VAE and Classification\n",
    "def loss_function(x_recon, x, z_mean, z_logvar, y_pred, y, beta=1, lambda_class=1):\n",
    "    # Reconstruction loss (e.g., MSE or BCE)\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss         = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    \n",
    "    # Reverse KL divergence loss (hypothetical for monitoring)\n",
    "    prior_mean = torch.zeros_like(z_mean)\n",
    "    prior_logvar = torch.zeros_like(z_logvar)\n",
    "    kl_reverse = -0.5 * torch.sum(1 + prior_logvar - prior_mean.pow(2) - prior_logvar.exp())\n",
    "  \n",
    "\n",
    "    \n",
    "    # Classification loss (cross-entropy)\n",
    "    class_loss = nn.functional.cross_entropy(y_pred, y)\n",
    "    \n",
    "    # Total loss\n",
    "    # total_loss = recon_loss + 1/beta * kl_loss + 1/lambda_class * class_loss\n",
    "    total_loss = recon_loss + beta * kl_loss + lambda_class * class_loss\n",
    "    return total_loss, recon_loss, kl_loss, class_loss, kl_reverse\n",
    "\n",
    "# Training loop\n",
    "def train_vae(model, dataloader, optimizer, scheduler, num_epochs=10, beta=1, lambda_class=1):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z_mean, z_logvar, y_pred, latent = model(x)\n",
    "            # beta = min(1, epoch / 100)+0.1  # Gradually increase beta up to 1 over the first 100 epochs\n",
    "            # lambda_class = 1+epoch/10\n",
    "            \n",
    "            total_loss, recon_loss, kl_loss, class_loss, kl_reverse = loss_function(x_recon, x, z_mean, z_logvar, y_pred, y, beta, lambda_class)\n",
    "            loss = class_loss \n",
    "            # loss = total_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}, kl: {kl_loss:.4f}, class: {class_loss:.4f}, lr: {scheduler.get_last_lr()[0]:.6f}, lambda: {lambda_class:.2f}, kl_reverse: {kl_reverse:.4f}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader.dataset)}')\n",
    "        scheduler.step()\n",
    "# Example usage\n",
    "input_dim = X.shape[1]\n",
    "vae_model = DichotomyVAE(input_dim=input_dim, output_dim = input_dim, latent_dim=2, num_classes=2)  # Assuming 4 features, 2 classes\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=400, T_mult=1)\n",
    "scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer,total_iters=5000)"
   ],
   "id": "eaf886bd8b4a6e67",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you already have your data loaded as X, y\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model with the dataloader\n",
    "train_vae(vae_model, dataloader, optimizer, scheduler, num_epochs=5000, beta=1, lambda_class=1)"
   ],
   "id": "4cef58c3a3215727",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def infer_vae(model, x_new):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        # Encode the input to get latent vector z\n",
    "        z_mean, z_logvar = model.encoder(x_new)\n",
    "        latent = model.reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Reconstruct the input from latent vector z\n",
    "        x_reconstructed = model.decoder(latent)\n",
    "        \n",
    "        # Predict class from the latent vector z (optional)\n",
    "        y_pred = model.classifier(latent)\n",
    "        predicted_class = torch.argmax(y_pred, dim=1)  # Get the class with highest probability\n",
    "        \n",
    "        return x_reconstructed, predicted_class, latent\n",
    "\n",
    "# Example usage\n",
    "X_new, y_new, returns_new = load_fx(data_start=0, window_size=window_size, data_end=6000, shift=1)\n",
    "# X_new = X_new[0:6055]\n",
    "# y_new = y_new[0:6055]\n",
    "X_new_tensor = torch.tensor(X_new, dtype=torch.float32)  # X_new is your new input data\n",
    "x_reconstructed, predicted_class, latent = infer_vae(vae_model, X_new_tensor)\n",
    "\n",
    "# print(\"Reconstructed Data:\", x_reconstructed)\n",
    "# print(\"Predicted Class:\", predicted_class)"
   ],
   "id": "d036742e66bbe1b5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predicted_class_value = predicted_class.detach().numpy().astype(np.float32)\n",
    "print(f'Class Mean: {np.mean(predicted_class_value)}')"
   ],
   "id": "f9588a5f39d3f5e4",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "x_reconstructed_value = x_reconstructed.detach().numpy()\n",
    "np.savetxt(\"x_reconstructed_value.txt\", x_reconstructed_value)"
   ],
   "id": "c4ec16a6ddd6c282",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.savetxt(\"latent.txt\", latent)",
   "id": "c848a8bddefd8118",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.savetxt(\"y.txt\", y_new)\n",
    "np.savetxt(\"returns.txt\", returns_new)"
   ],
   "id": "de84b81c2355d115",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "acc = accuracy_score(y_new, predicted_class_value)\n",
    "print(f\"Accuracy: {acc}\")"
   ],
   "id": "b8b283dcfc7b527b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pacmap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading preprocessed coil_20 dataset\n",
    "# you can change it with any dataset that is in the ndarray format, with the shape (N, D)\n",
    "# where N is the number of samples and D is the dimension of each sample\n",
    "Xr = x_reconstructed_value\n",
    "Xr = Xr.reshape(X_new.shape[0], -1)\n",
    "# y = np.load(\"./data/coil_20_labels.npy\", allow_pickle=True)\n",
    "\n",
    "# initializing the pacmap instance\n",
    "# Setting n_neighbors to \"None\" leads to an automatic choice shown below in \"parameter\" section\n",
    "embedding = pacmap.PaCMAP(n_components=2, n_neighbors=10, MN_ratio=0.5, FP_ratio=2.0) \n",
    "\n",
    "# fit the data (The index of transformed data corresponds to the index of the original data)\n",
    "X_transformed = embedding.fit_transform(Xr, init=\"pca\")\n",
    "# X_transformed = embedding.fit_transform(Xr, init=None)\n",
    "\n",
    "# visualize the embedding\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.scatter(X_transformed[:, 0], X_transformed[:, 1], cmap=\"autumn\", c=y_new, s=20)\n"
   ],
   "id": "bf78a9162a04eeb7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.savetxt('X_transformed.txt', X_transformed)",
   "id": "85cd90069223794d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import colormaps\n",
    "list(colormaps)"
   ],
   "id": "2df1e6d80d93a976",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "98ddab3930a7aa5c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\t3.\tWeighted Loss Functions:\n",
    "\t•\tYou can assign weights to the loss function during training, giving more importance to low-variance regions or down-weighting high-variance areas. This helps the network focus on general trends rather than noise."
   ],
   "id": "ae0673e55dc312a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
