{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.829701Z",
     "start_time": "2024-09-17T23:26:49.307478Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 11\n",
    "from kl.utils import load_fx\n",
    "X, y, returns = load_fx(data_start=0, data_end=5000, window_size=window_size, shift=1)"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.834752Z",
     "start_time": "2024-09-17T23:26:50.831278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Check for available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for GPU acceleration\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon) for GPU acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ],
   "id": "73cb33fa30a2481b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon) for GPU acceleration\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.840464Z",
     "start_time": "2024-09-17T23:26:50.836073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# SVM-like classifier (linear model)\n",
    "class SVMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)  # Linear classifier with one output for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # No activation, similar to SVM's linear decision boundary\n",
    "\n",
    "# Hinge loss function (for SVM-like behavior)\n",
    "def hinge_loss_fn(y_pred, y_true):\n",
    "    return torch.mean(torch.clamp(1 - y_pred * y_true, min=0))"
   ],
   "id": "c4c88cff894147c2",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.852170Z",
     "start_time": "2024-09-17T23:26:50.842613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hinge_loss\n",
    "from kl.utils import load_fx\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)  # Compute standard deviation from log variance\n",
    "    eps = torch.randn_like(std)     # Sample from standard normal distribution\n",
    "    return mu + eps * std  # Reparameterization: z = mu + sigma * epsilon\n",
    "\n",
    "\n",
    "# Define the quantum circuit for 8 qubits (one per feature)\n",
    "def quantum_circuit(params, x):\n",
    "    n_qubits = len(x)  # Ensure we are only working with 8 qubits\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(x[i], wires=i)  # Apply RX to qubit i with the i-th feature\n",
    "        qml.RY(params[0], wires=i)\n",
    "        qml.RZ(params[1], wires=i)\n",
    "    \n",
    "    # Measure the expectation value of Pauli-Z on all qubits\n",
    "    # Return only a single vector of length 8 (one value per qubit)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumEncoder(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(QuantumEncoder, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self.qcircuit = qml.QNode(quantum_circuit, self.dev)\n",
    "\n",
    "        # Layers to output mean and log variance\n",
    "        self.fc_mu = nn.Linear(n_qubits, n_qubits)\n",
    "        self.fc_logvar = nn.Linear(n_qubits, n_qubits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        params = torch.randn(2).to(x.device)  # Ensure params are on the same device as input\n",
    "        \n",
    "        for sample in x:\n",
    "            sample = sample.detach().cpu().numpy()  # Still converting to NumPy for PennyLane\n",
    "            output = self.qcircuit(params, sample)\n",
    "            outputs.append(np.array(output))\n",
    "        \n",
    "        outputs_np = np.stack(outputs, axis=0)\n",
    "\n",
    "        # Ensure latent_vector is moved to the correct device\n",
    "        latent_vector = torch.tensor(outputs_np, dtype=torch.float32).to(x.device)\n",
    "        \n",
    "        mu = self.fc_mu(latent_vector)\n",
    "        log_var = self.fc_logvar(latent_vector)\n",
    "\n",
    "        return mu, log_var\n",
    "    \n",
    "    \n",
    "# Define the classical decoder\n",
    "class ClassicalDecoder(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(ClassicalDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_qubits, 128)  # Change input size from 1 to 8 to match latent vector size\n",
    "        self.fc2 = nn.Linear(128, n_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decode the quantum output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "f7f01e9341c05564",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.857329Z",
     "start_time": "2024-09-17T23:26:50.853405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuantumVAE(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(QuantumVAE, self).__init__()\n",
    "        self.encoder = QuantumEncoder(n_qubits, n_features)  # Quantum encoder\n",
    "        self.decoder = ClassicalDecoder(n_qubits, n_features)  # Classical decoder\n",
    "        self.classifier = SVMClassifier(n_qubits)  # Embedded SVM classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and log variance from the encoder\n",
    "        mu, log_var = self.encoder(x)\n",
    "\n",
    "        # Reparameterization trick to sample latent vector z\n",
    "        latent = reparameterize(mu, log_var)\n",
    "\n",
    "        # Get reconstructed data and SVM classification output\n",
    "        reconstructed_x = self.decoder(latent)\n",
    "        classification_output = self.classifier(latent)\n",
    "\n",
    "        # Return latent vector, reconstructed data, classification output, and parameters for KL divergence\n",
    "        return latent, reconstructed_x, classification_output, mu, log_var"
   ],
   "id": "d2ec3d9c3431c2d8",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.861722Z",
     "start_time": "2024-09-17T23:26:50.858670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_class_variance(latent_vectors, labels):\n",
    "    # Calculate variance of latent vectors for each class\n",
    "    unique_labels = torch.unique(labels)\n",
    "    class_variances = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        class_latents = latent_vectors[labels == label]\n",
    "        class_variance = torch.var(class_latents, dim=0).mean()  # Average variance across all latent dimensions\n",
    "        class_variances[label.item()] = class_variance\n",
    "    \n",
    "    return class_variances"
   ],
   "id": "6361265caa90451",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:50.868491Z",
     "start_time": "2024-09-17T23:26:50.862793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_vae_with_svm(vae, dataloader, optimizer, n_epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x, labels = batch\n",
    "            x, labels = x.to(device), labels.to(device)  # Move batch data to device\n",
    "\n",
    "            # Forward pass\n",
    "            latent_vectors, reconstructed_x, svm_predictions, mu, log_var = vae(x)\n",
    "\n",
    "            # Compute variance for each class\n",
    "            class_variances = compute_class_variance(latent_vectors, labels)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_loss = nn.MSELoss()(reconstructed_x, x)\n",
    "\n",
    "            # KL divergence\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "            # SVM loss\n",
    "            svm_loss = 0\n",
    "            total_variance = sum(class_variances.values())  # Total variance normalization\n",
    "\n",
    "            for label in class_variances:\n",
    "                variance_weight = class_variances[label]\n",
    "                class_indices = (labels == label).unsqueeze(1).bool()\n",
    "                svm_loss += (variance_weight / total_variance) * hinge_loss_fn(svm_predictions[class_indices], class_indices.float())\n",
    "\n",
    "            beta = min(1.0, epoch / 10)  # Gradually increase beta over epochs\n",
    "            # Total loss\n",
    "            total_batch_loss = recon_loss + beta * kl_divergence + 0.1 * svm_loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += total_batch_loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(dataloader):.4f}, KL: {kl_divergence.item():.4f}, SVM Loss: {svm_loss:.4f}, Reconstruction Loss: {recon_loss:.4f}')"
   ],
   "id": "1e9e589a7add710c",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T23:26:51.641303Z",
     "start_time": "2024-09-17T23:26:50.869643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Quantum VAE\n",
    "vae_model = QuantumVAE(n_qubits=X.shape[1], n_features=X.shape[1])  # Adjust the number of qubits and features based on your data\n",
    "\n",
    "# Move the VAE model to the appropriate device\n",
    "vae_model = vae_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Example DataLoader (assuming you have X and y)\n",
    "dataloader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "\n",
    "# # Try a larger batch size to better utilize GPU\n",
    "# dataloader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)), batch_size=128, shuffle=True)\n",
    "\n",
    "# Train the VAE with the embedded SVM classifier\n",
    "train_vae_with_svm(vae_model, dataloader, optimizer, n_epochs=100)"
   ],
   "id": "de1726a7fd4fd268",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 17\u001B[0m\n\u001B[1;32m     11\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(TensorDataset(torch\u001B[38;5;241m.\u001B[39mtensor(X, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32), torch\u001B[38;5;241m.\u001B[39mtensor(y, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# # Try a larger batch size to better utilize GPU\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# dataloader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)), batch_size=128, shuffle=True)\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Train the VAE with the embedded SVM classifier\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[43mtrain_vae_with_svm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvae_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[66], line 11\u001B[0m, in \u001B[0;36mtrain_vae_with_svm\u001B[0;34m(vae, dataloader, optimizer, n_epochs)\u001B[0m\n\u001B[1;32m      8\u001B[0m x, labels \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Move batch data to device\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m latent_vectors, reconstructed_x, svm_predictions, mu, log_var \u001B[38;5;241m=\u001B[39m \u001B[43mvae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Compute variance for each class\u001B[39;00m\n\u001B[1;32m     14\u001B[0m class_variances \u001B[38;5;241m=\u001B[39m compute_class_variance(latent_vectors, labels)\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[64], line 10\u001B[0m, in \u001B[0;36mQuantumVAE.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# Get mean and log variance from the encoder\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     mu, log_var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# Reparameterization trick to sample latent vector z\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     latent \u001B[38;5;241m=\u001B[39m reparameterize(mu, log_var)\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[63], line 48\u001B[0m, in \u001B[0;36mQuantumEncoder.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m x:\n\u001B[1;32m     47\u001B[0m     sample \u001B[38;5;241m=\u001B[39m sample\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()  \u001B[38;5;66;03m# Still converting to NumPy for PennyLane\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqcircuit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     outputs\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39marray(output))\n\u001B[1;32m     51\u001B[0m outputs_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(outputs, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/workflow/qnode.py:1020\u001B[0m, in \u001B[0;36mQNode.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m qml\u001B[38;5;241m.\u001B[39mcapture\u001B[38;5;241m.\u001B[39menabled():\n\u001B[1;32m   1019\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m qml\u001B[38;5;241m.\u001B[39mcapture\u001B[38;5;241m.\u001B[39mqnode_call(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1020\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_impl_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/workflow/qnode.py:1008\u001B[0m, in \u001B[0;36mQNode._impl_call\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_gradient_fn(shots\u001B[38;5;241m=\u001B[39moverride_shots, tape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tape)\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1008\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_component\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverride_shots\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverride_shots\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1010\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m old_interface \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/workflow/qnode.py:957\u001B[0m, in \u001B[0;36mQNode._execution_component\u001B[0;34m(self, args, kwargs, override_shots)\u001B[0m\n\u001B[1;32m    951\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mfilterwarnings(\n\u001B[1;32m    952\u001B[0m         action\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    953\u001B[0m         message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.*argument is deprecated and will be removed in version 0.39.*\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    954\u001B[0m         category\u001B[38;5;241m=\u001B[39mqml\u001B[38;5;241m.\u001B[39mPennyLaneDeprecationWarning,\n\u001B[1;32m    955\u001B[0m     )\n\u001B[1;32m    956\u001B[0m     \u001B[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001B[39;00m\n\u001B[0;32m--> 957\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mqml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterface\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterface\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransform_program\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfull_transform_program\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43minner_transform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minner_transform_program\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m        \u001B[49m\u001B[43moverride_shots\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverride_shots\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mexecute_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    969\u001B[0m res \u001B[38;5;241m=\u001B[39m res[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    971\u001B[0m \u001B[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/workflow/execution.py:660\u001B[0m, in \u001B[0;36mexecute\u001B[0;34m(tapes, device, gradient_fn, interface, transform_program, inner_transform, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp, mcm_config)\u001B[0m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001B[39;00m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m no_interface_boundary_required:\n\u001B[0;32m--> 660\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43minner_execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m post_processing(results)\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    664\u001B[0m     device_vjp\n\u001B[1;32m    665\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(device, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshort_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlightning.gpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlightning.kokkos\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    666\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m interface \u001B[38;5;129;01min\u001B[39;00m jpc_interfaces\n\u001B[1;32m    667\u001B[0m ):  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/workflow/execution.py:212\u001B[0m, in \u001B[0;36m_make_inner_execute.<locals>.inner_execute\u001B[0;34m(tapes, **_)\u001B[0m\n\u001B[1;32m    209\u001B[0m transformed_tapes, transform_post_processing \u001B[38;5;241m=\u001B[39m transform_program(tapes)\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_tapes:\n\u001B[0;32m--> 212\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mdevice\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_tapes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexecution_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    214\u001B[0m     results \u001B[38;5;241m=\u001B[39m ()\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/modifiers/simulator_tracking.py:30\u001B[0m, in \u001B[0;36m_track_execute.<locals>.execute\u001B[0;34m(self, circuits, execution_config)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(untracked_execute)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecute\u001B[39m(\u001B[38;5;28mself\u001B[39m, circuits, execution_config\u001B[38;5;241m=\u001B[39mDefaultExecutionConfig):\n\u001B[0;32m---> 30\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43muntracked_execute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcircuits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(circuits, QuantumScript):\n\u001B[1;32m     32\u001B[0m         batch \u001B[38;5;241m=\u001B[39m (circuits,)\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/modifiers/single_tape_support.py:32\u001B[0m, in \u001B[0;36m_make_execute.<locals>.execute\u001B[0;34m(self, circuits, execution_config)\u001B[0m\n\u001B[1;32m     30\u001B[0m     is_single_circuit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     circuits \u001B[38;5;241m=\u001B[39m (circuits,)\n\u001B[0;32m---> 32\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_execute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcircuits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m is_single_circuit \u001B[38;5;28;01melse\u001B[39;00m results\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/logging/decorators.py:61\u001B[0m, in \u001B[0;36mlog_string_debug_func.<locals>.wrapper_entry\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     54\u001B[0m     s_caller \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m::L\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m     55\u001B[0m         [\u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39mgetouterframes(inspect\u001B[38;5;241m.\u001B[39mcurrentframe(), \u001B[38;5;241m2\u001B[39m)[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m3\u001B[39m]]\n\u001B[1;32m     56\u001B[0m     )\n\u001B[1;32m     57\u001B[0m     lgr\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms_caller\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_debug_log_kwargs,\n\u001B[1;32m     60\u001B[0m     )\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:630\u001B[0m, in \u001B[0;36mDefaultQubit.execute\u001B[0;34m(self, circuits, execution_config)\u001B[0m\n\u001B[1;32m    627\u001B[0m prng_keys \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_prng_keys()[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(circuits))]\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_workers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_simulate_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m            \u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrng\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdebugger\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_debugger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minterface\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minterface\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstate_cache\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_state_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprng_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmcm_method\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcm_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcm_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpostselect_mode\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcm_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpostselect_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_key\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcircuits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprng_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    644\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    646\u001B[0m vanilla_circuits \u001B[38;5;241m=\u001B[39m convert_to_numpy_parameters(circuits)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    647\u001B[0m seeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rng\u001B[38;5;241m.\u001B[39mintegers(\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m31\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(vanilla_circuits))\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:631\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    627\u001B[0m prng_keys \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_prng_keys()[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(circuits))]\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_workers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(\n\u001B[0;32m--> 631\u001B[0m         \u001B[43m_simulate_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m            \u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrng\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdebugger\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_debugger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minterface\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minterface\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstate_cache\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_state_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprng_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmcm_method\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcm_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcm_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpostselect_mode\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecution_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcm_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpostselect_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m c, _key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(circuits, prng_keys)\n\u001B[1;32m    644\u001B[0m     )\n\u001B[1;32m    646\u001B[0m vanilla_circuits \u001B[38;5;241m=\u001B[39m convert_to_numpy_parameters(circuits)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    647\u001B[0m seeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rng\u001B[38;5;241m.\u001B[39mintegers(\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m31\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(vanilla_circuits))\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:896\u001B[0m, in \u001B[0;36m_simulate_wrapper\u001B[0;34m(circuit, kwargs)\u001B[0m\n\u001B[1;32m    895\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_simulate_wrapper\u001B[39m(circuit, kwargs):\n\u001B[0;32m--> 896\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msimulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcircuit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/logging/decorators.py:61\u001B[0m, in \u001B[0;36mlog_string_debug_func.<locals>.wrapper_entry\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     54\u001B[0m     s_caller \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m::L\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m     55\u001B[0m         [\u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39mgetouterframes(inspect\u001B[38;5;241m.\u001B[39mcurrentframe(), \u001B[38;5;241m2\u001B[39m)[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m3\u001B[39m]]\n\u001B[1;32m     56\u001B[0m     )\n\u001B[1;32m     57\u001B[0m     lgr\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms_caller\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_debug_log_kwargs,\n\u001B[1;32m     60\u001B[0m     )\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/qubit/simulate.py:379\u001B[0m, in \u001B[0;36msimulate\u001B[0;34m(circuit, debugger, state_cache, **execution_kwargs)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(results)\n\u001B[1;32m    378\u001B[0m ops_key, meas_key \u001B[38;5;241m=\u001B[39m jax_random_split(prng_key)\n\u001B[0;32m--> 379\u001B[0m state, is_state_batched \u001B[38;5;241m=\u001B[39m \u001B[43mget_final_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcircuit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebugger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprng_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mops_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mexecution_kwargs\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m state_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    383\u001B[0m     state_cache[circuit\u001B[38;5;241m.\u001B[39mhash] \u001B[38;5;241m=\u001B[39m state\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/logging/decorators.py:61\u001B[0m, in \u001B[0;36mlog_string_debug_func.<locals>.wrapper_entry\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     54\u001B[0m     s_caller \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m::L\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m     55\u001B[0m         [\u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39mgetouterframes(inspect\u001B[38;5;241m.\u001B[39mcurrentframe(), \u001B[38;5;241m2\u001B[39m)[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m3\u001B[39m]]\n\u001B[1;32m     56\u001B[0m     )\n\u001B[1;32m     57\u001B[0m     lgr\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms_caller\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_debug_log_kwargs,\n\u001B[1;32m     60\u001B[0m     )\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/qubit/simulate.py:208\u001B[0m, in \u001B[0;36mget_final_state\u001B[0;34m(circuit, debugger, **execution_kwargs)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(op, MidMeasureMP):\n\u001B[1;32m    207\u001B[0m     prng_key, key \u001B[38;5;241m=\u001B[39m jax_random_split(prng_key)\n\u001B[0;32m--> 208\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[43mapply_operation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_state_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_state_batched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdebugger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebugger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprng_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtape_shots\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcircuit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshots\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mexecution_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;66;03m# Handle postselection on mid-circuit measurements\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(op, qml\u001B[38;5;241m.\u001B[39mProjector):\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/functools.py:889\u001B[0m, in \u001B[0;36msingledispatch.<locals>.wrapper\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    886\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfuncname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires at least \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    887\u001B[0m                     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1 positional argument\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 889\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/qubit/apply_operation.py:218\u001B[0m, in \u001B[0;36mapply_operation\u001B[0;34m(op, state, is_state_batched, debugger, **_)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;129m@singledispatch\u001B[39m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_operation\u001B[39m(\n\u001B[1;32m    154\u001B[0m     op: qml\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mOperator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_,\n\u001B[1;32m    159\u001B[0m ):\n\u001B[1;32m    160\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Apply and operator to a given state.\u001B[39;00m\n\u001B[1;32m    161\u001B[0m \n\u001B[1;32m    162\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m \n\u001B[1;32m    217\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_apply_operation_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_state_batched\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebugger\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/qubit/apply_operation.py:228\u001B[0m, in \u001B[0;36m_apply_operation_default\u001B[0;34m(op, state, is_state_batched, debugger)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"The default behaviour of apply_operation, accessed through the standard dispatch\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03mof apply_operation, as well as conditionally in other dispatches.\"\"\"\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28mlen\u001B[39m(op\u001B[38;5;241m.\u001B[39mwires) \u001B[38;5;241m<\u001B[39m EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m math\u001B[38;5;241m.\u001B[39mndim(state) \u001B[38;5;241m<\u001B[39m EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001B[1;32m    227\u001B[0m ) \u001B[38;5;129;01mor\u001B[39;00m (op\u001B[38;5;241m.\u001B[39mbatch_size \u001B[38;5;129;01mand\u001B[39;00m is_state_batched):\n\u001B[0;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_operation_einsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_state_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_state_batched\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m apply_operation_tensordot(op, state, is_state_batched\u001B[38;5;241m=\u001B[39mis_state_batched)\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/devices/qubit/apply_operation.py:102\u001B[0m, in \u001B[0;36mapply_operation_einsum\u001B[0;34m(op, state, is_state_batched)\u001B[0m\n\u001B[1;32m     99\u001B[0m         op\u001B[38;5;241m.\u001B[39m_batch_size \u001B[38;5;241m=\u001B[39m batch_size  \u001B[38;5;66;03m# pylint:disable=protected-access\u001B[39;00m\n\u001B[1;32m    100\u001B[0m reshaped_mat \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39mreshape(mat, new_mat_shape)\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43meinsum_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreshaped_mat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/math/multi_dispatch.py:554\u001B[0m, in \u001B[0;36meinsum\u001B[0;34m(indices, like, optimize, *operands)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m like \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    553\u001B[0m     like \u001B[38;5;241m=\u001B[39m get_interface(\u001B[38;5;241m*\u001B[39moperands)\n\u001B[0;32m--> 554\u001B[0m operands \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoerce\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperands\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlike\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlike\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m optimize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m like \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;66;03m# torch einsum doesn't support the optimize keyword argument\u001B[39;00m\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39meinsum(indices, \u001B[38;5;241m*\u001B[39moperands, like\u001B[38;5;241m=\u001B[39mlike)\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/autoray/autoray.py:81\u001B[0m, in \u001B[0;36mdo\u001B[0;34m(fn, like, *args, **kwargs)\u001B[0m\n\u001B[1;32m     79\u001B[0m backend \u001B[38;5;241m=\u001B[39m _choose_backend(fn, args, kwargs, like\u001B[38;5;241m=\u001B[39mlike)\n\u001B[1;32m     80\u001B[0m func \u001B[38;5;241m=\u001B[39m get_lib_fn(backend, fn)\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/math/single_dispatch.py:647\u001B[0m, in \u001B[0;36m_coerce_types_torch\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    645\u001B[0m     dev_indices\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    646\u001B[0m     dev_id \u001B[38;5;241m=\u001B[39m dev_indices\u001B[38;5;241m.\u001B[39mpop()\n\u001B[0;32m--> 647\u001B[0m     tensors \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    648\u001B[0m         torch\u001B[38;5;241m.\u001B[39mas_tensor(t, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdev_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    649\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tensors  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[1;32m    650\u001B[0m     ]\n\u001B[1;32m    651\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    652\u001B[0m     device \u001B[38;5;241m=\u001B[39m device_set\u001B[38;5;241m.\u001B[39mpop()\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/pennylane/math/single_dispatch.py:648\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    645\u001B[0m     dev_indices\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    646\u001B[0m     dev_id \u001B[38;5;241m=\u001B[39m dev_indices\u001B[38;5;241m.\u001B[39mpop()\n\u001B[1;32m    647\u001B[0m     tensors \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 648\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdev_id\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    649\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tensors  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[1;32m    650\u001B[0m     ]\n\u001B[1;32m    651\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    652\u001B[0m     device \u001B[38;5;241m=\u001B[39m device_set\u001B[38;5;241m.\u001B[39mpop()\n",
      "File \u001B[0;32m~/PycharmProjects/InformerBayes/attentionTransformers/lib/python3.10/site-packages/torch/cuda/__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    309\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def inference(vae, x):\n",
    "    vae.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        x = x.to(device)  # Move the input data to the device\n",
    "        latent_vectors, reconstructed_x, svm_predictions, mu, log_var = vae(x)\n",
    "        \n",
    "        class_labels = (svm_predictions > 0).float()  # Assuming binary classification\n",
    "        return reconstructed_x, class_labels, latent_vectors, mu, log_var"
   ],
   "id": "29a1c65befa069c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test = X\n",
    "# Example input data for inference (can be a batch of new data)\n",
    "new_data = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Perform inference\n",
    "reconstructed_data, predicted_labels, latent_vectors, mu, log_var = inference(vae_model, new_data)"
   ],
   "id": "a432df607a6850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predicted_labels_np = predicted_labels.cpu().detach().numpy()\n",
    "reconstructed_data_np = reconstructed_data.cpu().detach().numpy()\n",
    "latent_vectors_np = latent_vectors.cpu().detach().numpy()"
   ],
   "id": "98efc87396124857",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.savetxt('reconstructed_data.txt', reconstructed_data_np)\n",
    "# Print results\n",
    "# print(\"Reconstructed Data:\\n\", reconstructed_data)\n",
    "# print(\"Predicted Labels:\\n\", predicted_labels)\n",
    "# print(\"Latent Vectors:\\n\", latent_vectors)"
   ],
   "id": "2ed5e54fb7e70b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2d5a133b9dd83fe3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
