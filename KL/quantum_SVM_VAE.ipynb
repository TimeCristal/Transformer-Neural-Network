{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.742823Z",
     "start_time": "2024-09-17T22:30:43.920647Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 11\n",
    "from kl.utils import load_fx\n",
    "X, y, returns = load_fx(data_start=0, data_end=5000, window_size=window_size, shift=1)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.749457Z",
     "start_time": "2024-09-17T22:30:45.745190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Check for available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for GPU acceleration\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon) for GPU acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ],
   "id": "73cb33fa30a2481b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon) for GPU acceleration\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.754371Z",
     "start_time": "2024-09-17T22:30:45.750503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# SVM-like classifier (linear model)\n",
    "class SVMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)  # Linear classifier with one output for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # No activation, similar to SVM's linear decision boundary\n",
    "\n",
    "# Hinge loss function (for SVM-like behavior)\n",
    "def hinge_loss_fn(y_pred, y_true):\n",
    "    return torch.mean(torch.clamp(1 - y_pred * y_true, min=0))"
   ],
   "id": "c4c88cff894147c2",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.770417Z",
     "start_time": "2024-09-17T22:30:45.756434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hinge_loss\n",
    "from kl.utils import load_fx\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)  # Compute standard deviation from log variance\n",
    "    eps = torch.randn_like(std)     # Sample from standard normal distribution\n",
    "    return mu + eps * std  # Reparameterization: z = mu + sigma * epsilon\n",
    "\n",
    "\n",
    "# Define the quantum circuit for 8 qubits (one per feature)\n",
    "def quantum_circuit(params, x):\n",
    "    n_qubits = len(x)  # Ensure we are only working with 8 qubits\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(x[i], wires=i)  # Apply RX to qubit i with the i-th feature\n",
    "        qml.RY(params[0], wires=i)\n",
    "        qml.RZ(params[1], wires=i)\n",
    "    \n",
    "    # Measure the expectation value of Pauli-Z on all qubits\n",
    "    # Return only a single vector of length 8 (one value per qubit)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumEncoder(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(QuantumEncoder, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Quantum circuit and device\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self.qcircuit = qml.QNode(quantum_circuit, self.dev)\n",
    "\n",
    "        # Additional layers to output mean and log variance for latent space\n",
    "        self.fc_mu = nn.Linear(n_qubits, n_qubits)      # Mean for latent space\n",
    "        self.fc_logvar = nn.Linear(n_qubits, n_qubits)  # Log variance for latent space\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        params = torch.randn(2)  # Parameters for the quantum circuit\n",
    "        \n",
    "        # Process each sample in the batch\n",
    "        for sample in x:\n",
    "            sample = sample.detach().cpu().numpy()\n",
    "            output = self.qcircuit(params, sample)\n",
    "            outputs.append(np.array(output))\n",
    "        \n",
    "        outputs_np = np.stack(outputs, axis=0)\n",
    "        \n",
    "        # Move latent_vector to the device where the model is located\n",
    "        latent_vector = torch.tensor(outputs_np, dtype=torch.float32).to(x.device)\n",
    "        \n",
    "        # Generate mean and log variance\n",
    "        mu = self.fc_mu(latent_vector)\n",
    "        log_var = self.fc_logvar(latent_vector)\n",
    "\n",
    "        return mu, log_var  # Output mean and log variance for reparameterization\n",
    "    \n",
    "# Define the classical decoder\n",
    "class ClassicalDecoder(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(ClassicalDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_qubits, 128)  # Change input size from 1 to 8 to match latent vector size\n",
    "        self.fc2 = nn.Linear(128, n_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decode the quantum output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "f7f01e9341c05564",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.777664Z",
     "start_time": "2024-09-17T22:30:45.773894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuantumVAE(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(QuantumVAE, self).__init__()\n",
    "        self.encoder = QuantumEncoder(n_qubits, n_features)  # Quantum encoder\n",
    "        self.decoder = ClassicalDecoder(n_qubits, n_features)  # Classical decoder\n",
    "        self.classifier = SVMClassifier(n_qubits)  # Embedded SVM classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and log variance from the encoder\n",
    "        mu, log_var = self.encoder(x)\n",
    "\n",
    "        # Reparameterization trick to sample latent vector z\n",
    "        latent = reparameterize(mu, log_var)\n",
    "\n",
    "        # Get reconstructed data and SVM classification output\n",
    "        reconstructed_x = self.decoder(latent)\n",
    "        classification_output = self.classifier(latent)\n",
    "\n",
    "        # Return latent vector, reconstructed data, classification output, and parameters for KL divergence\n",
    "        return latent, reconstructed_x, classification_output, mu, log_var"
   ],
   "id": "d2ec3d9c3431c2d8",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.781335Z",
     "start_time": "2024-09-17T22:30:45.778584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_class_variance(latent_vectors, labels):\n",
    "    # Calculate variance of latent vectors for each class\n",
    "    unique_labels = torch.unique(labels)\n",
    "    class_variances = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        class_latents = latent_vectors[labels == label]\n",
    "        class_variance = torch.var(class_latents, dim=0).mean()  # Average variance across all latent dimensions\n",
    "        class_variances[label.item()] = class_variance\n",
    "    \n",
    "    return class_variances"
   ],
   "id": "6361265caa90451",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T22:30:45.787654Z",
     "start_time": "2024-09-17T22:30:45.782372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_vae_with_svm(vae, dataloader, optimizer, n_epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            # Unpack batch data (features and labels)\n",
    "            x, labels = batch\n",
    "            x, labels = x.to(device), labels.to(device)  # Move data to the device\n",
    "\n",
    "            # Forward pass through VAE\n",
    "            latent_vectors, reconstructed_x, svm_predictions, mu, log_var = vae(x)\n",
    "\n",
    "            # Compute variance for each class in the latent space\n",
    "            class_variances = compute_class_variance(latent_vectors, labels)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_loss = nn.MSELoss()(reconstructed_x, x)\n",
    "\n",
    "            # KL divergence\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "            # Initialize total SVM loss\n",
    "            svm_loss = 0\n",
    "            for label in class_variances:\n",
    "                variance_weight = class_variances[label]\n",
    "                class_indices = (labels == label).unsqueeze(1).bool()  # Ensure class_indices is boolean\n",
    "                svm_loss += variance_weight * hinge_loss_fn(svm_predictions[class_indices], class_indices.float())\n",
    "\n",
    "            # Total loss (reconstruction + weighted KL divergence + SVM loss)\n",
    "            total_batch_loss = recon_loss + kl_divergence + 0.1 * svm_loss\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += total_batch_loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(dataloader):.4f}, KL: {kl_divergence.item():.4f}, SVM Loss: {svm_loss:.4f}')"
   ],
   "id": "1e9e589a7add710c",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-17T22:30:45.788625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Quantum VAE\n",
    "vae_model = QuantumVAE(n_qubits=X.shape[1], n_features=X.shape[1])  # Adjust the number of qubits and features based on your data\n",
    "\n",
    "# Move the VAE model to the appropriate device\n",
    "vae_model = vae_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Example DataLoader (assuming you have X and y)\n",
    "dataloader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the VAE with the embedded SVM classifier\n",
    "train_vae_with_svm(vae_model, dataloader, optimizer, n_epochs=100)"
   ],
   "id": "de1726a7fd4fd268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 47.6658, KL: 30.4958, SVM Loss: 2.2457\n",
      "Epoch [2/100], Loss: 19.3705, KL: 15.7063, SVM Loss: 2.0265\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def inference(vae, x):\n",
    "    vae.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        x = x.to(device)  # Move the input data to the device\n",
    "        latent_vectors, reconstructed_x, svm_predictions, mu, log_var = vae(x)\n",
    "        \n",
    "        class_labels = (svm_predictions > 0).float()  # Assuming binary classification\n",
    "        return reconstructed_x, class_labels, latent_vectors, mu, log_var"
   ],
   "id": "29a1c65befa069c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "X_test = X\n",
    "# Example input data for inference (can be a batch of new data)\n",
    "new_data = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Perform inference\n",
    "reconstructed_data, predicted_labels, latent_vectors, mu, log_var = inference(vae_model, new_data)"
   ],
   "id": "a432df607a6850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "predicted_labels_np = predicted_labels.cpu().detach().numpy()\n",
    "reconstructed_data_np = reconstructed_data.cpu().detach().numpy()\n",
    "latent_vectors_np = latent_vectors.cpu().detach().numpy()"
   ],
   "id": "98efc87396124857",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "np.savetxt('reconstructed_data.txt', reconstructed_data_np)\n",
    "# Print results\n",
    "# print(\"Reconstructed Data:\\n\", reconstructed_data)\n",
    "# print(\"Predicted Labels:\\n\", predicted_labels)\n",
    "# print(\"Latent Vectors:\\n\", latent_vectors)"
   ],
   "id": "2ed5e54fb7e70b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2d5a133b9dd83fe3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
