{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.397513Z",
     "start_time": "2024-09-18T01:21:01.059506Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 11\n",
    "from kl.utils import load_fx\n",
    "X, y, returns = load_fx(data_start=0, data_end=5000, window_size=window_size, shift=1)"
   ],
   "execution_count": 141,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.404322Z",
     "start_time": "2024-09-18T01:21:03.399340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Check for available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for GPU acceleration\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon) for GPU acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "# device = torch.device(\"cpu\")    "
   ],
   "id": "73cb33fa30a2481b",
   "execution_count": 142,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.410717Z",
     "start_time": "2024-09-18T01:21:03.405433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# SVM-like classifier (linear model)\n",
    "class SVMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)  # Linear classifier with one output for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # No activation, similar to SVM's linear decision boundary\n",
    "\n",
    "# Hinge loss function (for SVM-like behavior)\n",
    "def hinge_loss_fn(y_pred, y_true):\n",
    "    return torch.mean(torch.clamp(1 - y_pred * y_true, min=0))"
   ],
   "id": "c4c88cff894147c2",
   "execution_count": 143,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.432619Z",
     "start_time": "2024-09-18T01:21:03.413289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hinge_loss\n",
    "from kl.utils import load_fx\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)  # Compute standard deviation from log variance\n",
    "    eps = torch.randn_like(std)     # Sample from standard normal distribution\n",
    "    return mu + eps * std  # Reparameterization: z = mu + sigma * epsilon\n",
    "\n",
    "\n",
    "# Define the quantum circuit for 8 qubits (one per feature)\n",
    "def quantum_circuit(params, x):\n",
    "    n_qubits = len(x)  # Ensure we are only working with 8 qubits\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(x[i], wires=i)  # Apply RX to qubit i with the i-th feature\n",
    "        qml.RY(params[0], wires=i)\n",
    "        qml.RZ(params[1], wires=i)\n",
    "    \n",
    "    # Measure the expectation value of Pauli-Z on all qubits\n",
    "    # Return only a single vector of length 8 (one value per qubit)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumEncoder(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features, latent_dim):\n",
    "        super(QuantumEncoder, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "        self.latent_dim = latent_dim  # New latent space size\n",
    "        \n",
    "        # Quantum circuit and device (still tied to n_qubits)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self.qcircuit = qml.QNode(quantum_circuit, self.dev)\n",
    "\n",
    "        # Fully connected layers for bottleneck (reduce to latent_dim)\n",
    "        self.fc_mu = nn.Linear(n_qubits, latent_dim)      # Mean for latent space\n",
    "        self.fc_logvar = nn.Linear(n_qubits, latent_dim)  # Log variance for latent space\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        params = torch.randn(2).cpu().numpy()  # Parameters for quantum circuit\n",
    "        \n",
    "        for sample in x:\n",
    "            sample = sample.detach().cpu().numpy()  # Convert to NumPy for quantum processing\n",
    "            output = self.qcircuit(params, sample)\n",
    "            outputs.append(np.array(output))\n",
    "        \n",
    "        outputs_np = np.stack(outputs, axis=0)\n",
    "        \n",
    "        # Convert to tensor and move to appropriate device\n",
    "        latent_vector = torch.tensor(outputs_np, dtype=torch.float32).to(x.device)\n",
    "        \n",
    "        # Bottleneck layer: Reduce latent space to desired latent_dim\n",
    "        mu = self.fc_mu(latent_vector)  # Reduced to latent_dim size\n",
    "        log_var = self.fc_logvar(latent_vector)  # Reduced to latent_dim size\n",
    "\n",
    "        return mu, log_var\n",
    "    \n",
    "# Define the classical decoder\n",
    "class ClassicalDecoder(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(ClassicalDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_qubits, 128)  # Change input size from 1 to 8 to match latent vector size\n",
    "        self.fc2 = nn.Linear(128, n_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decode the quantum output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "f7f01e9341c05564",
   "execution_count": 144,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.437376Z",
     "start_time": "2024-09-18T01:21:03.433760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuantumVAE(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features, latent_dim):\n",
    "        super(QuantumVAE, self).__init__()\n",
    "        self.encoder = QuantumEncoder(n_qubits, n_features, latent_dim)  # Pass latent_dim to encoder\n",
    "        self.decoder = ClassicalDecoder(latent_dim, n_features)  # Use latent_dim in the decoder\n",
    "        self.classifier = SVMClassifier(latent_dim)  # Use latent_dim in the SVM classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get mean and log variance from the encoder\n",
    "        mu, log_var = self.encoder(x)\n",
    "\n",
    "        # Reparameterization trick to sample latent vector z\n",
    "        latent = reparameterize(mu, log_var)\n",
    "\n",
    "        # Get reconstructed data and SVM classification output\n",
    "        reconstructed_x = self.decoder(latent)\n",
    "        classification_output = self.classifier(latent)\n",
    "\n",
    "        # Return latent vector, reconstructed data, classification output, and parameters for KL divergence\n",
    "        return latent, reconstructed_x, classification_output, mu, log_var"
   ],
   "id": "d2ec3d9c3431c2d8",
   "execution_count": 145,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.441317Z",
     "start_time": "2024-09-18T01:21:03.438340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_class_variance(latent_vectors, labels):\n",
    "    # Calculate variance of latent vectors for each class\n",
    "    unique_labels = torch.unique(labels)\n",
    "    class_variances = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        class_latents = latent_vectors[labels == label]\n",
    "        class_variance = torch.var(class_latents, dim=0).mean()  # Average variance across all latent dimensions\n",
    "        class_variances[label.item()] = class_variance\n",
    "    \n",
    "    return class_variances"
   ],
   "id": "6361265caa90451",
   "execution_count": 146,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:21:03.447843Z",
     "start_time": "2024-09-18T01:21:03.442305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_vae_with_svm(vae, dataloader, optimizer, n_epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x, labels = batch\n",
    "            x, labels = x.to(device), labels.to(device)  # Move batch data to device\n",
    "\n",
    "            # Forward pass\n",
    "            latent_vectors, reconstructed_x, svm_predictions, mu, log_var = vae(x)\n",
    "\n",
    "            # Compute variance for each class\n",
    "            class_variances = compute_class_variance(latent_vectors, labels)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_loss = nn.MSELoss()(reconstructed_x, x)\n",
    "\n",
    "            # KL divergence\n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "            # SVM loss\n",
    "            svm_loss = 0\n",
    "            total_variance = sum(class_variances.values())  # Total variance normalization\n",
    "\n",
    "            for label in class_variances:\n",
    "                variance_weight = class_variances[label]\n",
    "                class_indices = (labels == label).unsqueeze(1).bool()\n",
    "                svm_loss += (variance_weight / total_variance) * hinge_loss_fn(svm_predictions[class_indices], class_indices.float())\n",
    "\n",
    "            beta = min(1.0, epoch / 10)  # Gradually increase beta over epochs\n",
    "            # Total loss\n",
    "            total_batch_loss = recon_loss + epoch * epoch * kl_divergence + 0.1 * svm_loss\n",
    "            # total_batch_loss =  svm_loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += total_batch_loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(dataloader):.4f}, KL: {kl_divergence.item():.6f}, SVM Loss: {svm_loss:.4f}, Reconstruction Loss: {recon_loss:.4f}')"
   ],
   "id": "1e9e589a7add710c",
   "execution_count": 147,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T01:24:51.971504Z",
     "start_time": "2024-09-18T01:21:03.448813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Quantum VAE\n",
    "vae_model = QuantumVAE(n_qubits=X.shape[1], n_features=X.shape[1], latent_dim=2)  # Adjust the number of qubits and features based on your data\n",
    "\n",
    "# Move the VAE model to the appropriate device\n",
    "vae_model = vae_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Example DataLoader (assuming you have X and y)\n",
    "dataloader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "\n",
    "# # Try a larger batch size to better utilize GPU\n",
    "# dataloader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)), batch_size=128, shuffle=True)\n",
    "\n",
    "# Train the VAE with the embedded SVM classifier\n",
    "train_vae_with_svm(vae_model, dataloader, optimizer, n_epochs=50)"
   ],
   "id": "de1726a7fd4fd268",
   "execution_count": 148,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def inference(vae, x):\n",
    "    vae.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        x = x.to(device)  # Move the input data to the device\n",
    "        latent_vectors, reconstructed_x, svm_predictions, mu, log_var = vae(x)\n",
    "        \n",
    "        class_labels = (svm_predictions > 0).float()  # Assuming binary classification\n",
    "        return reconstructed_x, class_labels, latent_vectors, mu, log_var"
   ],
   "id": "29a1c65befa069c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test = X\n",
    "# Example input data for inference (can be a batch of new data)\n",
    "new_data = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Perform inference\n",
    "reconstructed_data, predicted_labels, latent_vectors, mu, log_var = inference(vae_model, new_data)"
   ],
   "id": "a432df607a6850",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predicted_labels_np = predicted_labels.cpu().detach().numpy()\n",
    "reconstructed_data_np = reconstructed_data.cpu().detach().numpy()\n",
    "latent_vectors_np = latent_vectors.cpu().detach().numpy()"
   ],
   "id": "98efc87396124857",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.savetxt('reconstructed_data.txt', reconstructed_data_np)\n",
    "np.savetxt('latent_vectors.txt', latent_vectors_np)\n",
    "np.savetxt('predicted_labels.txt', predicted_labels_np)\n",
    "np.savetxt('y_np.txt', y)\n",
    "# Print results\n",
    "# print(\"Reconstructed Data:\\n\", reconstructed_data)\n",
    "# print(\"Predicted Labels:\\n\", predicted_labels)\n",
    "# print(\"Latent Vectors:\\n\", latent_vectors)"
   ],
   "id": "2ed5e54fb7e70b8d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "acc = accuracy_score(y, predicted_labels_np)\n",
    "print(f\"Accuracy: {acc}\")"
   ],
   "id": "2d5a133b9dd83fe3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8636d4710758812",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
