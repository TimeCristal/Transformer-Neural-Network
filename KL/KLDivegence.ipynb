{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:26:57.515244Z",
     "start_time": "2024-09-06T18:26:57.507972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "def load_fx(data_len):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"dataset/EURUSD_Daily_200005300000_202405300000.csv\", delimiter=\"\\t\")\n",
    "    \n",
    "    # Extract the closing prices\n",
    "    closing = df[\"<CLOSE>\"].iloc[0:data_len]\n",
    "    \n",
    "    # Parameters\n",
    "    window_size = 10  # Example window size\n",
    "    # test_size = 0.2   # Test set size\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # X_train = scaler.fit_transform(X_train)\n",
    "    shift = 2\n",
    "    print(f\"shift {shift}\")\n",
    "    # Create sliding window features and labels\n",
    "    X, y = [], []\n",
    "    for i in range(len(closing) - window_size):\n",
    "        window = closing[i:i + window_size].values\n",
    "        target = 1 if closing[i + window_size] > closing[i + window_size - shift] else 0\n",
    "        # Standardize the data\n",
    "        # features = scaler.fit_transform(window[:-1].reshape(-1, 1))\n",
    "        features = scaler.fit_transform(closing[i:i + window_size-1].diff().dropna().values.reshape(-1, 1))\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    \n",
    "    \n",
    "    X = np.array(X).squeeze()\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ],
   "id": "bd96ce19641b448d",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example true labels and predicted probabilities (after transformation)\n",
    "y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.2, 0.8, 0.3, 0.7, 0.4, 0.6])\n",
    "\n",
    "# Bin the predictions and true labels\n",
    "bins = np.linspace(0, 1, 10)\n",
    "\n",
    "# Plot the histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# True distribution histogram\n",
    "plt.hist(y_true[y_true == 0], bins=bins, alpha=0.5, label='Class 0 - True', color='blue')\n",
    "plt.hist(y_true[y_true == 1], bins=bins, alpha=0.5, label='Class 1 - True', color='green')\n",
    "\n",
    "# Predicted distribution histogram\n",
    "plt.hist(y_pred[y_true == 0], bins=bins, alpha=0.5, label='Class 0 - Predicted', color='red')\n",
    "plt.hist(y_pred[y_true == 1], bins=bins, alpha=0.5, label='Class 1 - Predicted', color='orange')\n",
    "\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Comparison of True vs. Predicted Distributions')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ],
   "id": "initial_id",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example transformed features and true labels\n",
    "x_transformed = np.array([[0.1, 0.2], [0.8, 0.7], [0.2, 0.3], [0.7, 0.8], [0.3, 0.4], [0.6, 0.5]])\n",
    "y_true = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot class 0\n",
    "plt.scatter(x_transformed[y_true == 0][:, 0], x_transformed[y_true == 0][:, 1], \n",
    "            color='blue', label='Class 0', alpha=0.7)\n",
    "\n",
    "# Plot class 1\n",
    "plt.scatter(x_transformed[y_true == 1][:, 0], x_transformed[y_true == 1][:, 1], \n",
    "            color='red', label='Class 1', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Scatter Plot of Transformed Features')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "170f1add36a7f4c2",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:46:34.964860Z",
     "start_time": "2024-09-05T22:46:32.643841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# X,y = load_fx()\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = y\n",
    "\n",
    "# Map the species numbers to their names for better visualization\n",
    "df['species'] = df['species'].map({0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'})\n",
    "\n",
    "# Plotting histograms for two features: Petal Length and Sepal Width\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df, x='petal length (cm)', hue='species', element='step', stat='density', common_norm=False)\n",
    "plt.title('Histogram of Petal Length')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=df, x='sepal width (cm)', hue='species', element='step', stat='density', common_norm=False)\n",
    "plt.title('Histogram of Sepal Width')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot using two features: Petal Length and Sepal Width\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='petal length (cm)', y='sepal width (cm)', hue='species', palette='Set1')\n",
    "plt.title('Scatter Plot of Petal Length vs Sepal Width')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.show()"
   ],
   "id": "c9e9c10d88d48056",
   "execution_count": 82,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Example: Gaussian Assumption for KL Divergence Between Two Classes\n",
    "\n",
    "Let’s assume that the features follow a Gaussian distribution for simplicity. The KL divergence between two univariate Gaussian distributions P and Q with means \\mu_P, \\mu_Q and variances \\sigma_P^2, \\sigma_Q^2 is given by:\n",
    "\n",
    "\n",
    "D_{KL}(P || Q) = \\log\\left(\\frac{\\sigma_Q}{\\sigma_P}\\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n"
   ],
   "id": "3cd727c75b4afdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# # Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = y\n",
    "\n",
    "# Function to compute KL divergence between two Gaussian distributions\n",
    "def kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    return np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n",
    "\n",
    "# Compute KL divergence for each feature between Setosa (0) and Versicolor (1)\n",
    "kl_divergences = {}\n",
    "for feature in iris.feature_names:\n",
    "    mu_p, sigma_p = df[df['species'] == 0][feature].mean(), df[df['species'] == 0][feature].std()\n",
    "    mu_q, sigma_q = df[df['species'] == 1][feature].mean(), df[df['species'] == 1][feature].std()\n",
    "    \n",
    "    kl_pq = kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q)\n",
    "    kl_qp = kl_divergence_gaussian(mu_q, sigma_q, mu_p, sigma_p)\n",
    "    \n",
    "    kl_divergences[feature] = {'KL(Setosa || Versicolor)': kl_pq, 'KL(Versicolor || Setosa)': kl_qp}\n",
    "\n",
    "# Display the results\n",
    "kl_divergences_df = pd.DataFrame(kl_divergences).T\n",
    "print(kl_divergences_df)"
   ],
   "id": "75e5bcfce43a371d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To increase the KL divergence between the classes in the Iris dataset, we can manipulate the features or apply transformations that enhance the differences between the distributions of the classes. Here’s a step-by-step approach to achieve this:\n",
    "\n",
    "1. Feature Transformation\n",
    "\n",
    "\t•\tScaling: Apply scaling or standardization to the features to give them equal importance.\n",
    "\t•\tPolynomial Features: Introduce polynomial features to capture non-linear relationships that might increase the separation between the classes.\n",
    "\t•\tNon-linear Transformations: Apply non-linear transformations like logarithms, exponentials, or custom functions to emphasize differences between the classes.\n",
    "\t•\tDimensionality Reduction: Techniques like Principal Component Analysis (PCA) can be used to project the data onto a space where the class separability is maximized.\n",
    "\n",
    "2. Optimization-Based Approach\n",
    "\n",
    "\t•\tTrain a model to find a transformation that maximizes the KL divergence. For example, using a neural network or other models to learn a mapping of the input features to a new space where the KL divergence between classes is maximized."
   ],
   "id": "4d68eea38b51f97c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Polynomial Features\n",
    "\n",
    "Let’s try adding polynomial features and see if it increases the KL divergence. This method will introduce interactions between features, which might enhance the separation between the classes."
   ],
   "id": "831beeaddc0d569a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:48:13.796804Z",
     "start_time": "2024-09-05T22:48:12.666082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load Iris dataset\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "X, y = load_fx()\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = y\n",
    "\n",
    "# Filter out two classes for binary classification (Setosa vs Versicolor)\n",
    "df_binary = df[df['species'] != 2].copy()\n",
    "\n",
    "# Apply polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_poly = poly.fit_transform(df_binary[iris.feature_names])\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(iris.feature_names))\n",
    "df_poly['species'] = df_binary['species'].values\n",
    "\n",
    "# Function to compute KL divergence between two Gaussian distributions\n",
    "def kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    return np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n",
    "\n",
    "\n",
    "# Compute KL divergence for each polynomial feature between Setosa (0) and Versicolor (1)\n",
    "kl_divergences_poly = {}\n",
    "for feature in df_poly.columns[:-1]:  # Exclude the species column\n",
    "    mu_p, sigma_p = df_poly[df_poly['species'] == 0][feature].mean(), df_poly[df_poly['species'] == 0][feature].std()\n",
    "    mu_q, sigma_q = df_poly[df_poly['species'] == 1][feature].mean(), df_poly[df_poly['species'] == 1][feature].std()\n",
    "    \n",
    "    kl_pq = kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q)\n",
    "    kl_qp = kl_divergence_gaussian(mu_q, sigma_q, mu_p, sigma_p)\n",
    "    \n",
    "    kl_divergences_poly[feature] = {'KL(Setosa || Versicolor)': kl_pq, 'KL(Versicolor || Setosa)': kl_qp}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "kl_divergences_poly_df = pd.DataFrame(kl_divergences_poly).T\n",
    "kl_divergences_poly_df_sorted = kl_divergences_poly_df.sort_values(by='KL(Setosa || Versicolor)', ascending=False)\n",
    "\n",
    "# Display the top features with the highest KL divergence\n",
    "print(\"Top Features by KL Divergence (Polynomial)\")\n",
    "print(kl_divergences_poly_df_sorted.head(10))  # Display the top 10 features"
   ],
   "id": "162746addfdd2baa",
   "execution_count": 83,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To visually confirm the increased separation, you can create histograms or scatter plots for some of these top features. Here’s how you can plot the top few features:",
   "id": "6f6ef3669f250c58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:48:58.007348Z",
     "start_time": "2024-09-05T22:48:56.274675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select top features for visualization\n",
    "top_features = [\n",
    "    'petal length (cm)',\n",
    "    'petal width (cm)',\n",
    "    'petal length (cm)^2',\n",
    "    'petal length (cm) petal width (cm)'\n",
    "]\n",
    "\n",
    "# Plot histograms for the top features\n",
    "for feature in top_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df_poly, x=feature, hue='species', element='step', stat='density', common_norm=False)\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plot for interaction terms\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_poly, x='petal length (cm)', y='petal width (cm)', hue='species', palette='Set1')\n",
    "plt.title('Scatter Plot of Petal Length vs Petal Width')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.show()"
   ],
   "id": "9435c790e91b9c15",
   "execution_count": 84,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.1. Build a Classification Model\n",
    "\n",
    "First, we’ll build the logistic regression model using the original features, and then we’ll do the same using the polynomial features."
   ],
   "id": "59d24a7a21aa0cce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:49:36.451025Z",
     "start_time": "2024-09-05T22:49:36.370389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_binary[iris.feature_names], df_binary['species'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model on original features\n",
    "model_original = LogisticRegression()\n",
    "model_original.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_original = model_original.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Original Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_original))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_original, pos_label=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_original, pos_label=0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_original, pos_label=0))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_original))\n",
    "\n",
    "# Now let's train the model on the polynomial features\n",
    "X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(df_poly.drop('species', axis=1), df_poly['species'], test_size=0.3, random_state=42)\n",
    "\n",
    "model_poly = LogisticRegression(max_iter=1000)\n",
    "model_poly.fit(X_poly_train, y_poly_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_poly = model_poly.predict(X_poly_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nPolynomial Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_poly_test, y_pred_poly))\n",
    "print(\"Precision:\", precision_score(y_poly_test, y_pred_poly, pos_label=0))\n",
    "print(\"Recall:\", recall_score(y_poly_test, y_pred_poly, pos_label=0))\n",
    "print(\"F1 Score:\", f1_score(y_poly_test, y_pred_poly, pos_label=0))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_poly_test, y_pred_poly))"
   ],
   "id": "c92b2765b885c537",
   "execution_count": 85,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\t•\tprobability=True: This parameter in the SVC model allows the model to output probability estimates by using a cross-validation procedure within the training process. The predict_proba method then provides these probabilities.\n",
    "\t•\tpredict_proba Method: This method returns the probability estimates for each class, which sum to 1.0. This is what roc_auc_score expects for multiclass evaluation."
   ],
   "id": "e4d8c8cd66c936d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:50:28.611112Z",
     "start_time": "2024-09-05T22:50:23.994574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Use all three classes for classification\n",
    "X_full_train, X_full_test, y_full_train, y_full_test = train_test_split(df[iris.feature_names], df['species'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train an SVM with probability estimates\n",
    "model_svm = SVC(kernel='rbf', probability=True)\n",
    "model_svm.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = model_svm.predict(X_full_test)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_proba_svm = model_svm.predict_proba(X_full_test)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "print(\"SVM Model (RBF Kernel):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_full_test, y_pred_svm))\n",
    "print(\"Precision:\", precision_score(y_full_test, y_pred_svm, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_full_test, y_pred_svm, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_full_test, y_pred_svm, average='macro'))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_full_test, y_proba_svm, multi_class='ovr'))"
   ],
   "id": "886da9e67e6e3ff1",
   "execution_count": 86,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Correct Workflow Example\n",
    "\n",
    "Here’s how the workflow should be:\n",
    "\n",
    "\t1.\tFit PolynomialFeatures on the training data.\n",
    "\t2.\tUse transform to apply the learned transformation to the test data."
   ],
   "id": "44a9c1870924c362"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:52:39.006214Z",
     "start_time": "2024-09-05T22:52:38.883054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data (using binary classification: Setosa vs Versicolor)\n",
    "X = df_binary[iris.feature_names]\n",
    "y = df_binary['species']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit PolynomialFeatures on training data only\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly_train = poly.fit_transform(X_train)  # Fit and transform training data\n",
    "\n",
    "# Transform the test data without refitting\n",
    "X_poly_test = poly.transform(X_test)  # Only transform test data\n",
    "\n",
    "# Initialize and train a logistic regression model on the polynomial features\n",
    "model_poly = LogisticRegression(max_iter=1000)\n",
    "model_poly.fit(X_poly_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_poly = model_poly.predict(X_poly_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Polynomial Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_poly))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_poly, pos_label=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_poly, pos_label=0))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_poly, pos_label=0))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_poly))"
   ],
   "id": "d66066b8b7d0ea27",
   "execution_count": 90,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d2468cd1e64b9d7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Seccond Approach: Neural Network\n",
    "\n",
    "\t1.\tModel: Let  f(x)  be a neural network that takes the features  x  as input and outputs transformed features  x{\\prime} .\n",
    "\t2.\tLoss Function: Define the loss function as the negative KL divergence:\n",
    "\n",
    "\\mathcal{L}(\\theta) = -D_{KL}(P{\\prime}(x{\\prime}; \\theta) || Q{\\prime}(x{\\prime}; \\theta))\n",
    "\n",
    "\t3.\tOptimization: Use gradient descent-based optimization to minimize this loss function, effectively maximizing the KL divergence."
   ],
   "id": "f6c4562e4793d08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T22:55:31.475381Z",
     "start_time": "2024-09-05T22:55:30.281640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "X, y = load_fx()\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network model with log-softmax for KL Divergence\n",
    "class NeuralNetWithKL(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(NeuralNetWithKL, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)  # Use log-softmax for KLDivLoss\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.log_softmax(out)  # Log-Softmax as the final activation\n",
    "        return out\n",
    "\n",
    "# Set parameters for the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 8\n",
    "num_classes = 3  # 3 classes in the Iris dataset\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetWithKL(input_size, hidden_size1, hidden_size2, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')  # KL divergence loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)  # KL divergence expects log-probs and probs\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update model weights\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    _, actual = torch.max(y_test_tensor, 1)\n",
    "\n",
    "    accuracy = (predicted == actual).sum().item() / len(actual)\n",
    "    print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "9972b575abc33cd2",
   "execution_count": 92,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "VAE with KL divergence",
   "id": "b2f2ededdf0538dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:01:48.470625Z",
     "start_time": "2024-09-06T11:01:15.876416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# # Load Iris dataset\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "X, y = load_fx(data_len=5000)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Define the latent feature generator\n",
    "class LatentFeatureGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(LatentFeatureGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout to regularize\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)     # Mean of latent space\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size) # Log variance of latent space\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h1 = torch.relu(self.fc1(x))\n",
    "        # h1 = torch.nn.functional.leaky_relu(self.fc1(x))\n",
    "        h1 = torch.nn.functional.elu(self.fc1(x))\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "# Define the KL divergence loss function\n",
    "# Instead of averaging before computing KL divergence, compute the KL divergence for each pair of samples and sum the result:\n",
    "def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "    var1 = logvar1.exp()\n",
    "    var2 = logvar2.exp()\n",
    "    kl_div = 0.5 * torch.sum(var1 / var2 + ((mu2 - mu1).pow(2) / var2) - 1 + logvar2 - logvar1, dim=1)\n",
    "    return kl_div.mean()  # Average KL divergence across samples\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 1024\n",
    "latent_size = 15  # You can modify this value to change the latent feature space size\n",
    "\n",
    "model = LatentFeatureGenerator(input_size, hidden_size, latent_size)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Training loop to maximize KL divergence between class distributions\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Split data into class 1 and class 2 (Setosa and Versicolor for example)\n",
    "    class1_mask = (y_train == 0)  # Select Setosa\n",
    "    class2_mask = (y_train == 1)  # Select Versicolor\n",
    "    \n",
    "    mu_class1, logvar_class1 = model(X_train_tensor[class1_mask])\n",
    "    mu_class2, logvar_class2 = model(X_train_tensor[class2_mask])\n",
    "    # Downsample the larger class to match the smaller class\n",
    "    class_size = min(mu_class1.size(0), mu_class2.size(0))\n",
    "    mu_class1, logvar_class1 = mu_class1[:class_size], logvar_class1[:class_size]\n",
    "    mu_class2, logvar_class2 = mu_class2[:class_size], logvar_class2[:class_size]\n",
    "\n",
    "    # increase divergence\n",
    "    kl_loss = 1/kl_divergence(mu_class1, logvar_class1, mu_class2, logvar_class2)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    kl_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], KL Loss: {kl_loss.item():.4f}')\n",
    "\n",
    "# Extract new latent features from the model after training\n",
    "with torch.no_grad():\n",
    "    mu_train, _ = model(X_train_tensor)\n",
    "    mu_test, _ = model(X_test_tensor)\n",
    "    \n",
    "    print(f\"Latent Features Shape (train): {mu_train.shape}\")\n",
    "    print(f\"Latent Features Shape (test): {mu_test.shape}\")"
   ],
   "id": "fa90b50778070ea6",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step-by-Step Evaluation of KL Divergence:\n",
    "\n",
    "\t1.\tExtract the latent features for each class from the training set.\n",
    "\t2.\tEstimate distributions (e.g., assuming normal distributions) for the latent features of each class.\n",
    "\t3.\tCalculate KL divergence between these distributions for each dimension of the latent space.\n"
   ],
   "id": "1fd27abcc9fe8f80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:01:59.779369Z",
     "start_time": "2024-09-06T11:01:59.767171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Separate latent features for class 0 (Setosa) and class 1 (Versicolor)\n",
    "latent_class1 = mu_train[y_train == 0].numpy()  # Setosa\n",
    "latent_class2 = mu_train[y_train == 1].numpy()  # Versicolor\n",
    "\n",
    "# Function to calculate KL divergence between two normal distributions\n",
    "def kl_divergence_gaussian(mu1, sigma1, mu2, sigma2):\n",
    "    return np.log(sigma2 / sigma1) + (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5\n",
    "\n",
    "# Calculate KL divergence for each dimension in the latent space\n",
    "kl_values = []\n",
    "for i in range(latent_class1.shape[1]):\n",
    "    # Estimate mean and variance for both classes in the i-th dimension\n",
    "    mu1, sigma1 = latent_class1[:, i].mean(), latent_class1[:, i].std()\n",
    "    mu2, sigma2 = latent_class2[:, i].mean(), latent_class2[:, i].std()\n",
    "    \n",
    "    # Calculate KL divergence for the i-th dimension\n",
    "    kl_div = kl_divergence_gaussian(mu1, sigma1, mu2, sigma2)\n",
    "    kl_values.append(kl_div)\n",
    "    print(f'Dimension {i+1}: KL Divergence = {kl_div:.4f}')\n",
    "\n",
    "# Sum the KL divergences across dimensions for the total divergence\n",
    "total_kl_divergence = np.sum(kl_values)\n",
    "print(f'Total KL Divergence: {total_kl_divergence:.4f}')"
   ],
   "id": "a52316899aeb1283",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let’s plot histograms of the latent features (generated by the model) to visualize their distribution across the two classes.\n",
    "\n",
    "Here’s the code to plot histograms for each dimension of the latent space, showing the distribution of latent features for Class 0 (Setosa) and Class 1 (Versicolor):"
   ],
   "id": "71f455a2609fa492"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:02:13.924675Z",
     "start_time": "2024-09-06T11:02:12.803357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate latent features for each class\n",
    "latent_class1 = mu_train[y_train == 0].numpy()  # Setosa\n",
    "latent_class2 = mu_train[y_train == 1].numpy()  # Versicolor\n",
    "\n",
    "# Plot histograms for each latent dimension\n",
    "num_dimensions = latent_class1.shape[1]\n",
    "\n",
    "num_dimensions = 3\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i in range(num_dimensions):\n",
    "    plt.subplot(1, num_dimensions, i + 1)\n",
    "    plt.hist(latent_class1[:, i], bins=20, alpha=0.6, label='Setosa', color='blue')\n",
    "    plt.hist(latent_class2[:, i], bins=20, alpha=0.6, label='Versicolor', color='orange')\n",
    "    plt.title(f'Latent Dimension {i + 1}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "154f1da00f88912f",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let’s proceed by applying two models to the newly generated latent features to evaluate their classification performance. I will use Logistic Regression and Random Forests for this task.",
   "id": "bd3a38e7eccc94ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:02:40.361931Z",
     "start_time": "2024-09-06T11:02:40.220289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train Logistic Regression on latent features\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(mu_train.numpy(), y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred_log_reg = log_reg.predict(mu_test.numpy())\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log_reg * 100:.2f}%\")"
   ],
   "id": "50eea49cc26aaa53",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:02:52.301128Z",
     "start_time": "2024-09-06T11:02:50.341607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest on latent features\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(mu_train.numpy(), y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred_rf = rf_model.predict(mu_test.numpy())\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")"
   ],
   "id": "e79cdf99c0c8c2f9",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 42,
   "source": "traced_model = torch.jit.trace(model.forward,X_train_tensor[class1_mask])",
   "id": "f3c64eb89697a6cd",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 43,
   "source": "traced_model.save(\"LatentFeatureGenerator.pt\")",
   "id": "b3c6d3e6c9675ec9",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T10:31:46.400719Z",
     "start_time": "2024-09-06T10:31:44.591173Z"
    }
   },
   "cell_type": "code",
   "source": "X, y = load_fx(data_len=5000)",
   "id": "a296defa466ac6e6",
   "execution_count": 259,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:03:07.909126Z",
     "start_time": "2024-09-06T11:03:07.905840Z"
    }
   },
   "cell_type": "code",
   "source": "mu = mu_train.numpy()",
   "id": "c0359139127ed55a",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:07:52.629040Z",
     "start_time": "2024-09-06T19:07:51.112233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import copy\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "# select top 10 features using mRMR\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "\n",
    "X, y = load_fx(data_len=5000)\n",
    "\n",
    "def sel_fea(X, y):\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "    \n",
    "    selected_features_idx = mrmr_classif(X=X, y=y, K=3)\n",
    "    # X, y = load_fx(data_len=5000)\n",
    "    X = X.iloc[:, selected_features_idx].values\n",
    "    y = y.values.squeeze()\n",
    "    return X, y"
   ],
   "id": "c1337cf62a8fc481",
   "execution_count": 70,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:08:07.769022Z",
     "start_time": "2024-09-06T19:08:05.197445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "svc = SVC()\n",
    "svc.fit(X, y)\n",
    "p = svc.predict(X)\n",
    "\n",
    "wrong = p!=y"
   ],
   "id": "8afccd5c4bd69f8e",
   "execution_count": 71,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:08:20.770297Z",
     "start_time": "2024-09-06T19:08:12.425790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rng = np.random.RandomState(42)\n",
    "# iris = datasets.load_iris()\n",
    "oriiginal_target =  copy.deepcopy(y)\n",
    "# random_unlabeled_points = rng.rand(y.shape[0]) < 0.3\n",
    "# y[random_unlabeled_points] = -1\n",
    "y[wrong] = -1\n",
    "svc = SVC(probability=True, gamma=\"auto\")\n",
    "self_training_model = SelfTrainingClassifier(svc)\n",
    "self_training_model.fit(X, y)"
   ],
   "id": "8372c86c5798c955",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:08:27.149456Z",
     "start_time": "2024-09-06T19:08:26.521197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted_target = self_training_model.predict(X)\n",
    "accuracy_ssv = accuracy_score(oriiginal_target, predicted_target)\n",
    "print(f\"Random Forest Accuracy: {accuracy_ssv * 100:.2f}%\")"
   ],
   "id": "ba38c7864db95410",
   "execution_count": 73,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "4d90d07c15868dad",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
