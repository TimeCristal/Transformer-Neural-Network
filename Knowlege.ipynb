{
 "cells": [
  {
   "cell_type": "code",
   "id": "73b66595-4fdb-4629-aabb-8f8e7e27345d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:21:40.168637Z",
     "start_time": "2024-08-22T00:21:38.538280Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file containing EUR/USD exchange rate data\n",
    "df = pd.read_csv(\"dataset/EURUSDH4.csv\", delimiter=\"\\t\")\n",
    "#df = pd.read_csv(\"dataset/EURUSD_Daily_200005300000_202405300000.csv\", delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "# Extract the closing prices from the DataFrame\n",
    "closing = df[\"<CLOSE>\"].iloc[0:1000]\n",
    "\n",
    "def generate_triangle_wave_data(n_points=5000, amplitude=1.0, period=5):\n",
    "    data = []\n",
    "    for i in range(n_points):\n",
    "        data.append(amplitude * (2 * (i % period) / period - 1))\n",
    "    return np.array(data)\n",
    "\n",
    "# Generate triangle wave data\n",
    "#data = generate_triangle_wave_data(period=4) + generate_triangle_wave_data(period=6) + generate_triangle_wave_data(period=5)\n",
    "#closing = pd.Series(data=data)\n",
    "\n",
    "# Use the moving average instead of the raw closing prices\n",
    "#moving_average = closing.rolling(window=5).mean().dropna()  # Drop NaN values from the start\n",
    "\n",
    "#closing = df[\"<HIGH>\"]\n",
    "\n",
    "# # Normalize the data if not already done\n",
    "# closing = (closing - closing.mean()) / closing.std()\n",
    "\n",
    "def visibility_condition(prices, i, j):\n",
    "    \"\"\"Check if node j is visible from node i.\"\"\"\n",
    "    for k in range(i + 1, j):\n",
    "        if prices[k] >= prices[i] + (prices[j] - prices[i]) * (k - i) / (j - i):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def create_nvg(prices):\n",
    "    N = len(prices)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(N):\n",
    "        G.add_node(i, feature=prices[i])\n",
    "        for j in range(i + 1, N):\n",
    "            visibility = True\n",
    "            for k in range(i + 1, j):\n",
    "                # Linear interpolation between points i and j at point k\n",
    "                expected_value_at_k = prices[i] + (prices[j] - prices[i]) * (k - i) / (j - i)\n",
    "                \n",
    "                # Check if point k obstructs the visibility\n",
    "                if prices[k] >= expected_value_at_k:\n",
    "                    visibility = False\n",
    "                    break\n",
    "            if visibility:\n",
    "                G.add_edge(i, j)\n",
    "                \n",
    "    return G\n",
    "\n",
    "def plot_nvg_with_prices(prices, nvg, window_start, window_size):\n",
    "    \"\"\"Plot the NVG connections over the time series data.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the closing prices\n",
    "    plt.plot(range(window_start, window_start + window_size), prices, label=\"Closing Prices\", color='blue')\n",
    "    \n",
    "    # Overlay the NVG connections\n",
    "    for edge in nvg.edges():\n",
    "        i, j = edge\n",
    "        plt.plot([window_start + i, window_start + j], [prices[i], prices[j]], color='red', linestyle='-', linewidth=0.8)\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.title(f\"Closing Prices with NVG Connections (Window {window_start} to {window_start + window_size - 1})\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define the sliding window size\n",
    "window_size = 20\n",
    "\n",
    "# Generate NVGs for each sliding window\n",
    "sliding_window_graphs = []\n",
    "sliding_window_data = []\n",
    "for i in range(len(closing) - window_size + 1):\n",
    "    window_data = closing[i:i + window_size].values\n",
    "    \n",
    "    # Normalize the data for each window\n",
    "    window_data = (window_data - window_data.mean()) / window_data.std()\n",
    "    sliding_window_data.append(window_data)\n",
    "    \n",
    "    G = create_nvg(window_data)\n",
    "    sliding_window_graphs.append((G, i))\n",
    "\n"
   ],
   "execution_count": 259,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:21:40.175572Z",
     "start_time": "2024-08-22T00:21:40.171379Z"
    }
   },
   "cell_type": "code",
   "source": "print(window_data.std(), \" \" , window_data.mean())",
   "id": "40195682e2e40a4f",
   "execution_count": 260,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:21:40.378142Z",
     "start_time": "2024-08-22T00:21:40.176524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot the NVG and prices for the first sliding window\n",
    "IDX = 100\n",
    "first_window_graph, first_window_start = sliding_window_graphs[IDX]\n",
    "# plot_nvg_with_prices(closing[first_window_start:first_window_start + window_size], first_window_graph, first_window_start, window_size)\n",
    "plot_nvg_with_prices(sliding_window_data[IDX], first_window_graph, first_window_start, window_size)"
   ],
   "id": "4c4a00e2c4f3e56c",
   "execution_count": 261,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\t1.\tIterate Over All Sliding Windows: We’ll loop through all the sliding windows to perform any necessary operations on each NVG.\n",
    "\t2.\tStore Graphs for Model Training: We will prepare the graphs and features in a format suitable for graph-based machine learning models.\n",
    "\t3.\tFeature Masking and Edge Management: Mask the feature of the last node in each window and handle the edges accordingly."
   ],
   "id": "9634348ee8d1a97f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:21:40.384971Z",
     "start_time": "2024-08-22T00:21:40.380850Z"
    }
   },
   "cell_type": "code",
   "source": "sliding_window_data[0]",
   "id": "ad4785ac908883f",
   "execution_count": 262,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:21:40.388965Z",
     "start_time": "2024-08-22T00:21:40.386339Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install torch_geometric",
   "id": "e996fdc7748823fa",
   "execution_count": 263,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:21:41.704710Z",
     "start_time": "2024-08-22T00:21:40.390701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Example to generate edge weights with emphasis on last_node\n",
    "def generate_edge_weights(G, last_node):\n",
    "    weights = {}\n",
    "    for edge in G.edges:\n",
    "        # Assign a high weight to edges connected to the last_node\n",
    "        if last_node in edge:\n",
    "            weights[edge] = 1.0  # High weight\n",
    "        else:\n",
    "            weights[edge] = 0.1  # Low weight or zero\n",
    "    return weights\n",
    "\n",
    "# Example to generate node weights with emphasis on last_node\n",
    "def generate_node_weights(G, last_node):\n",
    "    weights = []\n",
    "    for node in G.nodes:\n",
    "        if node == last_node:\n",
    "            weights.append(1.0)  # High weight for last_node\n",
    "        else:\n",
    "            weights.append(0.1)  # Low weight or zero for others\n",
    "    return weights\n",
    "\n",
    "# Iterate over all sliding windows and prepare data for model training\n",
    "graph_data_list = []\n",
    "\n",
    "for G, start_idx in sliding_window_graphs:\n",
    "    # Extract the node features (closing prices)\n",
    "    node_features = np.array([G.nodes[n]['feature'] for n in G.nodes])\n",
    "    \n",
    "    # Mask the last node's feature (the \"future\" price, which we want to predict)\n",
    "    last_node = max(G.nodes)\n",
    "    #node_features[last_node] = 0.0#np.nan  # Masking the future node's feature\n",
    "    \n",
    "    # Generate edge weights\n",
    "    edge_weights = generate_edge_weights(G, last_node)\n",
    "    for edge in G.edges:\n",
    "        G.edges[edge]['weight'] = edge_weights[edge]\n",
    "    \n",
    "    # Generate node weights\n",
    "    node_weights = generate_node_weights(G, last_node)\n",
    "    \n",
    "    # Convert NetworkX graph to PyTorch Geometric format\n",
    "    graph_data = from_networkx(G)\n",
    "    \n",
    "    # Assign features to the graph\n",
    "    graph_data.x = torch.tensor(node_features, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Assign node weights (if necessary, as an additional feature)\n",
    "    graph_data.node_weights = torch.tensor(node_weights, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Assign edge weights to the graph (if they exist)\n",
    "    graph_data.edge_attr = torch.tensor([G.edges[edge]['weight'] for edge in G.edges], dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Store the processed graph\n",
    "    graph_data_list.append(graph_data)\n",
    "\n",
    "# Let's print a summary of the first graph data structure\n",
    "print(\"Summary of the first graph:\")\n",
    "print(graph_data_list[0])\n",
    "\n",
    "# In the model, you can use edge_attr and node_weights during the forward pass."
   ],
   "id": "96b78cc9-4987-461c-982c-6b8d77bc8b0a",
   "execution_count": 264,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\t1.\tDefine the VGAE Model: We’ll define the encoder and the VGAE model.\n",
    "\t2.\tTraining Loop: Set up the training loop to process the graph data, focusing on reconstructing the graph and predicting the masked node features.\n",
    "\t3.\tLoss Function: Include the standard loss components (reconstruction loss and KL divergence) and optionally add visibility constraint handling."
   ],
   "id": "c3b1033228e6b734"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:53.445092Z",
     "start_time": "2024-08-22T00:21:41.705725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch_geometric.nn import GCNConv, VGAE\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"runs/transformer_experiment\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# TODO: if predicted and real close price dont match increase model complexity\n",
    "\n",
    "class VGAEEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(VGAEEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, out_channels)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Apply Xavier (Glorot) initialization to all layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, GCNConv):\n",
    "                init.xavier_uniform_(m.lin.weight)  # GCNConv has a linear layer with weights accessible as `lin.weight`\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight).relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight=edge_weight).relu()\n",
    "        return self.conv_mu(x, edge_index, edge_weight=edge_weight), self.conv_logvar(x, edge_index, edge_weight=edge_weight)\n",
    "\n",
    "\n",
    "\n",
    "# Increase the hidden channels (complexity) and latent space dimensionality\n",
    "in_channels = 1\n",
    "hidden_channels = 16*2  # Increase this to make the model more complex\n",
    "out_channels = 8*2  # Larger latent space\n",
    "\n",
    "# Initialize the model, optimizer, and loss function components\n",
    "encoder = VGAEEncoder(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels).to(device)\n",
    "model = VGAE(encoder).to(device)\n",
    "\n",
    "\n",
    "LR = 0.0001\n",
    "LR = 0.01\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "\n",
    "# Function to handle NaN values in node features (e.g., zero-imputation)\n",
    "def handle_nan(x):\n",
    "    return torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "# Visibility loss function\n",
    "def visibility_loss_specific(z, ohlc_values, specific_edges):\n",
    "    loss = torch.tensor(0.0, device=z.device)  # Initialize as a tensor on the correct device\n",
    "    for i, j in specific_edges:\n",
    "        if not visibility_condition(ohlc_values, i, j):\n",
    "            predicted_similarity = (z[i] * z[j]).sum()\n",
    "            loss += F.relu(predicted_similarity)  # Penalize positive similarity for invalid edges\n",
    "    return loss\n",
    "\n",
    "def visibility_condition(prices, i, j):\n",
    "    \"\"\"Check if node j is visible from node i.\"\"\"\n",
    "    for k in range(i + 1, j):\n",
    "        if prices[k] >= prices[i] + (prices[j] - prices[i]) * (k - i) / (j - i):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Training loop\n",
    "epochs = 41\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_vis_loss = 0\n",
    "    total_link_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    \n",
    "    for graph_data in graph_data_list:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Handle NaN in features and move data to the correct device\n",
    "        graph_data.x = handle_nan(graph_data.x).to(device)\n",
    "        graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "        \n",
    "        # Forward pass through the model to get the latent variables\n",
    "        # z = model.encode(graph_data.x, graph_data.edge_index)\n",
    "        \n",
    "        # Assuming you have node features, edge indices, and edge weights\n",
    "        # START WEIGHT\n",
    "        # x = graph_data.x.to(device)\n",
    "        # edge_index = graph_data.edge_index.to(device)\n",
    "        # edge_weight = graph_data.edge_attr.to(device) if graph_data.edge_attr is not None else None\n",
    "         # Forward pass with edge weights\n",
    "        # z = model.encode(x, edge_index, edge_weight=edge_weight)\n",
    "        z = model.encode(graph_data.x, edge_index=graph_data.edge_index, edge_weight=graph_data.edge_weight)\n",
    "        # END WEIGHT\n",
    "        \n",
    "        # Decode to reconstruct the graph (edges)\n",
    "        edge_logits = model.decode(z, graph_data.edge_index)\n",
    "        link_loss = F.binary_cross_entropy_with_logits(edge_logits, torch.ones(edge_logits.size(0), device=device))\n",
    "        \n",
    "        # KL Divergence Loss (using internally stored mu and logvar)\n",
    "        kl_loss = model.kl_loss()\n",
    "        \n",
    "        # Visibility Loss for specific edge(s)\n",
    "        last_node = graph_data.x.size(0) - 1\n",
    "        # All Nodes\n",
    "        specific_edges = [(0, last_node)]\n",
    "        # Emphasise on last 5\n",
    "        #specific_edges = [(last_node - 5, last_node)]\n",
    "        \n",
    "        \n",
    "        vis_loss = visibility_loss_specific(z, graph_data.x, specific_edges).to(device)\n",
    "     \n",
    "        \n",
    "        # Total loss with dynamic weighting of KL loss\n",
    "        vis_loss_weight = 0.5  # Smaller weight for visibility loss\n",
    "        kl_weight = 1 / (1 + epoch)  # Decrease the influence of KL loss over time\n",
    "        loss = link_loss + kl_weight * kl_loss + vis_loss_weight * vis_loss\n",
    "        abs_kl_loss = torch.abs(kl_loss)\n",
    "        loss = link_loss* abs_kl_loss # Original Paper\n",
    "        \n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_vis_loss += vis_loss.item()  # Ensure to use .item() for scalar values\n",
    "        total_link_loss += link_loss.item()\n",
    "        total_kl_loss += (kl_weight * kl_loss).item()\n",
    "\n",
    "    scheduler.step(metrics=total_loss)\n",
    "    # Write Loss per Epoch\n",
    "    writer.add_scalar('Loss/Epoch', total_loss, epoch )\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.9f}, Visibility Loss: {total_vis_loss:.4f}, Link Loss: {total_link_loss:.4f}, KL Loss: {total_kl_loss:.4f}\")\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        torch.save(model.state_dict(), f\"knowlege_check_point/model{epoch}.pt\")\n",
    "        print(f\"Model saved at epoch {epoch}\")\n",
    "# Close writer    \n",
    "writer.close()\n",
    "# After training, we can test the model or analyze the results.\n"
   ],
   "id": "30e32070a47ca707",
   "execution_count": 265,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:53.475301Z",
     "start_time": "2024-08-22T00:40:53.457262Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"knowlege_check_point/model.pt\")",
   "id": "de9df31558337562",
   "execution_count": 266,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:53.740795Z",
     "start_time": "2024-08-22T00:40:53.476357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "plot_true_edges = True\n",
    "plot_predicted_edges = True\n",
    "idx = 400\n",
    "\n",
    "# Generate the true graph using the visibility rules\n",
    "true_graph = create_nvg(graph_data_list[idx].x.cpu().numpy().flatten())  # Assuming the features are normalized\n",
    "\n",
    "# Generate the predicted graph\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Encode the graph to get the latent representation\n",
    "    z = model.encode(graph_data_list[idx].x.to(device), graph_data_list[idx].edge_index.to(device))\n",
    "    \n",
    "    # Decode the latent representation to predict the edges\n",
    "    sampled_graph_decoded = model.decode(z, graph_data_list[idx].edge_index.to(device))\n",
    "    \n",
    "    # Extract the predicted value for the last node\n",
    "    last_node = graph_data_list[idx].x.size(0) - 1\n",
    "    predicted_last_price = sampled_graph_decoded[last_node].item()\n",
    "    print(f\"Predicted last price: {predicted_last_price:.4f}\")\n",
    "\n",
    "# Threshold the output to decide on the edges\n",
    "sampled_graph = torch.sigmoid(sampled_graph_decoded).cpu().numpy()\n",
    "flatt_sampled_graph = sampled_graph.flatten()\n",
    "print(\"flatt_sampled_graph\", flatt_sampled_graph.shape)\n",
    "threshold = np.percentile(flatt_sampled_graph, 90)\n",
    "print(f\"Threshold 90% percentile: {threshold:.4f}\")\n",
    "thresholded_graph = (sampled_graph > threshold).astype(int)\n",
    "\n",
    "# Extract the prices (closing values) for plotting\n",
    "prices = graph_data_list[idx].x.cpu().numpy().flatten()\n",
    "\n",
    "# Replace NaN with the predicted value for the last node\n",
    "# if np.isnan(prices[last_node]):\n",
    "prices[last_node] = predicted_last_price\n",
    "\n",
    "# Real Prices (before masking the last node)\n",
    "prices_true = sliding_window_data[idx]\n",
    "\n",
    "# Plot the price time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prices, color='blue', linestyle='-', marker='o', label='Predicted Closing Prices')\n",
    "plt.plot(prices_true, color='yellow', linestyle='-', marker='o', label='True Closing Prices')\n",
    "\n",
    "if plot_true_edges:\n",
    "    # Overlay the edges of the true graph\n",
    "    counter = 0\n",
    "    for edge in true_graph.edges():\n",
    "        i, j = edge\n",
    "        if counter == 0:\n",
    "            plt.plot([i, j], [prices[i], prices[j]], color='green', linestyle='-', linewidth=2.8, label='True edges')\n",
    "        else:\n",
    "            plt.plot([i, j], [prices[i], prices[j]], color='green', linestyle='-', linewidth=2.8)\n",
    "        counter += 1\n",
    "\n",
    "if plot_predicted_edges:\n",
    "    # Overlay the edges of the predicted graph\n",
    "    counter = 0\n",
    "    for edge_idx, (u, v) in enumerate(graph_data_list[idx].edge_index.T.cpu().numpy()):\n",
    "        if thresholded_graph[edge_idx] == 1:\n",
    "            if counter == 0:\n",
    "                plt.plot([u, v], [prices[u], prices[v]], color='red', linestyle='-', linewidth=.8, label='Predicted edges')\n",
    "            else:\n",
    "                plt.plot([u, v], [prices[u], prices[v]], color='red', linestyle='-', linewidth=.8)\n",
    "            counter += 1\n",
    "\n",
    "plt.title(\"Closing Prices with True and Predicted Graph Connections\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f1ba78668d483047",
   "execution_count": 267,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:53.895087Z",
     "start_time": "2024-08-22T00:40:53.744982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `sampled_graph` contains the edge probabilities/logits\n",
    "flatt_sampled_graph = sampled_graph.flatten()\n",
    "print(\"flatt_sampled_graph\", flatt_sampled_graph.shape)\n",
    "plt.hist(flatt_sampled_graph, bins=50)\n",
    "plt.title(\"Distribution of Edge Probabilities/Logits\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "id": "3c05572956cfee8a",
   "execution_count": 268,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:54.057187Z",
     "start_time": "2024-08-22T00:40:53.896548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming `model` is your trained VGAE or similar model\n",
    "# `graph_data` is the data you're evaluating on\n",
    "\n",
    "# Generate the true graph using the visibility rules\n",
    "true_graph = create_nvg(graph_data.x.cpu().numpy().flatten())  # Assuming the features are normalized\n",
    "\n",
    "# Get the set of true edges from the true graph\n",
    "true_edges = set(true_graph.edges())\n",
    "\n",
    "# Encode and decode to get predicted edges\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(graph_data.x.to(device), graph_data.edge_index.to(device))\n",
    "    edge_logits = model.decode(z, graph_data.edge_index.to(device))\n",
    "\n",
    "# Threshold the logits to decide which edges exist\n",
    "edge_probs = torch.sigmoid(edge_logits)\n",
    "\n",
    "flatt_edge_probs = edge_probs.flatten().cpu().numpy()\n",
    "plt.hist(flatt_edge_probs, bins=50)\n",
    "plt.title(\"Distribution of Edge Probabilities/Logits\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.625  # Commonly used threshold\n",
    "threshold = np.percentile(flatt_edge_probs, 90)\n",
    "predicted_edges_mask = (edge_probs > threshold)\n",
    "\n",
    "# The indices of the edges in `graph_data.edge_index` that are predicted as present\n",
    "predicted_edges_indices = predicted_edges_mask.nonzero(as_tuple=False).flatten()\n",
    "\n",
    "# Get the actual edge indices corresponding to the predicted edges\n",
    "predicted_edges_set = set(\n",
    "    (graph_data.edge_index[0, idx].item(), graph_data.edge_index[1, idx].item()) \n",
    "    for idx in predicted_edges_indices\n",
    ")\n",
    "\n",
    "# Now use `predicted_edges_set` and `true_edges` for evaluation\n",
    "tp = len(true_edges & predicted_edges_set)  # True Positives\n",
    "fp = len(predicted_edges_set - true_edges)  # False Positives\n",
    "fn = len(true_edges - predicted_edges_set)  # False Negatives\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score\n",
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}\")"
   ],
   "id": "ad59270f29686d13",
   "execution_count": 269,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:54.063778Z",
     "start_time": "2024-08-22T00:40:54.058706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: Choose a threshold based on the 90th percentile of the distribution\n",
    "threshold = np.percentile(flatt_edge_probs, 90)\n",
    "print(f\"Threshold 90 percentile: {threshold:.4f}\")\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "# Use Otsu's method to find an optimal threshold\n",
    "threshold = threshold_otsu(flatt_edge_probs)\n",
    "print(f\"Threshold otsu: {threshold:.4f}\")\n"
   ],
   "id": "b3dc2c9b5c6c4995",
   "execution_count": 270,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T00:40:54.068026Z",
     "start_time": "2024-08-22T00:40:54.065993Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "22bad88ec7d7a0e0",
   "execution_count": 270,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
