{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import ElasticNet, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \\\n",
    "    HistGradientBoostingClassifier, IsolationForest, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from datetime import datetime\n",
    "from KL.kl.utils import load_fx\n",
    "import matplotlib.pyplot as plt\n",
    "from mrmr import mrmr_classif\n",
    "import pandas as pd\n",
    "import chime"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_models(class_weight):\n",
    "    classifiers_list = []\n",
    "    classifiers_list.append(RandomForestClassifier(class_weight=class_weight))#class_weight\n",
    "    classifiers_list.append(ExtraTreesClassifier(class_weight=class_weight))#class_weight\n",
    "    classifiers_list.append(GradientBoostingClassifier())\n",
    "    classifiers_list.append(HistGradientBoostingClassifier(class_weight=class_weight))#class_weight\n",
    "    classifiers_list.append(GaussianNB())\n",
    "    classifiers_list.append(BernoulliNB())\n",
    "    classifiers_list.append(IsolationForest())\n",
    "    classifiers_list.append(ElasticNet())\n",
    "    # classifiers_list.append(KNeighborsClassifier()) \n",
    "    classifiers_list.append(LinearSVC(class_weight=class_weight))#class_weight/no predict_proba\n",
    "    classifiers_list.append(SGDClassifier(class_weight=class_weight))#class_weight/no predict_proba\n",
    "    classifiers_list.append(SVC(probability=True, class_weight=class_weight))#class_weight/ no predict_proba\n",
    "    classifiers_list.append(AdaBoostClassifier(algorithm='SAMME', n_estimators=100))\n",
    "    classifiers_list.append(BaggingClassifier(estimator=SVC(), n_estimators=100, random_state=0))\n",
    "    return classifiers_list"
   ],
   "id": "42f28c094942d469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_calibrated_models(class_weight):\n",
    "    method=\"isotonic\"\n",
    "    classifiers_list = []\n",
    "    classifiers_list.append(CalibratedClassifierCV(RandomForestClassifier(class_weight=class_weight), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(ExtraTreesClassifier(class_weight=class_weight), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(GradientBoostingClassifier(), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(HistGradientBoostingClassifier(class_weight=class_weight), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(GaussianNB(), cv=5, method=method))\n",
    "    \n",
    "    classifiers_list.append(CalibratedClassifierCV(BernoulliNB(), cv=5, method=method))\n",
    "    # classifiers_list.append(CalibratedClassifierCV(IsolationForest(), cv=5, method=method))\n",
    "    # classifiers_list.append(CalibratedClassifierCV(ElasticNet(), cv=5, method=method))\n",
    "    \n",
    "    classifiers_list.append(CalibratedClassifierCV(LinearSVC(class_weight=class_weight), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(SGDClassifier(class_weight=class_weight), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(SVC(probability=True, class_weight=class_weight), cv=5, method=method))\n",
    "    \n",
    "    classifiers_list.append(CalibratedClassifierCV(AdaBoostClassifier(algorithm='SAMME', n_estimators=100), cv=5, method=method))\n",
    "    classifiers_list.append(CalibratedClassifierCV(BaggingClassifier(estimator=SVC(), n_estimators=100, random_state=0), cv=5, method=method))\n",
    "    return classifiers_list"
   ],
   "id": "fa48e3beb8ea877b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_models_small(class_weight):\n",
    "    classifiers_list = []\n",
    "    classifiers_list.append(RandomForestClassifier(class_weight=class_weight))#class_weight\n",
    "    classifiers_list.append(GradientBoostingClassifier())\n",
    "    return classifiers_list"
   ],
   "id": "98c2fd31960557a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 10\n",
    "pair = 'EURUSD'\n",
    "X, y_high, y_low, y_close, returns = load_fx(data_start=0, data_end=6000, shift=3, window_size=window_size, pair=pair)"
   ],
   "id": "198151a29bc50298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "worm_up_period = 2\n",
    "n_correct = 0  # Initialize correct predictions count\n",
    "counter = 1\n",
    "eval_counter = 0\n",
    "predictions_list = []\n",
    "real_list = []\n",
    "classifiers_list = get_models()  # Assuming this function retrieves your models\n",
    "percent_accuracy_list = []\n",
    "returns_list = []\n",
    "for idx in range(5000, 5999):\n",
    "    \n",
    "    X_wnd_trn = X[0:idx]\n",
    "    X_wnd_tst = X[idx:idx+1]  # Testing on a forward step, not overlapping\n",
    "    y_close_wnd_trn = y_close[0:idx]\n",
    "    y_close_wnd_tst = y_close[idx:idx+1]  # Testing on a forward step, not overlapping\n",
    "    y_close_tst = y_close_wnd_tst[-1]  # Last future value\n",
    "    return_end = returns[idx]\n",
    "    # Get window length\n",
    "    arr_size = X_wnd_trn.shape[0]\n",
    "    arr = np.arange(arr_size)\n",
    "    n = int(arr_size * 0.99)  # 80% of data for training\n",
    "    start_time = datetime.now()\n",
    "    y_close_predict_list = []\n",
    "    \n",
    "    for classifier in classifiers_list:\n",
    "        # Shuffle and train each classifier with different parts of data\n",
    "        np.random.shuffle(arr)\n",
    "        X_wnd_trn_part = X_wnd_trn[arr, :]\n",
    "        y_close_wnd_trn_part = y_close_wnd_trn[arr]\n",
    "        \n",
    "        classifier.fit(X_wnd_trn_part, y_close_wnd_trn_part)\n",
    "        y_close_predict_wnd = classifier.predict(X_wnd_tst)\n",
    "        y_close_predict = y_close_predict_wnd[-1]  # Last prediction\n",
    "        y_close_predict_list.append(y_close_predict)\n",
    "\n",
    "    if counter > 1:\n",
    "        # Start collecting predictions after the first iteration\n",
    "        predictions_list.append(y_close_predict_list)  # Appending the new predictions\n",
    "        real_list.append(y_close_tst)  # Now append the real value after the corresponding prediction\n",
    "    \n",
    "    if counter > worm_up_period:\n",
    "        # Train the meta-model starting from the third iteration\n",
    "        predictions_list_mx = np.array(predictions_list)  # Convert predictions list to numpy array\n",
    "        real_list_mx = np.array(real_list)  # Convert real values list to numpy array\n",
    "        \n",
    "        # Debugging - Check the sizes of predictions_list_mx and real_list_mx\n",
    "        # print(f\"Iteration {counter}: Predictions list shape: {predictions_list_mx.shape}\")\n",
    "        # print(f\"Iteration {counter}: Real list length: {len(real_list_mx)}\")\n",
    "        \n",
    "        # Ensure shapes are consistent\n",
    "        if predictions_list_mx.shape[0] == len(real_list_mx):\n",
    "            # Train the meta-model\n",
    "            meta = GaussianNB()\n",
    "            X_recent = predictions_list_mx[:-1]\n",
    "            y_recent = real_list_mx[:-1]\n",
    "            meta.fit(X_recent, y_recent)  # Train on previous data\n",
    "            # print(np.shape(predictions_list_mx[:-1]), np.shape(real_list_mx[:-1]))\n",
    "            \n",
    "            # Predict using the meta-model on the current predictions\n",
    "            meta_prediction = meta.predict([y_close_predict_list])[0]  # Get the first (and only) prediction\n",
    "            # meta_prediction = meta.predict(predictions_list_mx)[-1]\n",
    "            # print(f\"Meta Prediction 1: {meta_prediction}\")\n",
    "            # Buy/Sell\n",
    "            if meta_prediction == 0:\n",
    "                # sell return negative\n",
    "                returns_list.append(-return_end)\n",
    "            else:\n",
    "                # buy return same\n",
    "                returns_list.append(return_end)\n",
    "            \n",
    "            # Increment n_correct if the prediction matches the real class\n",
    "            if meta_prediction == y_close_tst:\n",
    "                n_correct += 1\n",
    "            # Feature Selected Meta\n",
    "            # X_recent_df = pd.DataFrame(X_recent)\n",
    "            # y_recent_df = pd.Series(y_recent)\n",
    "            # max_k = np.shape(X_recent)[1]\n",
    "            # selected_features_list = mrmr_classif(X=X_recent_df, y=y_recent_df, K=max_k, show_progress=False)\n",
    "            # if len(selected_features_list) > 0:\n",
    "            #     selected_features = np.array(selected_features_list)\n",
    "            #     \n",
    "            #     print(np.shape(selected_features))\n",
    "            #     X_recent_selected = X_recent[:,selected_features]\n",
    "            #     print(X_recent_selected.shape)\n",
    "            #     \n",
    "            #     meta.fit(X_recent_selected, y_recent)  # Train on previous data\n",
    "            # else:\n",
    "            #     print('No features selected')\n",
    "                \n",
    "            eval_counter+=1    \n",
    "        else:\n",
    "            # Debugging - Print when the condition isn't met\n",
    "            print(f\"Skipping Meta Prediction at Iteration {counter}, inconsistent shapes\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    execution_time = end_time - start_time\n",
    "    if counter > worm_up_period:\n",
    "        print(f\"Test Accuracy for step {eval_counter} is: {n_correct / eval_counter} \")\n",
    "        percent_accuracy_list.append(n_correct / eval_counter)\n",
    "    else:\n",
    "        print(f'warming up:  {counter}')\n",
    "    \n",
    "    counter += 1  # Increment the counter"
   ],
   "id": "c6aa1d854e2ca7e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "percent_accuracy_list_mx = np.array(percent_accuracy_list)\n",
    "plt.plot(percent_accuracy_list_mx)"
   ],
   "id": "fde4c333efb633f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "returns_list_mx = np.array(returns_list)\n",
    "plt.bar(height= returns_list_mx, x = np.arange(np.shape(returns_list_mx)[0]))"
   ],
   "id": "4f3a4b34e0c21860",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot equity\n",
    "plt.plot(np.cumsum(returns_list_mx))"
   ],
   "id": "f7b45a0e0e9641f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "c = RidgeClassifierCV(class_weight={})"
   ],
   "id": "bcd5bdd15bfa42ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_end = 5000\n",
    "shift = 3\n",
    "X, y_high, y_low, y_close, returns = load_fx(data_start=0, data_end=data_end, shift=shift, window_size=window_size, pair=pair)\n",
    "def one_train_set(X, y):\n",
    "    # find indices of instances for both classes\n",
    "    y0_idx = np.where(y == 0)[0]\n",
    "    y1_idx = np.where(y == 1)[0]\n",
    "    \n",
    "    # Arrange array of indices for the class to be reduces here we reduce class 0\n",
    "    arr_idx = np.arange(len(y0_idx))\n",
    "    # shuffle indices for class 0\n",
    "    np.random.shuffle(arr_idx)\n",
    "    # take only half of instances with class 0\n",
    "    arr_idx_half = arr_idx[0:int(len(y1_idx)*0.5)]\n",
    "    \n",
    "    # class 0 take half size\n",
    "    y0 = y[arr_idx_half]\n",
    "    X0 = X[arr_idx_half,:]\n",
    "    # class 1 take full size\n",
    "    y1 = y[y1_idx]\n",
    "    X1 = X[y1_idx,:]\n",
    "    \n",
    "    # concatenate half zero and full one\n",
    "    # now X and y have 3/4 of the Instances - 2/3 are ones and 1/3 is zero\n",
    "    X1_major = np.concatenate((X0, X1), axis=0)\n",
    "    y1_selected = np.concatenate((y0, y1))\n",
    "    \n",
    "    # create indices for reduces Set\n",
    "    arr_idx_reduces = np.arange(len(y1_selected))\n",
    "    \n",
    "    # shuffle indices  Again Just in Case\n",
    "    np.random.shuffle(arr_idx_reduces)\n",
    "    \n",
    "    # Apply shuffled indices\n",
    "    X1_major = X1_major[arr_idx_reduces,:]\n",
    "    y1_selected = y1_selected[arr_idx_reduces]\n",
    "    \n",
    "    # Convert to DataFrame for mrmr_classif \n",
    "    X1_major_df = pd.DataFrame(X1_major)\n",
    "    y1_selected_df = pd.DataFrame(y1_selected)\n",
    "    \n",
    "    # Select K features. Return list.\n",
    "    selected_features_list = mrmr_classif(X=X1_major_df, y=y1_selected_df, K=4, show_progress=False)\n",
    "    # Convert list of selected indices to array\n",
    "    selected_features_idx1 = np.array(selected_features_list)\n",
    "    \n",
    "    # now X1_selected have 50% less instances with zero class and features dedicated for class one\n",
    "    X1_selected = X1_major[:,selected_features_idx1]\n",
    "    return selected_features_idx1\n",
    "\n",
    "selected_features_idx1 = one_train_set(X, y_close)\n",
    "# Define the classifier with class weights\n",
    "model0 = RandomForestClassifier(class_weight={0: 2, 1: 1})  # Assign higher weight to class 0\n",
    "model1 = RandomForestClassifier(class_weight={0: 1, 1: 2})  # Assign higher weight to class 1\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "# model1.fit(X[:,selected_features_idx1], y_close)\n",
    "# model1.fit(X1_selected, y1_selected)\n",
    "# model0.fit(X, y_close)\n",
    "model1.fit(X, y_close)"
   ],
   "id": "a60c07525dcb9bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_end = 5000\n",
    "shift = 2\n",
    "window_size = 10\n",
    "portion = 0.5\n",
    "\n",
    "zero_weight, one_weight = {0: 2, 1: 1}, {0: 1, 1: 2}\n",
    "# zero_weight, one_weight = {0: 1, 1: 1}, {0: 1, 1: 1}\n",
    "\n",
    "#             0         1          2        3         4         5  \n",
    "symbols = ['EURUSD', 'GBPUSD', 'AUDUSD', 'USDJPY', 'USDCAD', 'USDCHF']\n",
    "model_list = []\n",
    "model_dict = {}\n",
    "for idx in range(len(symbols)):\n",
    "    pair = symbols[idx]\n",
    "    X, y_high, y_low, y_close, returns = load_fx(data_start=0, data_end=data_end, shift=shift, window_size=window_size, pair=pair)\n",
    "    \n",
    "    # classifiers_collection_0 = get_models(zero_weight)\n",
    "    # classifiers_collection_1 = get_models(one_weight)\n",
    "    \n",
    "    classifiers_collection_0 = get_calibrated_models(zero_weight)\n",
    "    classifiers_collection_1 = get_calibrated_models(one_weight)\n",
    "    \n",
    "    # classifiers_collection_0 = get_models_small(zero_weight)\n",
    "    # classifiers_collection_1 = get_models_small(one_weight)\n",
    "\n",
    "\n",
    "    LEN = len(y_close)\n",
    "    arr_idx = np.arange(LEN)\n",
    "    \n",
    "    for model in classifiers_collection_0:\n",
    "        np.random.shuffle(arr_idx)\n",
    "        if hasattr(model, 'class_weight'):\n",
    "            # Fit model with class_weight\n",
    "            model.fit(X[arr_idx[:int(LEN*portion)],:], y_close[arr_idx[:int(LEN*portion)]])\n",
    "        else:\n",
    "            # Fit model with sample_weight since it doesn't support class_weight\n",
    "            sample_weights0 = np.where(y_close[arr_idx[:int(LEN*portion)]] == 0, 2, 1)\n",
    "            model.fit(X[arr_idx[:int(LEN*portion)],:], y_close[arr_idx[:int(LEN*portion)]], sample_weight=sample_weights0)\n",
    "            \n",
    "    for model in classifiers_collection_1:\n",
    "        np.random.shuffle(arr_idx)\n",
    "        if hasattr(model, 'class_weight'):\n",
    "            # Fit model with class_weight\n",
    "            model.fit(X[arr_idx[:int(LEN*portion)],:], y_close[arr_idx[:int(LEN*portion)]])\n",
    "        else:\n",
    "            # Define sample weights (e.g., higher weight for samples of class 1)\n",
    "            sample_weights1 = np.where(y_close[arr_idx[:int(LEN*portion)]] == 1, 2, 1)\n",
    "            model.fit(X[arr_idx[:int(LEN*portion)],:], y_close[arr_idx[:int(LEN*portion)]], sample_weight=sample_weights1)\n",
    "\n",
    "    # Store models in dictionary\n",
    "    model_dict[pair] = {'class_0': classifiers_collection_0, 'class_1': classifiers_collection_1}\n",
    "chime.success() \n",
    "print('Train models done')"
   ],
   "id": "76e11ed6a79d095a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data for all pairs at the start\n",
    "pair_data, return_data, class_data = {}, {}, {}\n",
    "for pair in symbols:\n",
    "    X, y_high, y_low, y_close, returns = load_fx(data_start=0, data_end=6000, shift=shift, window_size=window_size, pair=pair)\n",
    "    pair_data[pair] = X\n",
    "    return_data[pair] = returns\n",
    "    class_data[pair] = y_close\n",
    "print('Load data done')    "
   ],
   "id": "f1363c28b167c0b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_day, end_day = 5000, 5990\n",
    "buy_confidence_threshold = len(model_dict[pair]['class_1'])*0.95\n",
    "sell_confidence_threshold = len(model_dict[pair]['class_0'])*0.05\n",
    "buy_coount, sell_count = 0, 0\n",
    "prob_sum_class_0_list, prob_sum_class_1_list = [],[]\n",
    "profit = []\n",
    "probs_zero = np.zeros([end_day - start_day ,len(symbols), len(model_dict[pair]['class_0'])])\n",
    "probs_one = np.zeros([end_day - start_day ,len(symbols), len(model_dict[pair]['class_1'])])\n",
    "# Assume that for each pair we have two collections of models: class_0 (sell) and class_1 (buy)\n",
    "x_dim = 0\n",
    "class_data_eval = np.zeros([end_day - start_day ,len(symbols)])\n",
    "return_data_eval = np.zeros([end_day - start_day ,len(symbols)])\n",
    "for day in range(start_day, end_day):\n",
    "    # print(f\"Evaluating models for day {day}\")\n",
    "    y_dim = 0\n",
    "    # Loop through each currency pair\n",
    "    for pair in model_dict:\n",
    "        class_data_eval[x_dim][y_dim] = class_data[pair][day]\n",
    "        return_data_eval[x_dim][y_dim] = return_data[pair][day]\n",
    "        # Get data for the current day\n",
    "        X_today = pair_data[pair][day].reshape(1, -1)  # Ensure X_today is 2D\n",
    "        return_today = return_data[pair][day]\n",
    "        # Sum the probabilities for Class 0 (sell) and Class 1 (buy)\n",
    "        prob_sum_class_0 = 0  # Sum of sell probabilities\n",
    "        prob_sum_class_1 = 0  # Sum of buy probabilities\n",
    "        # prob_class_0_list = []\n",
    "        z_zero_dim = 0\n",
    "        # Evaluate models in class_0 (sell) ensemble\n",
    "        for model in model_dict[pair]['class_0']:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                prob_class_0 = model.predict_proba(X_today)[0,0] # Probability of Class 0 (sell) \n",
    "                prob_sum_class_0 += prob_class_0\n",
    "                probs_zero[x_dim, y_dim, z_zero_dim] = prob_class_0\n",
    "            else:\n",
    "                class_prediction = model.predict(X_today)\n",
    "                prob_sum_class_0 += class_prediction\n",
    "                probs_zero[x_dim, y_dim, z_zero_dim] = class_prediction\n",
    "            z_zero_dim += 1\n",
    "            \n",
    "        z_one_dim = 0\n",
    "        # Evaluate models in class_1 (buy) ensemble\n",
    "        for model in model_dict[pair]['class_1']:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                prob_class_1 = model.predict_proba(X_today)[0,1] # Probability of Class 1 (buy)\n",
    "                prob_sum_class_1 += prob_class_1\n",
    "                probs_one[x_dim, y_dim, z_one_dim] = prob_class_1\n",
    "            else:\n",
    "                class_prediction = model.predict(X_today)\n",
    "                prob_sum_class_1 += class_prediction\n",
    "                probs_one[x_dim, y_dim, z_one_dim] = class_prediction\n",
    "            z_one_dim += 1    \n",
    "        \n",
    "        prob_sum_class_0_list.append(prob_sum_class_0)\n",
    "        prob_sum_class_1_list.append(prob_sum_class_1)\n",
    "        \n",
    "        \n",
    "        # # Check if probabilities exceed the confidence threshold\n",
    "        if prob_sum_class_0 > prob_sum_class_1:\n",
    "            sell_count += 1\n",
    "            profit.append(-return_today)\n",
    "            #print(f\"{pair}: SELL signal with confidence {prob_sum_class_0}\") \n",
    "           \n",
    "        if prob_sum_class_1 > prob_sum_class_0:\n",
    "            buy_coount += 1\n",
    "            profit.append(return_today)\n",
    "\n",
    "        y_dim += 1\n",
    "    x_dim += 1\n",
    "prob_sum_class_0_list = np.array(prob_sum_class_0_list)\n",
    "prob_sum_class_1_list = np.array(prob_sum_class_1_list)\n",
    "profit = np.array(profit)\n",
    "chime.success()\n",
    "print('Inference done')"
   ],
   "id": "c7d0d0061ece7cfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(np.cumsum(profit), label='')\n",
    "plt.plot(profit, label='')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "22d999f64a010ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "struct = {}\n",
    "struct[\"probs_zero\"] = probs_zero\n",
    "struct[\"probs_one\"] = probs_one\n",
    "struct[\"class_data_eval\"] = class_data_eval\n",
    "struct[\"return_data_eval\"] = return_data_eval\n",
    "struct[\"symbols\"] = symbols\n",
    "import pickle\n",
    "\n",
    "with open('probs.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(struct, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "id": "8fc493dac7cd2a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('probs.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    struct = pickle.load(f)\n",
    "probs_zero = struct[\"probs_zero\"]\n",
    "probs_one = struct[\"probs_one\"] \n",
    "class_data_eval = struct[\"class_data_eval\"] \n",
    "return_data_eval = struct[\"return_data_eval\"] \n",
    "symbols = struct[\"symbols\"]"
   ],
   "id": "ea54e19db126a7c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(prob_sum_class_0_list, label='Class 0')\n",
    "plt.plot(prob_sum_class_1_list, label='Class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7162ccb2e0d8e85b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for x_idx in range(probs_one.shape[0]):\n",
    "    print('---------')\n",
    "    for y_idx in range(probs_one.shape[1]):\n",
    "        formatted_a = \",\".join([f\"{x:.2f}\" for x in probs_one[x_idx, y_idx, :]])\n",
    "        formatted_b = \",\".join([f\"{x:.2f}\" for x in probs_zero[x_idx, y_idx, :]])\n",
    "        print(f\"{symbols[y_idx]} p_one: [{formatted_a}], p_zero: [{formatted_b}] class: {class_data_eval[x_idx, y_idx]}\")"
   ],
   "id": "e06d8052f5f48e05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'probs_zero: {probs_zero.shape} | probs_one: {probs_one.shape} | Items: {probs_zero.shape[0]} | Pairs: {probs_zero.shape[1]} | Models: {probs_zero.shape[2]}')\n",
    "print(f'class_data_eval: {class_data_eval.shape} | return_data: {return_data_eval.shape}')"
   ],
   "id": "e3e89af05997d809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import f_classif, mutual_info_classif, chi2, SelectFromModel\n",
    "f_statistic, p_values_f = f_classif(X,y_close)\n",
    "chi2_statistic, p_values_chi2 = chi2(X,y_close)\n",
    "mi = mutual_info_classif(X, y_close)\n",
    "sm = SelectFromModel(estimator=RandomForestClassifier(n_estimators=100), threshold=0.5)\n",
    "sm.fit(X, y_close)\n",
    "sm.get_feature_names_out()"
   ],
   "id": "dae41745bd71d2a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reload more data\n",
    "X, y_high, y_low, y_close, returns = load_fx(data_start=0, data_end=6400, shift=shift, window_size=window_size, pair=pair)\n",
    "counter, one_count = 0, 0\n",
    "profit = []\n",
    "y_pred_list = []\n",
    "y_pred_proba_list = []\n",
    "for idx in range(data_end-10, len(y_high)-1):\n",
    "    X_wnd = X[idx,:].reshape(1, -1)\n",
    "    #X_wnd_selected = X_wnd[selected_features_idx1].reshape(1, -1)\n",
    "    X_wnd_selected = X_wnd\n",
    "    # y_pred = model1.predict(X_wnd_selected)\n",
    "    \n",
    "    # y_pred_list.append(y_pred)\n",
    "    for models in model_list:\n",
    "        model0, model1 = model[0], model[1]\n",
    "        \n",
    "        \n",
    "        if hasattr(model1, 'predict_proba'):\n",
    "            y_pred_proba0 = model0.predict_proba(X_wnd_selected)\n",
    "            y_pred_proba1 = model1.predict_proba(X_wnd_selected)\n",
    "            y_pred_proba_list.append(y_pred_proba1)\n",
    "           \n",
    "            if float(y_pred_proba1[:,1]) > 0.6:\n",
    "                 # print(\"buy\")\n",
    "                 profit.append(returns[idx])\n",
    "            if float(y_pred_proba0[:,0]) > 0.6:\n",
    "                 # print(\"sell\")\n",
    "                 profit.append(-returns[idx])\n",
    "        else:\n",
    "            print(\"No probability\")\n",
    "            y_pred_0 = model0.predict(X_wnd_selected)\n",
    "            y_pred_1 = model1.predict(X_wnd_selected)\n",
    "            # y_pred_proba_list.append(y_pred_proba1)\n",
    "           \n",
    "            if y_pred_1 == 1:\n",
    "                 profit.append(returns[idx])\n",
    "            if y_pred_0 == 0:\n",
    "                 profit.append(-returns[idx])"
   ],
   "id": "dab3c7b58effb095",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(f\"Test Accuracy for step {counter} is: {counter / one_count}\")  \n",
    "y_pred_proba_list = np.array(y_pred_proba_list).squeeze()\n",
    "y_pred_list = np.array(y_pred_list).squeeze()\n",
    "profit = np.array(profit)\n",
    "plt.plot(np.cumsum(profit))"
   ],
   "id": "b9428869e40b68b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Foo():\n",
    "    def __init__(self, X, y, max_k):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_k = max_k\n",
    "        self.y0_idx= np.where(self.y==0)[0]\n",
    "        self.y1_idx= np.where(self.y==1)[0]\n",
    "        self.y0 = y_close[self.y0_idx] \n",
    "        self.y1 = y_close[self.y1_idx]\n",
    "        self.X0 = X[self.y0_idx,:]\n",
    "        self.X1 = X[self.y1_idx,:]\n",
    "        self.X0_df = pd.DataFrame(self.X0)\n",
    "        self.X1_df = pd.DataFrame(self.X1)\n",
    "        self.y0_df = pd.Series(self.y0)\n",
    "        self.y1_df = pd.Series(self.y1)\n",
    "\n",
    "        selected_features_list0 = mrmr_classif(X=self.X0_df, y=self.y0_df, K=self.max_k, show_progress=False)\n",
    "        selected_features0 = np.array(selected_features_list0)\n",
    "        print(selected_features0.shape)\n",
    "        self.X0_selected = self.X[:,selected_features0]\n",
    "        \n",
    "        selected_features_list1 = mrmr_classif(X=self.X1_df, y=self.y1_df, K=self.max_k, show_progress=False)\n",
    "        selected_features1 = np.array(selected_features_list1)\n",
    "        self.X1_selected = self.X[:,selected_features1]\n",
    "    def get_zero(self):\n",
    "        return self.X0_selected\n",
    "    def get_one(self):\n",
    "        return self.X1_selected\n",
    "    \n",
    "    "
   ],
   "id": "9d14341b9dd34756",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f = Foo(X, y_close, 5)\n",
    "X0 = f.get_zero()"
   ],
   "id": "2be4d4560c3863b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3eb16b34efb737f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d03d8cc92f38bb74",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
