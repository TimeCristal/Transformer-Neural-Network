{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###\t1.\tInstall Necessary Libraries:\n",
    "Ensure that you have the necessary libraries installed. You can install PyTorch-Geometric and its dependencies using pip if you haven’t already:"
   ],
   "id": "ef426fa6397a4116"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T22:01:45.302343Z",
     "start_time": "2024-08-22T22:01:45.291123Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install torch torch-geometric",
   "id": "initial_id",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###\t2.\tLoad and Prepare the Dataset:\n",
    "We’ll use the Cora dataset, which is a standard citation network dataset. The nodes represent documents, and the edges represent citations between them."
   ],
   "id": "e17dde0561c63c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T22:01:45.338080Z",
     "start_time": "2024-08-22T22:01:45.304190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric import transforms\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# Use the first graph in the dataset\n",
    "data = dataset[0]\n",
    "\n",
    "# Split the edges into training, validation, and test sets\n",
    "transform = RandomLinkSplit(is_undirected=True, key=\"edge_label\", split_labels=True)\n",
    "\n",
    "train_data, val_data, test_data = transform(dataset[0])"
   ],
   "id": "a5f4bfab39622ded",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###\t3.\tDefine the VGAE Model:\n",
    "We need to define the VGAE model, which consists of an encoder that maps the input features into a latent space. The encoder is typically implemented using Graph Convolutional Networks (GCNs)."
   ],
   "id": "e76734ca84783d48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T22:01:45.349676Z",
     "start_time": "2024-08-22T22:01:45.340628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logstd = self.conv_logstd(x, edge_index)\n",
    "        return mu, logstd\n",
    "\n",
    "# Initialize the VGAE model\n",
    "out_channels = 16\n",
    "model = VGAE(GCNEncoder(dataset.num_features, out_channels))"
   ],
   "id": "2c6bd8c2f741c16",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###\t4.\tTraining the Model:\n",
    "The model is trained by optimizing a loss function that combines a reconstruction loss (which ensures the graph is reconstructed correctly) and a KL divergence loss (which regularizes the latent space)."
   ],
   "id": "2145d6d6aad9fda8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T22:01:48.758936Z",
     "start_time": "2024-08-22T22:01:45.350937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.edge_index)\n",
    "    loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Train the model for 200 epochs\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ],
   "id": "7fe6efb800a06eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 3.5240\n",
      "Epoch: 001, Loss: 2.8785\n",
      "Epoch: 002, Loss: 2.5037\n",
      "Epoch: 003, Loss: 2.2285\n",
      "Epoch: 004, Loss: 1.9997\n",
      "Epoch: 005, Loss: 1.7829\n",
      "Epoch: 006, Loss: 1.6121\n",
      "Epoch: 007, Loss: 1.4911\n",
      "Epoch: 008, Loss: 1.4171\n",
      "Epoch: 009, Loss: 1.3659\n",
      "Epoch: 010, Loss: 1.3272\n",
      "Epoch: 011, Loss: 1.3136\n",
      "Epoch: 012, Loss: 1.2871\n",
      "Epoch: 013, Loss: 1.2628\n",
      "Epoch: 014, Loss: 1.2442\n",
      "Epoch: 015, Loss: 1.1994\n",
      "Epoch: 016, Loss: 1.1672\n",
      "Epoch: 017, Loss: 1.1505\n",
      "Epoch: 018, Loss: 1.1083\n",
      "Epoch: 019, Loss: 1.0933\n",
      "Epoch: 020, Loss: 1.0909\n",
      "Epoch: 021, Loss: 1.0823\n",
      "Epoch: 022, Loss: 1.0713\n",
      "Epoch: 023, Loss: 1.0498\n",
      "Epoch: 024, Loss: 1.0218\n",
      "Epoch: 025, Loss: 1.0118\n",
      "Epoch: 026, Loss: 1.0059\n",
      "Epoch: 027, Loss: 1.0100\n",
      "Epoch: 028, Loss: 1.0046\n",
      "Epoch: 029, Loss: 0.9797\n",
      "Epoch: 030, Loss: 0.9950\n",
      "Epoch: 031, Loss: 0.9810\n",
      "Epoch: 032, Loss: 0.9715\n",
      "Epoch: 033, Loss: 0.9756\n",
      "Epoch: 034, Loss: 0.9647\n",
      "Epoch: 035, Loss: 0.9717\n",
      "Epoch: 036, Loss: 0.9670\n",
      "Epoch: 037, Loss: 0.9609\n",
      "Epoch: 038, Loss: 0.9428\n",
      "Epoch: 039, Loss: 0.9392\n",
      "Epoch: 040, Loss: 0.9368\n",
      "Epoch: 041, Loss: 0.9338\n",
      "Epoch: 042, Loss: 0.9333\n",
      "Epoch: 043, Loss: 0.9205\n",
      "Epoch: 044, Loss: 0.9328\n",
      "Epoch: 045, Loss: 0.9276\n",
      "Epoch: 046, Loss: 0.9184\n",
      "Epoch: 047, Loss: 0.9326\n",
      "Epoch: 048, Loss: 0.9175\n",
      "Epoch: 049, Loss: 0.9266\n",
      "Epoch: 050, Loss: 0.9055\n",
      "Epoch: 051, Loss: 0.9122\n",
      "Epoch: 052, Loss: 0.9142\n",
      "Epoch: 053, Loss: 0.9132\n",
      "Epoch: 054, Loss: 0.9053\n",
      "Epoch: 055, Loss: 0.9107\n",
      "Epoch: 056, Loss: 0.9117\n",
      "Epoch: 057, Loss: 0.9024\n",
      "Epoch: 058, Loss: 0.9031\n",
      "Epoch: 059, Loss: 0.9066\n",
      "Epoch: 060, Loss: 0.9140\n",
      "Epoch: 061, Loss: 0.8996\n",
      "Epoch: 062, Loss: 0.9002\n",
      "Epoch: 063, Loss: 0.9009\n",
      "Epoch: 064, Loss: 0.8965\n",
      "Epoch: 065, Loss: 0.9053\n",
      "Epoch: 066, Loss: 0.8952\n",
      "Epoch: 067, Loss: 0.8974\n",
      "Epoch: 068, Loss: 0.8880\n",
      "Epoch: 069, Loss: 0.8861\n",
      "Epoch: 070, Loss: 0.8834\n",
      "Epoch: 071, Loss: 0.8866\n",
      "Epoch: 072, Loss: 0.8853\n",
      "Epoch: 073, Loss: 0.8951\n",
      "Epoch: 074, Loss: 0.8913\n",
      "Epoch: 075, Loss: 0.8858\n",
      "Epoch: 076, Loss: 0.8827\n",
      "Epoch: 077, Loss: 0.8822\n",
      "Epoch: 078, Loss: 0.8760\n",
      "Epoch: 079, Loss: 0.8839\n",
      "Epoch: 080, Loss: 0.8858\n",
      "Epoch: 081, Loss: 0.8814\n",
      "Epoch: 082, Loss: 0.8911\n",
      "Epoch: 083, Loss: 0.8806\n",
      "Epoch: 084, Loss: 0.8833\n",
      "Epoch: 085, Loss: 0.8840\n",
      "Epoch: 086, Loss: 0.8911\n",
      "Epoch: 087, Loss: 0.8812\n",
      "Epoch: 088, Loss: 0.8716\n",
      "Epoch: 089, Loss: 0.8841\n",
      "Epoch: 090, Loss: 0.8736\n",
      "Epoch: 091, Loss: 0.8710\n",
      "Epoch: 092, Loss: 0.8791\n",
      "Epoch: 093, Loss: 0.8767\n",
      "Epoch: 094, Loss: 0.8717\n",
      "Epoch: 095, Loss: 0.8823\n",
      "Epoch: 096, Loss: 0.8783\n",
      "Epoch: 097, Loss: 0.8652\n",
      "Epoch: 098, Loss: 0.8654\n",
      "Epoch: 099, Loss: 0.8747\n",
      "Epoch: 100, Loss: 0.8739\n",
      "Epoch: 101, Loss: 0.8664\n",
      "Epoch: 102, Loss: 0.8666\n",
      "Epoch: 103, Loss: 0.8785\n",
      "Epoch: 104, Loss: 0.8678\n",
      "Epoch: 105, Loss: 0.8675\n",
      "Epoch: 106, Loss: 0.8675\n",
      "Epoch: 107, Loss: 0.8705\n",
      "Epoch: 108, Loss: 0.8756\n",
      "Epoch: 109, Loss: 0.8655\n",
      "Epoch: 110, Loss: 0.8698\n",
      "Epoch: 111, Loss: 0.8647\n",
      "Epoch: 112, Loss: 0.8716\n",
      "Epoch: 113, Loss: 0.8654\n",
      "Epoch: 114, Loss: 0.8612\n",
      "Epoch: 115, Loss: 0.8740\n",
      "Epoch: 116, Loss: 0.8606\n",
      "Epoch: 117, Loss: 0.8666\n",
      "Epoch: 118, Loss: 0.8690\n",
      "Epoch: 119, Loss: 0.8583\n",
      "Epoch: 120, Loss: 0.8723\n",
      "Epoch: 121, Loss: 0.8686\n",
      "Epoch: 122, Loss: 0.8614\n",
      "Epoch: 123, Loss: 0.8680\n",
      "Epoch: 124, Loss: 0.8580\n",
      "Epoch: 125, Loss: 0.8578\n",
      "Epoch: 126, Loss: 0.8669\n",
      "Epoch: 127, Loss: 0.8625\n",
      "Epoch: 128, Loss: 0.8691\n",
      "Epoch: 129, Loss: 0.8622\n",
      "Epoch: 130, Loss: 0.8583\n",
      "Epoch: 131, Loss: 0.8579\n",
      "Epoch: 132, Loss: 0.8670\n",
      "Epoch: 133, Loss: 0.8542\n",
      "Epoch: 134, Loss: 0.8607\n",
      "Epoch: 135, Loss: 0.8554\n",
      "Epoch: 136, Loss: 0.8663\n",
      "Epoch: 137, Loss: 0.8539\n",
      "Epoch: 138, Loss: 0.8555\n",
      "Epoch: 139, Loss: 0.8564\n",
      "Epoch: 140, Loss: 0.8545\n",
      "Epoch: 141, Loss: 0.8637\n",
      "Epoch: 142, Loss: 0.8518\n",
      "Epoch: 143, Loss: 0.8572\n",
      "Epoch: 144, Loss: 0.8586\n",
      "Epoch: 145, Loss: 0.8544\n",
      "Epoch: 146, Loss: 0.8539\n",
      "Epoch: 147, Loss: 0.8541\n",
      "Epoch: 148, Loss: 0.8653\n",
      "Epoch: 149, Loss: 0.8494\n",
      "Epoch: 150, Loss: 0.8507\n",
      "Epoch: 151, Loss: 0.8597\n",
      "Epoch: 152, Loss: 0.8462\n",
      "Epoch: 153, Loss: 0.8450\n",
      "Epoch: 154, Loss: 0.8461\n",
      "Epoch: 155, Loss: 0.8545\n",
      "Epoch: 156, Loss: 0.8566\n",
      "Epoch: 157, Loss: 0.8505\n",
      "Epoch: 158, Loss: 0.8554\n",
      "Epoch: 159, Loss: 0.8551\n",
      "Epoch: 160, Loss: 0.8516\n",
      "Epoch: 161, Loss: 0.8506\n",
      "Epoch: 162, Loss: 0.8424\n",
      "Epoch: 163, Loss: 0.8467\n",
      "Epoch: 164, Loss: 0.8557\n",
      "Epoch: 165, Loss: 0.8442\n",
      "Epoch: 166, Loss: 0.8587\n",
      "Epoch: 167, Loss: 0.8453\n",
      "Epoch: 168, Loss: 0.8530\n",
      "Epoch: 169, Loss: 0.8540\n",
      "Epoch: 170, Loss: 0.8452\n",
      "Epoch: 171, Loss: 0.8572\n",
      "Epoch: 172, Loss: 0.8441\n",
      "Epoch: 173, Loss: 0.8434\n",
      "Epoch: 174, Loss: 0.8587\n",
      "Epoch: 175, Loss: 0.8468\n",
      "Epoch: 176, Loss: 0.8423\n",
      "Epoch: 177, Loss: 0.8480\n",
      "Epoch: 178, Loss: 0.8437\n",
      "Epoch: 179, Loss: 0.8427\n",
      "Epoch: 180, Loss: 0.8480\n",
      "Epoch: 181, Loss: 0.8476\n",
      "Epoch: 182, Loss: 0.8361\n",
      "Epoch: 183, Loss: 0.8371\n",
      "Epoch: 184, Loss: 0.8517\n",
      "Epoch: 185, Loss: 0.8455\n",
      "Epoch: 186, Loss: 0.8421\n",
      "Epoch: 187, Loss: 0.8392\n",
      "Epoch: 188, Loss: 0.8509\n",
      "Epoch: 189, Loss: 0.8411\n",
      "Epoch: 190, Loss: 0.8383\n",
      "Epoch: 191, Loss: 0.8442\n",
      "Epoch: 192, Loss: 0.8392\n",
      "Epoch: 193, Loss: 0.8437\n",
      "Epoch: 194, Loss: 0.8477\n",
      "Epoch: 195, Loss: 0.8462\n",
      "Epoch: 196, Loss: 0.8417\n",
      "Epoch: 197, Loss: 0.8451\n",
      "Epoch: 198, Loss: 0.8462\n",
      "Epoch: 199, Loss: 0.8405\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###\t5.\tEvaluate the Model:\n",
    "After training, we can evaluate the model’s performance on the test set. The evaluation typically involves predicting the probability of edges (links) between nodes and comparing them to the true test edges."
   ],
   "id": "8640e14e743f1b75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T22:01:48.785344Z",
     "start_time": "2024-08-22T22:01:48.761391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# Test the model\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        pos_pred = model.decode(z, data.pos_edge_label_index).cpu().numpy()\n",
    "        neg_pred = model.decode(z, data.neg_edge_label_index).cpu().numpy()\n",
    "\n",
    "        y_pred = torch.cat([torch.tensor(pos_pred), torch.tensor(neg_pred)])\n",
    "        y_true = torch.cat([torch.ones(pos_pred.shape[0]), torch.zeros(neg_pred.shape[0])])\n",
    "\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        ap_score = average_precision_score(y_true, y_pred)\n",
    "\n",
    "        return roc_auc, ap_score\n",
    "\n",
    "roc_auc, ap_score = test(test_data)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}, Average Precision Score: {ap_score:.4f}')"
   ],
   "id": "16f0ff13b208eff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.9009, Average Precision Score: 0.9086\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
