{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:36:20.529459Z",
     "start_time": "2024-10-10T14:36:18.698094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from KL.kl.utils import load_fx\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "X, _, _, y, _ = load_fx(data_start=0, data_end=6000, shift=3, window_size=10, pair='EURUSD')\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Train/Test split\n",
    "train_data, test_data = X[:5500], X[5500:]\n",
    "train_target, test_target = y[:5500], y[5500:]"
   ],
   "id": "1818a7d64932c65b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:36:20.539053Z",
     "start_time": "2024-10-10T14:36:20.531098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        # 3. Regularization (Dropout, L2 Regularization):\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Dropout to avoid overfitting\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)  # LSTM encoder\n",
    "        h_n = self.dropout(h_n)  # Apply dropout\n",
    "        mu = self.fc_mu(h_n[-1])\n",
    "        logvar = self.fc_logvar(h_n[-1])\n",
    "        return mu, logvar\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_size, output_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "# VAE Model with Cross-Entropy and KL Divergence\n",
    "class VAE(nn.Module):  \n",
    "    def __init__(self, input_size, hidden_size, latent_size, output_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, input_size)  # Decoder transforms X\n",
    "        self.fc_pred = nn.Linear(latent_size, output_size)  # Prediction layer for y\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Output transformed X (via decoder)\n",
    "        X_transformed = self.decoder(z)\n",
    "        \n",
    "        # Output predicted y (via latent representation)\n",
    "        y_pred = self.fc_pred(z)\n",
    "        \n",
    "        return y_pred, X_transformed, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Deterministic latent space (no randomness)\n",
    "        return mu  # No sampling\n",
    "\n",
    "# Loss function combining KL Divergence and Cross-Entropy\n",
    "def loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "    # Cross-Entropy Loss for classification (minimize)\n",
    "    cross_entropy = nn.BCEWithLogitsLoss()(recon_x, x)\n",
    "\n",
    "    # KL Divergence (maximize)\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Combine the losses, minimizing cross-entropy and maximizing KL divergence\n",
    "    return cross_entropy - beta * kl_divergence"
   ],
   "id": "4c792b3e133542fd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:36:20.543902Z",
     "start_time": "2024-10-10T14:36:20.540527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, validation_loss):\n",
    "        if validation_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = validation_loss\n",
    "            self.counter = 0  # Reset counter if validation loss improves\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience  # Return True if stopping criteria met"
   ],
   "id": "f97a8a377873b43c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:36:20.553156Z",
     "start_time": "2024-10-10T14:36:20.546214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Cross-validation setup\n",
    "def cross_validate(data, target, model, criterion, optimizer, epochs=10, print_every = 10, k_folds=5, beta=1.0):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(data)):\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        # Reshape the data to (samples, 1, features) to match LSTM input\n",
    "        train_data = data[train_idx].unsqueeze(1)  # Adding a sequence length of 1\n",
    "        train_target = target[train_idx].unsqueeze(-1)  # Reshape to [batch_size, 1]\n",
    "        test_data = data[test_idx].unsqueeze(1)    # Adding a sequence length of 1\n",
    "        test_target = target[test_idx].unsqueeze(-1)    # Reshape to [batch_size, 1]\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = TensorDataset(train_data, train_target)\n",
    "        test_dataset = TensorDataset(test_data, test_target)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Reset model and optimizer\n",
    "        model.apply(reset_weights)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch_data, batch_target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                # outputs, mu, logvar = model(batch_data)\n",
    "                # Forward pass (predict y and transform X)\n",
    "                y_pred, X_transformed, mu, logvar = model(batch_data)\n",
    "                loss = criterion(y_pred, batch_target, mu, logvar, beta=beta)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluation on the test set\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_data, batch_target in test_loader:\n",
    "                    y_pred, X_transformed, mu, logvar = model(batch_data)\n",
    "                    loss = criterion(y_pred, batch_target, mu, logvar, beta=beta)\n",
    "                    test_loss += loss.item()\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Test Loss: {test_loss / len(test_loader)}\")      "
   ],
   "id": "5139021175f28f04",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:36:20.560973Z",
     "start_time": "2024-10-10T14:36:20.554386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validate_with_early_stopping(data, target, model, criterion, optimizer, epochs=1000, k_folds=5, patience=10, beta=1.0, print_every=10):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(data)):\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        train_data = data[train_idx].unsqueeze(1)  # Add sequence length of 1\n",
    "        train_target = target[train_idx].unsqueeze(-1)  # Reshape target\n",
    "        test_data = data[test_idx].unsqueeze(1)  # Add sequence length of 1\n",
    "        test_target = target[test_idx].unsqueeze(-1)  # Reshape target\n",
    "        \n",
    "        train_dataset = TensorDataset(train_data, train_target)\n",
    "        test_dataset = TensorDataset(test_data, test_target)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        model.apply(reset_weights)  # Reset model weights for each fold\n",
    "        \n",
    "        early_stopper = EarlyStopping(patience=patience)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch_data, batch_target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred, X_transformed, mu, logvar = model(batch_data)\n",
    "                loss = criterion(y_pred, batch_target, mu, logvar, beta=beta)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_data, batch_target in test_loader:\n",
    "                    y_pred, X_transformed, mu, logvar = model(batch_data)\n",
    "                    loss = criterion(y_pred, batch_target, mu, logvar, beta=beta)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            val_loss /= len(test_loader)  # Average validation loss\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Validation Loss: {val_loss}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if early_stopper.step(val_loss):\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Best validation loss for fold {fold+1}: {early_stopper.best_loss}\")"
   ],
   "id": "349adf97bb7bba5c",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-10T14:36:20.561897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Weight reset for cross-validation\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.LSTM):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# Model initialization\n",
    "# 1. Increase Hidden Size and Latent Size:\n",
    "factor = 16 # --> 1024\n",
    "input_size = 8  # Number of features in the sequence\n",
    "hidden_size = 64*factor  # Hidden size for LSTM\n",
    "latent_size = 3#32*factor  # Latent space size\n",
    "output_size = 1  # Binary class output\n",
    "\n",
    "model = VAE(input_size, hidden_size, latent_size, output_size)\n",
    "# 2. try smaller Lr\n",
    "Lr = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=Lr, weight_decay=1e-5)\n",
    "\n",
    "# Cross-Validation on your training data\n",
    "# 3. Encorage KL divergence - > Larger Beta\n",
    "Beta = 0.5#10\n",
    "# cross_validate(train_data, train_target, model, loss_function, optimizer, epochs=1500, beta=Beta, print_every=500)\n",
    "cross_validate_with_early_stopping(data=train_data, target=train_target, model=model, criterion=loss_function, optimizer=optimizer, epochs=20000, beta=Beta, print_every=1000)"
   ],
   "id": "7380a76d06bd2886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/20000 | Validation Loss: 0.7090647419293722\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_batch(model, test_data):\n",
    "    test_dataset = TensorDataset(test_data, test_target)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_predicted = []  # List to store all outputs\n",
    "    all_targets = []  # List to store all targets\n",
    "    all_transformed_X = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_target in test_loader:\n",
    "            # Ensure batch_target has correct shape [batch_size, 1]\n",
    "            batch_target = batch_target.unsqueeze(-1)  # Reshape target to match output size\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred, X_transformed, mu, logvar = model(batch_data)\n",
    "            \n",
    "            all_transformed_X.append(X_transformed)\n",
    "            # Accumulate outputs and targets\n",
    "            all_predicted.append(y_pred)\n",
    "            all_targets.append(batch_target)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_function(y_pred, batch_target, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Combine the outputs and targets from all batches\n",
    "    all_predicted = torch.cat(all_predicted, dim=0)  # Concatenate along the batch dimension\n",
    "    all_targets = torch.cat(all_targets, dim=0)  # Concatenate along the batch dimension\n",
    "    all_transformed_X = torch.cat(all_transformed_X, dim=0)\n",
    "    return all_predicted, all_targets, all_transformed_X, test_loss"
   ],
   "id": "d8be8aaa4e95ddb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Final evaluation on the last 500 test samples\n",
    "test_data = test_data.view(-1, 1, 8)  # Ensure test data has correct 3D shape [batch_size, sequence_length, input_size]\n",
    "\n",
    "all_predicted, all_targets, all_transformed_X, test_loss = predict_batch(model, test_data)\n",
    "\n",
    "print(f\"Final Test Loss: {test_loss / test_data.shape[0]}\")\n",
    "\n",
    "# Now analyze the accumulated outputs and targets\n",
    "print(f\"All outputs shape: {all_predicted.shape}\")\n",
    "print(f\"All targets shape: {all_targets.shape}\")\n",
    "print(f\"All transformed X shape: {all_transformed_X.shape}\")\n",
    "\n",
    "# all_transformed_X = all_transformed_X.numpy()\n",
    "\n",
    "# Example: Convert outputs to probabilities and compute accuracy\n",
    "predicted = torch.sigmoid(all_predicted)  # Apply sigmoid to get probabilities\n",
    "predicted_classes = (predicted > 0.5).float()  # Convert probabilities to binary predictions\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (predicted_classes == all_targets).float().mean()\n",
    "print(f\"Accuracy: {accuracy.item() * 100:.2f}%\")"
   ],
   "id": "c5f9c0d23b886bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Check class distribution in the training data\n",
    "print(f\"Class 0 percentage: {(train_target == 0).float().mean().item() * 100:.2f}%\")\n",
    "print(f\"Class 1 percentage: {(train_target == 1).float().mean().item() * 100:.2f}%\")"
   ],
   "id": "7433122982b5f447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the transformed X to numpy for PCA\n",
    "all_transformed_X_np = all_transformed_X.detach().cpu().numpy()\n",
    "\n",
    "# Perform PCA to reduce dimensions to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(all_transformed_X_np)\n",
    "\n",
    "print(f\"Reduced X shape: {X_pca.shape}\")  # This should be (n_samples, 2)\n",
    "\n",
    "# Convert the predicted y to numpy for plotting\n",
    "all_predicted_y_np = predicted_classes#all_predicted_y.detach().cpu().numpy()\n"
   ],
   "id": "da97157ba71bfa3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Scatter plot of the 2D PCA components, colored by class\n",
    "all_targets = all_targets.detach().cpu().numpy()\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=all_predicted_y_np, cmap='coolwarm', alpha=0.7)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=all_targets, cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(label='Class')\n",
    "plt.title('2D PCA of Transformed X')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.savefig('2D_PCA_of_Transformed_X.png')\n",
    "plt.show()"
   ],
   "id": "e652efbd74ec0ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "np.savetxt('all_predicted_y_np.csv', all_predicted_y_np)",
   "id": "5c55b64381a391f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from KL.kl.utils import load_fx_3d\n",
    "X_3d_matrix, y, returns = load_fx_3d(shift=3, pair = 'EURUSD')"
   ],
   "id": "fa8d742b02eeaaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "pca = PCA(n_components=2)\n",
    "test_data = test_data.detach().numpy()\n",
    "test_data = test_data[:,0,:]\n",
    "X_pca = pca.fit_transform(test_data)"
   ],
   "id": "1e3714b2f1e739de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=test_target, cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(label='Class')\n",
    "plt.title('2D PCA of Original X')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.savefig('2D_PCA_of_Original_X.png')\n",
    "plt.show()"
   ],
   "id": "d5757be9dfe1e09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c006c9e78c305c2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
