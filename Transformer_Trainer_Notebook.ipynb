{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- Remove non alphanumeric characters for simple training"
   ],
   "metadata": {
    "id": "VmYGVziz50tu"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FOwRggVcwtzP",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.400117Z",
     "start_time": "2024-08-13T22:44:45.395438Z"
    }
   },
   "source": [
    "from transformer import Transformer # this is the transformer.py file\n",
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": [
    "english_file = '/Users/krasimirtrifonov/Documents/GitHub/Transformer-Neural-Network/dataset/Present.txt'\n",
    "kannada_file = '/Users/krasimirtrifonov/Documents/GitHub/Transformer-Neural-Network/dataset/Future.txt'\n",
    "\n",
    "# Generated this by filtering Appendix code\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "kannada_vocabulary = [START_TOKEN, ' ', '0', '1','2','3','4','5','6','7','8','9', PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "english_vocabulary = kannada_vocabulary\n"
   ],
   "metadata": {
    "id": "6TApzOj5xCwR",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.585111Z",
     "start_time": "2024-08-13T22:44:45.582076Z"
    }
   },
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}\n",
    "kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ],
   "metadata": {
    "id": "gA8ESmCrNoc7",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.627741Z",
     "start_time": "2024-08-13T22:44:45.624942Z"
    }
   },
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "source": [
    "with open(english_file, 'r') as file:\n",
    "    english_sentences = file.readlines()\n",
    "with open(kannada_file, 'r') as file:\n",
    "    kannada_sentences = file.readlines()\n",
    "\n",
    "# Limit Number of sentences\n",
    "TOTAL_SENTENCES = 200000\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
    "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
    "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
   ],
   "metadata": {
    "id": "9SYGjRdoxRg-",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.660198Z",
     "start_time": "2024-08-13T22:44:45.654938Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": "english_sentences[:10]",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUB-BkgFxXfM",
    "outputId": "bcf7e19c-d1df-4b69-bdfa-eb74ac1a4338",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:45:42.657539Z",
     "start_time": "2024-08-13T22:45:42.652919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['405 763543']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "source": [
    "kannada_sentences[:10]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OT-aznAxc5U",
    "outputId": "716c4145-fdc2-4bb0-e883-c3dcad30c2da",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.685041Z",
     "start_time": "2024-08-13T22:44:45.681893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['33 54',\n",
       " '552',\n",
       " '92 46525 4 553334',\n",
       " '5654 82545',\n",
       " '6548554 4838 544337 54',\n",
       " '5643 2355442 4673 56',\n",
       " '465 4 25543 5456',\n",
       " '477554 65',\n",
       " '3544744 545',\n",
       " '20455 344545 55']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8VAutsTxlaR",
    "outputId": "ff8fba72-020d-4f0c-b3c5-31c102b6fe9b",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.765509Z",
     "start_time": "2024-08-13T22:44:45.760102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length Kannada: 24.87999999999988\n",
      "97th percentile length English: 28.0\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": [
    "max_sequence_length = 200\n",
    "\n",
    "def is_valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(kannada_sentences)):\n",
    "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
    "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
    "      and is_valid_tokens(kannada_sentence, kannada_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HG9ezqvaxl4b",
    "outputId": "d13be774-ca07-4333-856e-76186f71caae",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.824302Z",
     "start_time": "2024-08-13T22:44:45.815715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2005\n",
      "Number of valid sentences: 2005\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "source": [
    "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
   ],
   "metadata": {
    "id": "o80QDn4CxsV7",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.833358Z",
     "start_time": "2024-08-13T22:44:45.830923Z"
    }
   },
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "source": [
    "kannada_sentences[:3]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35xhLztQiLIQ",
    "outputId": "aa70ad04-2e45-4c78-c852-61e92c51a96a",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.841968Z",
     "start_time": "2024-08-13T22:44:45.839155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['33 54', '552', '92 46525 4 553334']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "d_model = 512\n",
    "batch_size = 30\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "max_sequence_length = 200\n",
    "kn_vocab_size = len(kannada_vocabulary)\n",
    "\n",
    "transformer = Transformer(d_model, \n",
    "                          ffn_hidden,\n",
    "                          num_heads, \n",
    "                          drop_prob, \n",
    "                          num_layers, \n",
    "                          max_sequence_length,\n",
    "                          kn_vocab_size,\n",
    "                          english_to_index,\n",
    "                          kannada_to_index,\n",
    "                          START_TOKEN, \n",
    "                          END_TOKEN, \n",
    "                          PADDING_TOKEN)"
   ],
   "metadata": {
    "id": "xqOFnclmyxAE",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.904765Z",
     "start_time": "2024-08-13T22:44:45.866728Z"
    }
   },
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "source": [
    "transformer"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zc2hYQk9yxX0",
    "outputId": "c060f588-6a2e-4179-9475-5acafd641f1f",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.908550Z",
     "start_time": "2024-08-13T22:44:45.905726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(14, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(14, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNormalization()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, kannada_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.kannada_sentences = kannada_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.kannada_sentences[idx]"
   ],
   "metadata": {
    "id": "asUJX-STy7fg",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.962436Z",
     "start_time": "2024-08-13T22:44:45.959716Z"
    }
   },
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = TextDataset(english_sentences, kannada_sentences)"
   ],
   "metadata": {
    "id": "-auNWjkdzDge",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:45.978497Z",
     "start_time": "2024-08-13T22:44:45.976127Z"
    }
   },
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roH2A4m4zF4z",
    "outputId": "f4353aa8-2f37-43b6-be0f-12aab9a35145",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:46.011932Z",
     "start_time": "2024-08-13T22:44:46.009133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGeHNlzozIGF",
    "outputId": "ec3596fe-feee-426c-dce8-373fb07080fd",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:46.039593Z",
     "start_time": "2024-08-13T22:44:46.037191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('6644541 07355', '552')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ],
   "metadata": {
    "id": "5YDttjQ0zMrv",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:46.052295Z",
     "start_time": "2024-08-13T22:44:46.049593Z"
    }
   },
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EnjHKB1zM8Y",
    "outputId": "1a825e56-6706-46ee-85b5-ed7f0ac657fb",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:46.066204Z",
     "start_time": "2024-08-13T22:44:46.063107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('405 763543', '6644541 07355', '4 5350855 4343 5554 466454', '633 56', '523 244366 6 6455 556', '563364 43 5 33547 3', '64465 4435 452 56554 2674554', '546 5', '54 64463', '5964 56 44 4456', '54531 4654 4554 35466', '4836 3496 4464534 4455', '434 33554 36845 5353 544243', '5 545656 4 547 5455', '5 555344 5445555', '53 544764 45322 4445', '473 6 235 47', '9 36554 31', '656550 8 54 4547 68', '3546 3 3413554 0 76 246', '356 64 55524', '5 5544464 545456', '4555454 1 7 5335 46', '93555 7', '43344 559 5443', '255835 63 55 65 5435', '43 24764 6412552', '63409 3355 4', '5555 54 1235465 553 367464', '5664 536 467 445453'), ('33 54', '552', '92 46525 4 553334', '5654 82545', '6548554 4838 544337 54', '5643 2355442 4673 56', '465 4 25543 5456', '477554 65', '3544744 545', '20455 344545 55', '7645864 6454 3454555 8835', '536 4567 3565635 3', '5528 4245 4544 45549', '6 5 42255 53', '5437754 572', '5555577 446 6698', '375 54 4', '835 52', '44455 456 4364 56 6', '7 55 4522635 327725 5', '55465 654', '55542 7644', '67 35 5 6455 446', '35555', '6 25', '5544 8565 4437255 54 226', '2514554 64', '582545 47418 6', '455 3643 45 6 444', '4456 7443 7534545')]\n",
      "[('583542 75534', '54 624545 15555 404343', '1414533 8544', '456 63 5 4 274365', '5652 656 354', '550552 550 5454 43', '3 5 4544 4', '998 33063', '6556 47 5 43547 5473 51', '35 478524 5172433 776', '4757 44694', '52 843456 5 655 45', '44495 3 3544465 56 6', '33 45 3354356 5 487455', '54545 97', '6365 544430 464 94454', '444 53245 5 51472', '4584 44 5', '66 3 5 484545 577', '5 4645 456165 4', '357 534 534 4564 3 7 55', '3444025', '5864554', '44675 46 58 454', '5 5 56545', '553645 70634 623265 2534 2575', '5452 53 45686 455 6', '3565654 44 16654 45', '4544 854', '3 4455 5134 63 565 44557'), ('6204', '546244 604 6733 845546', '545 717', '536 4 5315225 333', '7442 345043', '654633 6444 6355456 5', '03545 55651 704541', '4345434 557', '3354 36 63576 4654 6 6354', '6 4484 152 5', '6648323 5', '21 54424 51 344 458', '6 4653 3 4855593 544', '46 4544885 542327 3551', '2546 15756', '4 22 64543 4', '6565 56436 3244', '3654 4 52', '65 480 7 6448358 6', '42 6413569 564448 82552', '4 8355 555745 7344 85 35425 5', '450652', '352464', '4 924545 3344', '56435 39', '7 44 462756 45064', '64354 623 4456 534', '55556 753 5553 4', '5574555 5445454', '53 35 57 5 45656 7745')]\n",
      "[('494 56714', '53 73346 544545 65535 356446', '34584 6435 226 44', '5 4364', '5 854642', '463 546404', '5544754 4556574 54462', '4 54 445336', '45 4454 4 646 457885', '4555', '3643414 547', '42 55 4', '3643 74353 4 55 6454505', '7 554 653545 4 34', '4455 3525 4544 45634 5 5534', '4643 4 442 5 14', '244414 53 44492 745352 4537644', '64354 46 46375 4634576 5125', '6746 555 45686 49354', '7554560 07 5295 45544', '5 654633 55', '3755673 545 2 635542', '773643 344546 2', '564 5 3453554 545 585', '456554 5 555 25 45736', '4 5466554 4 7354543', '544224 445436 5355664 4413 85', '547423 6835 55574', '4554643 5454', '455 6555 464344 5'), ('455', '476655 445 535 4', '6 6554 4', '37255 4', '3515 44', '5436', '4 54473 45', '36 44', '444444 55654 45 364455', '456657', '4 524', '552 2755465 5', '45511 3 5437 544 534', '54442 4544564 4546454 6544 554', '47 445 355 47 4545', '543545 5 33455 446 4', '55 4 56 155 167644', '45 555 54 4504445', '5445355 4055440 335 65', '53455 12452 4276', '451744 547', '44455 544563 745743', '634454 9553455 34', '44636 6447954 54450 4', '45454 45 5 5655', '544 55624 74355 4', '44 46548 5545 4544', '6543 4544517 56577', '5 55444', '455654 5554455 4455 5')]\n",
      "[('444 45444 4752 4536', '5554455 535 5835', '5 5553 6465 4 345 45', '54 5165 6455 544648', '4444795 4', '3544544 562 35 5555633', '431 25445 56645', '54555 544 454574 45', '6 44455 55775', '544 5 244 55555 5545', '7345544 3', '45456 44 63364', '54 4 83554 635554', '554 55 5345 563 5', '324 55547 55445', '5 335 545644', '5 5345544 25445', '5445546 5245553', '26 44544 6465 45', '5524 364 5635', '45 4462 55555', '45 14 5 555', '44754 44', '5 5 54443 4445544', '4354424 5345545 444572', '5 5 54', '4554 554344', '4445646 55 445144 5556', '544 45663', '5 24 44444'), ('3445 4346454 4135 55655', '4532 55', '4333652 54 55476 4 3234', '335 64 55', '545963 4556', '5351 4545642 5', '47 455 347', '44555 45455 435', '464 4454645 5', '43 455595 545544 55974 4455', '5454 745', '454424 5', '4254435 4544245 5455455 5456', '44556 455464 054456 554545 5455', '674454 553554', '5 533 545', '3255 556 6544', '45343', '35 454554 5', '556564 6 445', '55544 645544 55544', '2645 45 35454', '44 55455', '544355 5556445 4554 5445', '464555 45554', '4344 54 2', '436', '4452 30 454 5', '4547575 544', '4553 7546557 546')]\n",
      "[('4324 2 55535 545 332554', '5 745555 34', '54515 255 434344 54', '654 4235 4355635', '45646 24744 446', '5 447', '37 554', '44425 5 454524 44 463154', '454 4445 0664', '274 5 635 65 535474 6', '5 55651 654', '1 57262 673553', '3655664 4', '59456 4462 3 5 3345 43 5', '64 5547404 35', '5 5 24766', '5476 6453274 44244', '6 684554 16 64 4353556 764', '6443445 54', '4535 450554 5455', '5 655 5 34455', '568565 4555 5 4386 544557', '5446 6561 4', '3524556 55', '367454 52635 5655', '55445 34 34 4 35544', '464554 7 745', '456 553454 2245445 655 4563444', '54 545455', '6 65454 45463 356454'), ('543 565 4535541 6 4555', '54 5554', '3 47435 583', '6535564 1434 56', '2 354456', '2644 43', '654444 533', '5 0486634 4 2304366 73', '3466527 7627', '4550 255 3264435 5734 65', '4454344 03725', '64 513', '7639', '6596 6538563 6 33 365 447', '37 45', '595644 45325', '35544 6 22434', '534635 246 6 3464558 5', '5', '4333654 44', '3043455 6554533 6553', '24 4645 2444549 5 63', '2554 33544', '354', '54446 3', '555654 44 4445 4', '443 54', '5 5 5 44576', '655', '4245535 63645 45 3445')]\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "id": "XnanjzqtzQi8",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:46.140741Z",
     "start_time": "2024-08-13T22:44:46.102420Z"
    }
   },
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, kn_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
    "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ],
   "metadata": {
    "id": "_saWU5QmVem2",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:46.145384Z",
     "start_time": "2024-08-13T22:44:46.141768Z"
    }
   },
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modify mask such that the padding tokens cannot look ahead.\n",
    "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n",
    " "
   ],
   "metadata": {
    "id": "gdgtTSKvwN9_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
   ],
   "metadata": {
    "id": "xLcXI4wkMLck"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, kn_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
    "        optim.zero_grad()\n",
    "        kn_predictions = transformer(eng_batch,\n",
    "                                     kn_batch,\n",
    "                                     encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     enc_start_token=False,\n",
    "                                     enc_end_token=False,\n",
    "                                     dec_start_token=True,\n",
    "                                     dec_end_token=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
    "        loss = criterian(\n",
    "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
    "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in kn_sentence_predicted:\n",
    "              if idx == kannada_to_index[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += index_to_kannada[idx.item()]\n",
    "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            kn_sentence = (\"\",)\n",
    "            eng_sentence = (\"should we go to the mall?\",)\n",
    "            for word_counter in range(max_sequence_length):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          kn_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),\n",
    "                                          enc_start_token=False,\n",
    "                                          enc_end_token=False,\n",
    "                                          dec_start_token=True,\n",
    "                                          dec_end_token=False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = index_to_kannada[next_token_index]\n",
    "                kn_sentence = (kn_sentence[0] + next_token, )\n",
    "                if next_token == END_TOKEN:\n",
    "                  break\n",
    "            \n",
    "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
    "            print(\"-------------------------------------------\")"
   ],
   "metadata": {
    "id": "ju59VDGLuOqf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0ad34e31-521a-4ca2-f444-26b5374946f6",
    "ExecuteTime": {
     "end_time": "2024-08-13T22:44:47.847287Z",
     "start_time": "2024-08-13T22:44:46.146216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0 : 3.1729023456573486\n",
      "English: 405 763543\n",
      "Kannada Translation: 33 54\n",
      "Kannada Prediction: 0066 1 59666<PADDING>2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'s'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 51\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word_counter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_sequence_length):\n\u001B[1;32m     50\u001B[0m     encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\u001B[38;5;241m=\u001B[39m create_masks(eng_sentence, kn_sentence)\n\u001B[0;32m---> 51\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43meng_sentence\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mkn_sentence\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mencoder_self_attention_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdecoder_self_attention_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdecoder_cross_attention_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m                              \u001B[49m\u001B[43menc_start_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m                              \u001B[49m\u001B[43menc_end_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdec_start_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdec_end_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m     next_token_prob_distribution \u001B[38;5;241m=\u001B[39m predictions[\u001B[38;5;241m0\u001B[39m][word_counter] \u001B[38;5;66;03m# not actual probs\u001B[39;00m\n\u001B[1;32m     61\u001B[0m     next_token_index \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(next_token_prob_distribution)\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/miniconda3/envs/dlg_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/dlg_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/Transformer-Neural-Network/transformer.py:301\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \n\u001B[1;32m    292\u001B[0m             x, \n\u001B[1;32m    293\u001B[0m             y, \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    299\u001B[0m             dec_start_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;66;03m# We should make this true\u001B[39;00m\n\u001B[1;32m    300\u001B[0m             dec_end_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m): \u001B[38;5;66;03m# x, y are batch of sentences\u001B[39;00m\n\u001B[0;32m--> 301\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder_self_attention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menc_start_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menc_end_token\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token\u001B[38;5;241m=\u001B[39mdec_start_token, end_token\u001B[38;5;241m=\u001B[39mdec_end_token)\n\u001B[1;32m    303\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(out)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/dlg_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/dlg_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/Transformer-Neural-Network/transformer.py:178\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x, self_attention_mask, start_token, end_token)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, self_attention_mask, start_token, end_token):\n\u001B[0;32m--> 178\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentence_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_token\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers(x, self_attention_mask)\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/opt/miniconda3/envs/dlg_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/dlg_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/Transformer-Neural-Network/transformer.py:70\u001B[0m, in \u001B[0;36mSentenceEmbedding.forward\u001B[0;34m(self, x, start_token, end_token)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, start_token, end_token): \u001B[38;5;66;03m# sentence\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_token\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)\n\u001B[1;32m     72\u001B[0m     pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_encoder()\u001B[38;5;241m.\u001B[39mto(get_device())\n",
      "File \u001B[0;32m~/Documents/GitHub/Transformer-Neural-Network/transformer.py:65\u001B[0m, in \u001B[0;36mSentenceEmbedding.batch_tokenize\u001B[0;34m(self, batch, start_token, end_token)\u001B[0m\n\u001B[1;32m     63\u001B[0m tokenized \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence_num \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(batch)):\n\u001B[0;32m---> 65\u001B[0m    tokenized\u001B[38;5;241m.\u001B[39mappend( \u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[43msentence_num\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_token\u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m     66\u001B[0m tokenized \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(tokenized)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized\u001B[38;5;241m.\u001B[39mto(get_device())\n",
      "File \u001B[0;32m~/Documents/GitHub/Transformer-Neural-Network/transformer.py:54\u001B[0m, in \u001B[0;36mSentenceEmbedding.batch_tokenize.<locals>.tokenize\u001B[0;34m(sentence, start_token, end_token)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(sentence, start_token, end_token):\n\u001B[0;32m---> 54\u001B[0m     sentence_word_indicies \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlanguage_to_index\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(sentence)]\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m start_token:\n\u001B[1;32m     56\u001B[0m         sentence_word_indicies\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlanguage_to_index[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSTART_TOKEN])\n",
      "\u001B[0;31mKeyError\u001B[0m: 's'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "id": "1nosVPGVijId"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transformer.eval()\n",
    "def translate(eng_sentence):\n",
    "  eng_sentence = (eng_sentence,)\n",
    "  kn_sentence = (\"\",)\n",
    "  for word_counter in range(max_sequence_length):\n",
    "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
    "    predictions = transformer(eng_sentence,\n",
    "                              kn_sentence,\n",
    "                              encoder_self_attention_mask.to(device), \n",
    "                              decoder_self_attention_mask.to(device), \n",
    "                              decoder_cross_attention_mask.to(device),\n",
    "                              enc_start_token=False,\n",
    "                              enc_end_token=False,\n",
    "                              dec_start_token=True,\n",
    "                              dec_end_token=False)\n",
    "    next_token_prob_distribution = predictions[0][word_counter]\n",
    "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "    next_token = index_to_kannada[next_token_index]\n",
    "    kn_sentence = (kn_sentence[0] + next_token, )\n",
    "    if next_token == END_TOKEN:\n",
    "      break\n",
    "  return kn_sentence[0]"
   ],
   "metadata": {
    "id": "ZOQe-juylBiJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"what should we do when the day starts?\")\n",
    "print(translation)\n",
    "#ದಿನ ಪ್ರಾರಂಭವಾದಾಗ ನಾವು ಏನು ಮಾಡಬೇಕು?"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDVH_YsxlK6q",
    "outputId": "83c47f99-53c0-4c2d-c26a-aaa426f50563"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"how is this the truth?\")\n",
    "print(translation)\n",
    "#ಇದು ಹೇಗೆ ಸತ್ಯ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9yfawBnul0W",
    "outputId": "d9e6e6b7-683b-45f9-f013-c53c31038306"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"the world is a large place with different people\")\n",
    "print(translation)\n",
    "#ಪ್ರಪಂಚವು ವಿಭಿನ್ನ ಜನರೊಂದಿಗೆ ದೊಡ್ಡ ಸ್ಥಳವಾಗಿದೆ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpdYBk5-urcQ",
    "outputId": "ca7249c5-efda-4f41-f052-ecef9691be82"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"my name is ajay\")\n",
    "print(translation)\n",
    "#ನನ್ನ ಹೆಸರು ಅಜಯ್"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ni9e2UYUuxi3",
    "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"i cannot stand this smell\")\n",
    "print(translation)\n",
    "#ನಾನು ಈ ವಾಸನೆಯನ್ನು ಸಹಿಸುವುದಿಲ್ಲ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJuJKHqFldW3",
    "outputId": "71aa2c6c-ec77-4b02-d39b-bd724012fb53"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"noodles are the best\")\n",
    "print(translation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxHC4Lirlfu8",
    "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"why care about this?\")\n",
    "print(translation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLVOSI0Oli16",
    "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This translated pretty well : \"What is the reason. Why\" without punctuation."
   ],
   "metadata": {
    "id": "MStWCoAt0Ixp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"this is the best thing ever\")\n",
    "print(translation)\n",
    "# ಇದು ಎಂದೆಂದಿಗೂ ಉತ್ತಮವಾಗಿದೆ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB6TEJfGlkRT",
    "outputId": "adb465d5-b0ed-4be4-9251-62c079f49491"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The translation : \"This is very unusual\""
   ],
   "metadata": {
    "id": "zxsUjSybxYkh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"i am here\")\n",
    "print(translation)\n",
    "# ನಾನು ಇಲ್ಲಿದ್ದೇನೆ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQwDbuWBlmmA",
    "outputId": "7ae0e2c0-02c0-4c74-bc47-26b67da55a06"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Translation: \"I have heard\". \n",
    "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
   ],
   "metadata": {
    "id": "fmyZ2-I6x0Yf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"click this\")\n",
    "print(translation)\n",
    "# ಇದನ್ನು ಕ್ಲಿಕ್ ಮಾಡಿ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ifeV4bGluIj",
    "outputId": "6bce922d-d0db-432c-e6c6-97d482f1dea6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"where is the mall?\")\n",
    "print(translation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RB5DUBEl1kD",
    "outputId": "9109e504-8b9e-45dc-e13c-e878b3741e4e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"what should we do?\")\n",
    "print(translation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdeJ9CvMn5LM",
    "outputId": "044b5dac-29a9-4b60-e66a-4e10739a9756"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is correct; but it absolutely fumbles on the next one"
   ],
   "metadata": {
    "id": "LoikFnov1rj-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"today, what should we do\")\n",
    "print(translation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GFeyzrg1fIZ",
    "outputId": "2b4dfb04-def2-4725-9e76-5476bf85e2ef"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"why did they activate?\")\n",
    "print(translation)\n",
    "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0upygLS-sXcO",
    "outputId": "53a62435-ab16-4d4c-8a9f-0bf07af2a501"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"why did they do this?\")\n",
    "print(translation)\n",
    "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSrsqEGmtcl2",
    "outputId": "1b8b8faa-5370-426a-f2f2-fe852696bf7a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That turned out well!"
   ],
   "metadata": {
    "id": "f7ISM5rd3BLJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"i am well.\")\n",
    "print(translation)\n",
    "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjTcH2HFtyld",
    "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Translation: \"I will give you something\""
   ],
   "metadata": {
    "id": "cP0YX2g74eP7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translation = translate(\"whats the word on the street?\")\n",
    "print(translation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pGdN13kt5Br",
    "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kind of close semantically. Translation is something like: \"What is this about\""
   ],
   "metadata": {
    "id": "kFYZ6pOe4o-X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Insights\n",
    "\n",
    "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
    "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so. \n",
    "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
    "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
    "- Create a translator with a language you understand ideally."
   ],
   "metadata": {
    "id": "BzrOcNUk1-e5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
   ],
   "metadata": {
    "id": "cd6_k0Uu5V7f"
   }
  }
 ]
}
